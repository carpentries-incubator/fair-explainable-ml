{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63dd7d14",
   "metadata": {},
   "source": [
    "# OOD detection: distance-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2bfae1",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::::: questions\n",
    "- How do distance-based methods like Mahalanobis distance and KNN work for OOD detection?\n",
    "- What is contrastive learning and how does it improve feature representations?\n",
    "- How does contrastive learning enhance the effectiveness of distance-based OOD detection methods?\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "::::::::::::::::::::::::::::::::::::::: objectives\n",
    "- Gain a thorough understanding of distance-based OOD detection methods, including Mahalanobis distance and KNN.\n",
    "- Learn the principles of contrastive learning and its role in improving feature representations.\n",
    "- Explore the synergy between contrastive learning and distance-based OOD detection methods to enhance detection performance.\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# Example 3: Distance-Based Methods\n",
    "*Lee et al., A simple unified framework for detecting out-of-distribution samples and adversarial attacks. NeurIPS 2018.*\n",
    "\n",
    "With softmax and energy-based methods, we focus on the models outputs to determine a threshold that defines ID and OOD data. With distance-based methods, we focus on the feature representations learned by the model.\n",
    "\n",
    "In the case of neural networks, a common approach is to use the penultimate layer as a feature representation that can define an ID clusters for each class. You can then use distance to the closesent centroid as a proxy for OOD measure.\n",
    "\n",
    "## Mahalanobis distance (parametric)\n",
    "Model the feature space as a mixture of multivariate Gaussian distribution, one for each class. use distance to the closest centroid as proxy for OOD measure\n",
    "\n",
    "### Limiations of parametric approach\n",
    "* Strong distributional assumption (features may not necessarily be Gassian-distributed)\n",
    "* Suboptimal embedding\n",
    "\n",
    "## Nearest Neighbor Distance (non-parametric)\n",
    "*Sun et al., Out-of-distribution Detection with Deep Nearest Neighbors, ICML 2022*\n",
    "\n",
    "* Sample considered OOD if it has a large KNN distrance w.r.t. training data (and vice versa)\n",
    "* No distributional assumptions about underlying embedding space. Stronger generality and flexibility than mahalanobis distancew\n",
    "\n",
    "## CIDER\n",
    "This one might be out of scope...\n",
    "\n",
    "Ming et al., How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection\n",
    "# Contrastive Learning\n",
    "\n",
    "* Explain the basic idea of contrastive learning: learning representations by contrasting positive and negative pairs.\n",
    "* Highlight the role of contrastive learning in learning discriminative features that can separate in-distribution (ID) from OOD data more effectively.\n",
    "* Illustrate how contrastive learning improves the feature space, making distance-based methods (like Mahalanobis and KNN) more effective.\n",
    "* Provide examples or case studies where contrastive learning has been applied to enhance OOD detection.\n",
    "# Example X: Comparing feature representations with and without contrastive learning\n",
    "\n",
    "\n",
    "## Returning to UMAP\n",
    "Notice how in our UMAP visualization, we say three distinct clusters representing each class. However, our model still confidently rated many sandals as being tshirts. The crux of this issue is that models do not know what they don't know. They simply draw classifcation boundaries between the classes available to them during training.\n",
    "\n",
    "One way to get around this problem is to train models to learn discriminative features...\n",
    "\n",
    "## Contrastive learning\n",
    "In this experiment, we use both a traditional neural network and a contrastive learning model to classify images from the Fashion MNIST dataset, focusing on T-shirts (class 0) and Trousers (class 1). Additionally, we evaluate the models on out-of-distribution (OOD) data, specifically Sandals (class 5). To visualize the models' learned features, we extract features from specific layers of the neural networks and reduce their dimensionality using UMAP.\n",
    "\n",
    "## Overview of steps\n",
    "\n",
    "#### 1) Train model\n",
    "\n",
    "* With or without contrastive learning\n",
    "* Focusing on T-shirts (class 0) and Trousers (class 1)\n",
    "* Additionally, we evaluate the models on out-of-distribution (OOD) data, specifically Sandals (class 5)\n",
    "\n",
    "#### 2) Feature Extraction:\n",
    "\n",
    "* After training, we set the models to evaluation mode to prevent updates to the model parameters.\n",
    "* For each subset of the data (training, validation, and OOD), we pass the images through the entire network up to the first fully connected layer.\n",
    "* The output of this layer, which captures high-level features and abstractions, is then used as a 1D feature vector.\n",
    "* These feature vectors are detached from the computational graph and converted to NumPy arrays for further processing.\n",
    "\n",
    "#### 3) Dimensionality Reduction and Visualization:\n",
    "\n",
    "* We combine the feature vectors from the training, validation, and OOD data into a single dataset.\n",
    "* UMAP (Uniform Manifold Approximation and Projection) is used to reduce the dimensionality of the feature vectors from the high-dimensional space to 2D, making it possible to visualize the relationships between different data points.\n",
    "* The reduced features are then plotted, with different colors representing the training data (T-shirts and Trousers), validation data (T-shirts and Trousers), and OOD data (Sandals).\n",
    "\n",
    "By visualizing the features generated from different subsets of the data, we can observe how well the models have learned to distinguish between in-distribution classes (T-shirts and Trousers) and handle OOD data (Sandals). This approach allows us to evaluate the robustness and generalization capabilities of the models in dealing with data that may not have been seen during training.\n",
    "## Standard neural network w/out contrastive learning\n",
    "\n",
    "### 1) Train model\n",
    "We'll first train our vanilla CNN w/out contrastive learning.\n",
    "\n",
    "* Focusing on T-shirts (class 0) and Trousers (class 1)\n",
    "* Additionally, we evaluate the models on out-of-distribution (OOD) data, specifically Sandals (class 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cea007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model for classification\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes for T-shirts and Trousers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load Fashion MNIST dataset and filter for T-shirts and Trousers\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_indices = np.where((train_dataset.targets == 0) | (train_dataset.targets == 1))[0]\n",
    "val_indices = np.where((test_dataset.targets == 0) | (test_dataset.targets == 1))[0]\n",
    "ood_indices = np.where(test_dataset.targets == 5)[0]\n",
    "\n",
    "# Use a subset of the data for quicker training\n",
    "train_subset = Subset(train_dataset, np.random.choice(train_indices, size=5000, replace=False))\n",
    "val_subset = Subset(test_dataset, np.random.choice(val_indices, size=1000, replace=False))\n",
    "ood_subset = Subset(test_dataset, np.random.choice(ood_indices, size=1000, replace=False))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(ood_subset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "classification_model = ClassificationModel().to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for standard neural network\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_train_loss = 0\n",
    "    classification_model.train()\n",
    "    for batch_images, batch_labels in train_loader:\n",
    "        batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classification_model(batch_images)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    total_val_loss = 0\n",
    "    classification_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in val_loader:\n",
    "            batch_images, batch_labels = batch_images.to(device), batch_labels.to(device)\n",
    "            outputs = classification_model(batch_images)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, n_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss - Classification Model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784513f",
   "metadata": {},
   "source": [
    "### 2) Extracting learned features\n",
    "\n",
    "* After training, we set the models to evaluation mode to prevent updates to the model parameters.\n",
    "* For each subset of the data (training, validation, and OOD), we pass the images through the entire network up to the first fully connected layer.\n",
    "* The output of this layer, which captures high-level features and abstractions, is then used as a 1D feature vector.\n",
    "* These feature vectors are detached from the computational graph and converted to NumPy arrays for further processing.\n",
    "\n",
    "#### Why later layer features are better\n",
    "In both the traditional neural network and the contrastive learning model, we will extract features from the first fully connected layer (fc1) before the final classification layer. Here's why this layer is particularly suitable for feature extraction:\n",
    "\n",
    "* **Hierarchical feature representation**: In neural networks, the initial layers typically capture low-level features such as edges, textures, and simple shapes (e.g., with CNNs). As you move deeper into the network, the layers capture higher-level, more abstract features that are more relevant for the final classification task. These high-level features are combinations of the low-level features and are typically more discriminative.\n",
    "\n",
    "* **Better separation of classes**: Features from later layers have been transformed through several layers of non-linear activations and pooling operations, making them more suitable for distinguishing between classes.\n",
    "These features are usually more compact and have a better separation in the feature space, which helps in visualization and understanding the model's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using the trained classification model\n",
    "classification_model.eval()\n",
    "train_features = []\n",
    "train_labels_list = []\n",
    "for batch_images, batch_labels in train_loader:\n",
    "    batch_images = batch_images.to(device)\n",
    "    features = classification_model.fc1(classification_model.flatten(classification_model.conv1(batch_images)))\n",
    "    train_features.append(features.detach().cpu().numpy())\n",
    "    train_labels_list.append(batch_labels.numpy())\n",
    "\n",
    "val_features = []\n",
    "val_labels_list = []\n",
    "for batch_images, batch_labels in val_loader:\n",
    "    batch_images = batch_images.to(device)\n",
    "    features = classification_model.fc1(classification_model.flatten(classification_model.conv1(batch_images)))\n",
    "    val_features.append(features.detach().cpu().numpy())\n",
    "    val_labels_list.append(batch_labels.numpy())\n",
    "\n",
    "ood_features = []\n",
    "ood_labels_list = []\n",
    "for batch_images, batch_labels in ood_loader:\n",
    "    batch_images = batch_images.to(device)\n",
    "    features = classification_model.fc1(classification_model.flatten(classification_model.conv1(batch_images)))\n",
    "    ood_features.append(features.detach().cpu().numpy())\n",
    "    ood_labels_list.append(batch_labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa24b6",
   "metadata": {},
   "source": [
    "### 3) Dimensionality Reduction and Visualization:\n",
    "\n",
    "* We combine the feature vectors from the training, validation, and OOD data into a single dataset.\n",
    "* UMAP (Uniform Manifold Approximation and Projection) is used to reduce the dimensionality of the feature vectors from the high-dimensional space to 2D, making it possible to visualize the relationships between different data points.\n",
    "* The reduced features are then plotted, with different colors representing the training data (T-shirts and Trousers), validation data (T-shirts and Trousers), and OOD data (Sandals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cba146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.concatenate(train_features)\n",
    "train_labels = np.concatenate(train_labels_list)\n",
    "val_features = np.concatenate(val_features)\n",
    "val_labels = np.concatenate(val_labels_list)\n",
    "ood_features = np.concatenate(ood_features)\n",
    "ood_labels = np.concatenate(ood_labels_list)\n",
    "\n",
    "# Perform UMAP to visualize the classification model features\n",
    "combined_features = np.vstack([train_features, val_features, ood_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels, np.full(len(ood_labels), 2)])  # Use 2 for OOD class\n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_results = umap_reducer.fit_transform(combined_features)\n",
    "\n",
    "# Split the results back into train, val, and OOD data\n",
    "umap_train_features = umap_results[:len(train_features)]\n",
    "umap_val_features = umap_results[len(train_features):len(train_features) + len(val_features)]\n",
    "umap_ood_features = umap_results[len(train_features) + len(val_features):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271be2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting UMAP components for classification model\n",
    "alpha = .2\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot train T-shirts\n",
    "scatter1 = plt.scatter(umap_train_features[train_labels == 0, 0], umap_train_features[train_labels == 0, 1], c='blue', alpha=alpha, label='Train T-shirts (ID)')\n",
    "# Plot train Trousers\n",
    "scatter2 = plt.scatter(umap_train_features[train_labels == 1, 0], umap_train_features[train_labels == 1, 1], c='red', alpha=alpha, label='Train Trousers (ID)')\n",
    "# Plot val T-shirts\n",
    "scatter3 = plt.scatter(umap_val_features[val_labels == 0, 0], umap_val_features[val_labels == 0, 1], c='blue', alpha=alpha, marker='x', label='Val T-shirts (ID)')\n",
    "# Plot val Trousers\n",
    "scatter4 = plt.scatter(umap_val_features[val_labels == 1, 0], umap_val_features[val_labels == 1, 1], c='red', alpha=alpha, marker='x', label='Val Trousers (ID)')\n",
    "# Plot OOD Sandals\n",
    "scatter5 = plt.scatter(umap_ood_features[:, 0], umap_ood_features[:, 1], c='green', alpha=alpha, marker='o', label='OOD Sandals')\n",
    "plt.legend(handles=[scatter1, scatter2, scatter3, scatter4, scatter5])\n",
    "plt.xlabel('First UMAP Component')\n",
    "plt.ylabel('Second UMAP Component')\n",
    "plt.title('UMAP of Classification Model Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa097308",
   "metadata": {},
   "source": [
    "## Neural network trained with contrastive learning\n",
    "\n",
    "### What is Contrastive Learning?\n",
    "\n",
    "Contrastive learning is a technique where the model learns to distinguish between similar and dissimilar pairs of data. This can be achieved through different types of learning: supervised, unsupervised, and self-supervised.\n",
    "\n",
    "* Supervised Contrastive Learning: Uses labeled data to create pairs or groups of similar and dissimilar data points based on their labels.\n",
    "\n",
    "* Unsupervised Contrastive Learning: Does not use any labels. Instead, it relies on inherent patterns in the data to create pairs. For example, random pairs of data points might be assumed to be dissimilar, while augmented versions of the same data point might be assumed to be similar.\n",
    "\n",
    "* Self-Supervised Contrastive Learning: A form of unsupervised learning where the model generates its own supervisory signal from the data. This typically involves data augmentation techniques where positive pairs are created by augmenting the same image (e.g., cropping, rotating), and negative pairs are formed from different images.\n",
    "\n",
    "In contrastive learning, the model learns to bring similar pairs closer in the embedding space while pushing dissimilar pairs further apart. This approach is particularly useful for tasks like image retrieval, clustering, and representation learning.\n",
    "\n",
    "Certainly! Let's expand on how we are treating the T-shirt, Trouser, and Sandals classes in the context of our supervised contrastive learning framework.\n",
    "\n",
    "### Key Concepts in Our Code\n",
    "\n",
    "### Data Preparation\n",
    "   - **Dataset**: We use the Fashion MNIST dataset, which contains images of various clothing items, each labeled with a specific class.\n",
    "   - **Class Filtering**: For this exercise, we are focusing on three classes from the Fashion MNIST dataset:\n",
    "     - **T-shirts** (class label 0)\n",
    "     - **Trousers** (class label 1)\n",
    "     - **Sandals** (class label 5)\n",
    "   - **In-Distribution (ID) Data**: We treat T-shirts and Trousers as our primary classes for training. These are considered \"in-distribution\" data.\n",
    "   - **Out-of-Distribution (OOD) Data**: Sandals are treated as a different class for testing the robustness of our learned embeddings, making them \"out-of-distribution\" data.\n",
    "\n",
    "### Pairs Creation\n",
    "\n",
    "For each image in our training set:\n",
    "- **Positive Pair**: We find another image of the same class (either T-shirt or Trouser). These pairs are labeled as similar.\n",
    "- **Negative Pair**: We randomly choose an image from a different class (T-shirt paired with Trouser or vice versa). These pairs are labeled as dissimilar.\n",
    "\n",
    "By creating these pairs, the model learns to produce embeddings where similar images (same class) are close together, and dissimilar images (different classes) are farther apart.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The model is a simple Convolutional Neural Network (CNN) designed to output embeddings. It consists of:\n",
    "- Two convolutional layers to extract features from the images.\n",
    "- Fully connected layers to map these features to a 50-dimensional embedding space.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "- **Forward Pass**: The model processes pairs of images and outputs their embeddings.\n",
    "- **Contrastive Loss**: We use a contrastive loss function to train the model. This loss encourages embeddings of similar pairs to be close and embeddings of dissimilar pairs to be far apart. Specifically, we:\n",
    "  - Normalize the embeddings.\n",
    "  - Calculate similarity scores.\n",
    "  - Compute the contrastive loss, which penalizes similar pairs if they are not close enough and dissimilar pairs if they are too close.\n",
    "\n",
    "### Differences from Standard Neural Network Training\n",
    "\n",
    "- **Data Pairing**: In contrastive learning, we create pairs of data points. Standard neural network training typically involves individual data points with corresponding labels.\n",
    "- **Loss Function**: We use a contrastive loss function instead of the typical cross-entropy loss used in classification tasks. The contrastive loss is designed to optimize the relative distances between pairs of embeddings.\n",
    "- **Supervised Learning**: Our approach uses labeled data to form similar and dissimilar pairs, making it supervised contrastive learning. This contrasts with self-supervised or unsupervised methods where labels are not used.\n",
    "\n",
    "### Specific Type of Contrastive Learning\n",
    "\n",
    "The specific contrastive learning technique we are using here is a form of **supervised contrastive learning**. This involves using labeled data to create similar and dissimilar pairs of images. The model is trained to output embeddings where a contrastive loss function is applied to these pairs. By doing so, the model learns to map images into an embedding space where similar images are close together, and dissimilar images are farther apart.\n",
    "\n",
    "By training with this method, the model learns robust feature representations that are useful for various downstream tasks, even with limited labeled data. This is powerful because it allows leveraging labeled data to improve the model's performance and generalizability.\n",
    "\n",
    "### Application of the Framework\n",
    "\n",
    "1. **Training with In-Distribution Data**:\n",
    "   - **T-shirts and Trousers**: These classes are used to train the model. Positive and negative pairs are created within this subset to teach the model to distinguish between the two classes.\n",
    "   \n",
    "2. **Testing with Out-of-Distribution Data**:\n",
    "   - **Sandals**: This class is used to test the robustness of the embeddings learned by the model. By introducing a completely different class during testing, we can evaluate how well the model generalizes to new, unseen data.\n",
    "\n",
    "This framework demonstrates how supervised contrastive learning can be effectively applied to learn discriminative embeddings that can generalize well to both in-distribution and out-of-distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = self.images[idx]\n",
    "        label1 = self.labels[idx]\n",
    "        idx2 = np.random.choice(np.where(self.labels == label1)[0])\n",
    "        img2 = self.images[idx2]\n",
    "        return img1, img2, label1\n",
    "\n",
    "# Load Fashion MNIST dataset and filter for T-shirts and Trousers\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_indices = np.where((train_dataset.targets == 0) | (train_dataset.targets == 1))[0]\n",
    "val_indices = np.where((test_dataset.targets == 0) | (test_dataset.targets == 1))[0]\n",
    "ood_indices = np.where(test_dataset.targets == 5)[0]\n",
    "\n",
    "# Use a subset of the data for quicker training\n",
    "train_subset = Subset(train_dataset, np.random.choice(train_indices, size=5000, replace=False))\n",
    "val_subset = Subset(test_dataset, np.random.choice(val_indices, size=1000, replace=False))\n",
    "ood_subset = Subset(test_dataset, np.random.choice(ood_indices, size=1000, replace=False))\n",
    "\n",
    "# Create DataLoaders for the subsets\n",
    "train_images = np.array([train_dataset[i][0].numpy() for i in train_indices])\n",
    "train_labels = np.array([train_dataset[i][1] for i in train_indices])\n",
    "val_images = np.array([test_dataset[i][0].numpy() for i in val_indices])\n",
    "val_labels = np.array([test_dataset[i][1] for i in val_indices])\n",
    "ood_images = np.array([test_dataset[i][0].numpy() for i in ood_indices])\n",
    "ood_labels = np.array([test_dataset[i][1] for i in ood_indices])\n",
    "\n",
    "train_loader = DataLoader(PairDataset(train_images, train_labels), batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(PairDataset(val_images, val_labels), batch_size=256, shuffle=False)\n",
    "ood_loader = DataLoader(PairDataset(ood_images, ood_labels), batch_size=256, shuffle=False)\n",
    "\n",
    "# Inspect the data loaders\n",
    "for batch_images1, batch_images2, batch_labels in train_loader:\n",
    "    print(f\"train_loader batch_images1 shape: {batch_images1.shape}\")\n",
    "    print(f\"train_loader batch_images2 shape: {batch_images2.shape}\")\n",
    "    print(f\"train_loader batch_labels shape: {batch_labels.shape}\")\n",
    "    break\n",
    "\n",
    "for batch_images1, batch_images2, batch_labels in val_loader:\n",
    "    print(f\"val_loader batch_images1 shape: {batch_images1.shape}\")\n",
    "    print(f\"val_loader batch_images2 shape: {batch_images2.shape}\")\n",
    "    print(f\"val_loader batch_labels shape: {batch_labels.shape}\")\n",
    "    break\n",
    "\n",
    "for batch_images1, batch_images2, batch_labels in ood_loader:\n",
    "    print(f\"ood_loader batch_images1 shape: {batch_images1.shape}\")\n",
    "    print(f\"ood_loader batch_images2 shape: {batch_images2.shape}\")\n",
    "    print(f\"ood_loader batch_labels shape: {batch_labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model for contrastive learning\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 50)  # Embedding size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define contrastive loss function\n",
    "def contrastive_loss(z_i, z_j, temperature=0.5):\n",
    "    z_i = nn.functional.normalize(z_i, dim=1)\n",
    "    z_j = nn.functional.normalize(z_j, dim=1)\n",
    "    batch_size = z_i.size(0)\n",
    "    z = torch.cat([z_i, z_j], dim=0)\n",
    "\n",
    "    sim = torch.mm(z, z.t()) / temperature\n",
    "    sim_i_j = torch.diag(sim, batch_size)\n",
    "    sim_j_i = torch.diag(sim, -batch_size)\n",
    "\n",
    "    positives = torch.cat([sim_i_j, sim_j_i], dim=0)\n",
    "    negatives_mask = ~torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "    negatives = sim[negatives_mask].view(2 * batch_size, -1)\n",
    "\n",
    "    loss = -torch.mean(positives) + torch.mean(negatives)\n",
    "    return loss\n",
    "\n",
    "# Training loop for contrastive learning\n",
    "def train_contrastive_model(model, train_loader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for img1, img2, _ in train_loader:\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z_i = model(img1)\n",
    "            z_j = model(img2)\n",
    "\n",
    "            loss = contrastive_loss(z_i, z_j)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Instantiate the model, optimizer, and start training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "contrastive_model = ContrastiveModel().to(device)\n",
    "optimizer = optim.Adam(contrastive_model.parameters(), lr=0.001)\n",
    "\n",
    "train_contrastive_model(contrastive_model, train_loader, optimizer, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88790561",
   "metadata": {},
   "source": [
    "### 2) Extracting learned features\n",
    "\n",
    "* After training, we set the models to evaluation mode to prevent updates to the model parameters.\n",
    "* For each subset of the data (training, validation, and OOD), we pass the images through the entire network up to the first fully connected layer.\n",
    "* The output of this layer, which captures high-level features and abstractions, is then used as a 1D feature vector.\n",
    "* These feature vectors are detached from the computational graph and converted to NumPy arrays for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using the trained contrastive model\n",
    "contrastive_model.eval()\n",
    "train_features = []\n",
    "train_labels_list = []\n",
    "for img1, _, label1 in train_loader:\n",
    "    img1 = img1.to(device)\n",
    "    features = contrastive_model.fc1(contrastive_model.flatten(contrastive_model.conv1(img1)))\n",
    "    train_features.append(features.detach().cpu().numpy())\n",
    "    train_labels_list.append(label1.numpy())\n",
    "\n",
    "val_features = []\n",
    "val_labels_list = []\n",
    "for img1, _, label1 in val_loader:\n",
    "    img1 = img1.to(device)\n",
    "    features = contrastive_model.fc1(contrastive_model.flatten(contrastive_model.conv1(img1)))\n",
    "    val_features.append(features.detach().cpu().numpy())\n",
    "    val_labels_list.append(label1.numpy())\n",
    "\n",
    "ood_features = []\n",
    "ood_labels_list = []\n",
    "for img1, _, label1 in ood_loader:\n",
    "    img1 = img1.to(device)\n",
    "    features = contrastive_model.fc1(contrastive_model.flatten(contrastive_model.conv1(img1)))\n",
    "    ood_features.append(features.detach().cpu().numpy())\n",
    "    ood_labels_list.append(label1.numpy())\n",
    "\n",
    "train_features = np.concatenate(train_features)\n",
    "train_labels = np.concatenate(train_labels_list)\n",
    "val_features = np.concatenate(val_features)\n",
    "val_labels = np.concatenate(val_labels_list)\n",
    "ood_features = np.concatenate(ood_features)\n",
    "ood_labels = np.concatenate(ood_labels_list)\n",
    "\n",
    "# Diagnostic print statements\n",
    "print(f\"train_features shape: {train_features.shape}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "print(f\"val_features shape: {val_features.shape}\")\n",
    "print(f\"val_labels shape: {val_labels.shape}\")\n",
    "print(f\"ood_features shape: {ood_features.shape}\")\n",
    "print(f\"ood_labels shape: {ood_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f2448",
   "metadata": {},
   "source": [
    "### 3) Dimensionality Reduction and Visualization:\n",
    "\n",
    "* We combine the feature vectors from the training, validation, and OOD data into a single dataset.\n",
    "* UMAP (Uniform Manifold Approximation and Projection) is used to reduce the dimensionality of the feature vectors from the high-dimensional space to 2D, making it possible to visualize the relationships between different data points.\n",
    "* The reduced features are then plotted, with different colors representing the training data (T-shirts and Trousers), validation data (T-shirts and Trousers), and OOD data (Sandals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ee75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the labels array for OOD matches the feature array length\n",
    "combined_features = np.vstack([train_features, val_features, ood_features])\n",
    "combined_labels = np.hstack([train_labels, val_labels, np.full(len(ood_features), 2)])  # Use 2 for OOD class\n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "umap_results = umap_reducer.fit_transform(combined_features)\n",
    "\n",
    "# Split the results back into train, val, and OOD data\n",
    "umap_train_features = umap_results[:len(train_features)]\n",
    "umap_val_features = umap_results[len(train_features):len(train_features) + len(val_features)]\n",
    "umap_ood_features = umap_results[len(train_features) + len(val_features):]\n",
    "\n",
    "# Plotting UMAP components for contrastive learning model\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot train T-shirts\n",
    "scatter1 = plt.scatter(umap_train_features[train_labels == 0, 0], umap_train_features[train_labels == 0, 1], c='blue', alpha=0.5, label='Train T-shirts (ID)')\n",
    "# Plot train Trousers\n",
    "scatter2 = plt.scatter(umap_train_features[train_labels == 1, 0], umap_train_features[train_labels == 1, 1], c='red', alpha=0.5, label='Train Trousers (ID)')\n",
    "# Plot val T-shirts\n",
    "scatter3 = plt.scatter(umap_val_features[val_labels == 0, 0], umap_val_features[val_labels == 0, 1], c='blue', alpha=0.5, marker='x', label='Val T-shirts (ID)')\n",
    "# Plot val Trousers\n",
    "scatter4 = plt.scatter(umap_val_features[val_labels == 1, 0], umap_val_features[val_labels == 1, 1], c='red', alpha=0.5, marker='x', label='Val Trousers (ID)')\n",
    "# Plot OOD Sandals\n",
    "scatter5 = plt.scatter(umap_ood_features[:, 0], umap_ood_features[:, 1], c='green', alpha=0.5, marker='o', label='OOD Sandals')\n",
    "plt.legend(handles=[scatter1, scatter2, scatter3, scatter4, scatter5])\n",
    "plt.xlabel('First UMAP Component')\n",
    "plt.ylabel('Second UMAP Component')\n",
    "plt.title('UMAP of Contrastive Model Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecef5d",
   "metadata": {},
   "source": [
    "# Limitations of Threshold-Based OOD Detection Methods\n",
    "\n",
    "Threshold-based out-of-distribution (OOD) detection methods are widely used due to their simplicity and intuitive nature. However, they come with several significant limitations that need to be considered:\n",
    "\n",
    "1. **Dependence on OOD Data Choice**:\n",
    "   - **Variety and Representation**: The effectiveness of threshold-based methods heavily relies on the variety and representativeness of the OOD data used during threshold selection. If the chosen OOD samples do not adequately cover the possible range of OOD scenarios, the threshold may not generalize well to unseen OOD data.\n",
    "   - **Threshold Determination**: To determine a robust threshold, it is essential to include a diverse set of OOD samples. This helps in setting a threshold that can effectively distinguish between in-distribution and out-of-distribution data across various scenarios. Without a comprehensive OOD dataset, the threshold might either be too conservative, causing many ID samples to be misclassified as OOD, or too lenient, failing to detect OOD samples accurately.\n",
    "\n",
    "2. **Impact of High Thresholds**:\n",
    "   - **False OOD Classification**: High thresholds can lead to a significant number of ID samples being incorrectly classified as OOD. This false OOD classification results in the loss of potentially valuable data, reducing the efficiency and performance of the model.\n",
    "   - **Data Efficiency**: In applications where retaining as much ID data as possible is crucial, high thresholds can be particularly detrimental. It’s important to strike a balance between detecting OOD samples and retaining ID samples to ensure the model’s overall performance and data efficiency.\n",
    "\n",
    "3. **Sensitivity to Model Confidence**:\n",
    "   - **Model Calibration**: Threshold-based methods rely on the model's confidence scores, which can be misleading if the model is poorly calibrated. Overconfident predictions for ID samples or underconfident predictions for OOD samples can result in suboptimal threshold settings.\n",
    "   - **Confidence Variability**: The variability in confidence scores across different models and architectures can make it challenging to set a universal threshold. Each model might require different threshold settings, complicating the deployment and maintenance of threshold-based OOD detection systems.\n",
    "\n",
    "4. **Lack of Discriminative Features**:\n",
    "   - **Boundary-Based Detection**: Threshold-based methods focus on class boundaries rather than learning discriminative features that can effectively separate ID and OOD samples. This approach can be less robust, particularly in complex or high-dimensional data spaces where class boundaries might be less clear.\n",
    "   - **Feature Learning**: By relying solely on confidence scores, these methods miss the opportunity to learn and leverage features that are inherently more discriminative. This limitation highlights the need for advanced techniques like contrastive learning, which focuses on learning features that distinguish between ID and OOD samples more effectively.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While threshold-based OOD detection methods offer a straightforward approach, their limitations underscore the importance of considering additional OOD samples for robust threshold determination and the potential pitfalls of high thresholds. Transitioning to methods that learn discriminative features rather than relying solely on class boundaries can address these limitations, paving the way for more effective OOD detection. This sets the stage for discussing contrastive learning, which provides a powerful framework for learning such discriminative features."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
