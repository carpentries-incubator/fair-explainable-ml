{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c17650-24df-413b-9af5-e39621bbddf1",
   "metadata": {},
   "source": [
    "-   What are energy-based methods for out-of-distribution (OOD) detection, and how do they compare to softmax-based approaches?\n",
    "-   How does the energy metric enhance separability between in-distribution and OOD data?\n",
    "-   What are the challenges and limitations of energy-based OOD detection methods?\n",
    "\n",
    "-   Understand the concept of energy-based OOD detection and its theoretical foundations.\n",
    "-   Compare energy-based methods to softmax-based approaches, highlighting their strengths and limitations.\n",
    "-   Learn how to implement energy-based OOD detection using tools like PyTorch-OOD.\n",
    "-   Explore challenges in applying energy-based methods, including threshold tuning and generalizability to diverse OOD scenarios.\n",
    "\n",
    "# Example 2: Energy-Based OOD Detection\n",
    "\n",
    "Traditional approaches, such as softmax-based methods, rely on output probabilities to flag OOD data. While simple and intuitive, these methods often struggle to distinguish OOD data effectively in complex scenarios, especially in high-dimensional spaces.\n",
    "\n",
    "Energy-based OOD detection offers a modern and robust alternative. This “output-based” approach leverages the **energy score**, a scalar value derived from a model’s output logits, to measure the compatibility between input data and the model’s learned distribution. The energy score directly ties to the Gibbs distribution, which allows for a nuanced interpretation of data compatibility. By computing energy scores and interpreting them probabilistically, energy-based methods enhance separability between in-distribution (ID) and OOD data. These techniques, particularly effective with neural networks, address some of the key limitations of softmax-based approaches.\n",
    "\n",
    "In this episode, we will explore the theoretical foundations of energy-based OOD detection, implement it using the PyTorch-OOD library, and compare its performance to softmax-based methods. Along the way, we will provide intuitive explanations of key concepts like the Gibbs distribution to ensure accessibility for ML practitioners. We will also discuss the challenges of energy-based methods, including threshold tuning and generalization to diverse OOD scenarios. This understanding will set the stage for hybrid and training-time regularization methods discussed in future episodes.\n",
    "\n",
    "### intuition behind the Gibbs distribution and energy scores\n",
    "\n",
    "The Gibbs distribution is a probability distribution used to model systems in equilibrium, and it connects naturally to the concept of energy. In the context of machine learning:\n",
    "\n",
    "-   **Energy as a compatibility measure**: Think of energy as a score that measures how “compatible” a data point is with the model’s learned distribution. A **lower energy score** indicates higher compatibility (i.e., the model sees the input as likely to belong to the training distribution).\n",
    "\n",
    "-   **From energy to probability**: The Gibbs distribution converts energy into probabilities. For an input (x), the probability of observing (x) is proportional to (e^{-(x)}). This exponential relationship means that even small differences in energy can lead to significant changes in probability, making energy scores sensitive and effective for OOD detection.\n",
    "\n",
    "-   **Why energy works for OOD detection**: OOD data often results in higher energy scores because the model’s output logits are less aligned with any known class. By setting a threshold on energy scores, we can distinguish OOD data from ID data more effectively than with softmax probabilities alone.\n",
    "\n",
    "For practitioners, the Gibbs distribution provides a bridge between abstract “energy” values and interpretable probabilities, helping us reason about uncertainty and compatibility in a model’s predictions.\n",
    "\\* E(x, y) = energy value\n",
    "\n",
    "-   if x and y are “compatitble”, lower energy\n",
    "\n",
    "-   Energy can be turned into probability through Gibbs distribution\n",
    "\n",
    "    -   looks at integral over all possible y’s\n",
    "\n",
    "-   With energy scores, ID and OOD distributions become much more separable\n",
    "\n",
    "**Learn more**: Liu et al., Energy-based Out-of-distribution Detection, NeurIPS 2020; https://arxiv.org/pdf/2010.03759\n",
    "\n",
    "## Introducing PyTorch OOD\n",
    "\n",
    "The PyTorch-OOD library provides methods for OOD detection and other closely related fields, such as anomoly detection or novelty detection. Visit the docs to learn more: [pytorch-ood.readthedocs.io/en/latest/info.html](https://pytorch-ood.readthedocs.io/en/latest/info.html)\n",
    "\n",
    "This library will provide a streamlined way to calculate both energy and softmax scores from a trained model.\n",
    "\\### Setup example\n",
    "In this example, we will train a CNN model on the FashionMNIST dataset. We will then repeat a similar process as we did with softmax scores to evaluate how well the energy metric can separate ID and OOD data.\n",
    "\n",
    "We’ll start by fresh by loading our data again. This time, let’s treat all remaining classes in the MNIST fashion dataset as OOD. This should yield a more robust model that is more reliable when presented with all kinds of data.\n",
    "\n",
    "``` python\n",
    "train_data, test_data, ood_data, train_labels, test_labels, ood_labels = prep_ID_OOD_datasests([0,1], list(range(2,10))) # use remaining 8 classes in dataset as OOD\n",
    "fig = plot_data_sample(train_data, ood_data)\n",
    "fig.savefig('../images/OOD-detection_image-data-preview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Visualizing OOD and ID data\n",
    "\n",
    "### UMAP (or similar)\n",
    "\n",
    "Recall in our previous example, we used PCA to visualize the ID and OOD data distributions. This was appropriate given that we were evaluating OOD/ID data in the context of a linear model. However, when working with nonlinear models such as CNNs, it makes more sense to investigate how the data is represented in a nonlinear space. Nonlinear embedding methods, such as Uniform Manifold Approximation and Projection (UMAP), are more suitable in such scenarios.\n",
    "\n",
    "UMAP is a non-linear dimensionality reduction technique that preserves both the global structure and the local neighborhood relationships in the data. UMAP is often better at maintaining the continuity of data points that lie on non-linear manifolds. It can reveal nonlinear patterns and structures that PCA might miss, making it a valuable tool for analyzing ID and OOD distributions.\n",
    "\n",
    "``` python\n",
    "plot_umap = True # leave off for now to save time testing downstream materials\n",
    "if plot_umap:\n",
    "    import umap\n",
    "    # Flatten images for PCA and logistic regression\n",
    "    train_data_flat = train_data.reshape((train_data.shape[0], -1))\n",
    "    test_data_flat = test_data.reshape((test_data.shape[0], -1))\n",
    "    ood_data_flat = ood_data.reshape((ood_data.shape[0], -1))\n",
    "    \n",
    "    print(f'train_data_flat.shape={train_data_flat.shape}')\n",
    "    print(f'test_data_flat.shape={test_data_flat.shape}')\n",
    "    print(f'ood_data_flat.shape={ood_data_flat.shape}')\n",
    "    \n",
    "    # Perform UMAP to visualize the data\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    combined_data = np.vstack([train_data_flat, ood_data_flat])\n",
    "    combined_labels = np.hstack([train_labels, np.full(ood_data_flat.shape[0], 2)])  # Use 2 for OOD class\n",
    "    \n",
    "    umap_results = umap_reducer.fit_transform(combined_data)\n",
    "    \n",
    "    # Split the results back into in-distribution and OOD data\n",
    "    umap_in_dist = umap_results[:len(train_data_flat)]\n",
    "    umap_ood = umap_results[len(train_data_flat):]\n",
    "```\n",
    "\n",
    "The warning message indicates that UMAP has overridden the n_jobs parameter to 1 due to the random_state being set. This behavior ensures reproducibility by using a single job. If you want to avoid the warning and still use parallelism, you can remove the random_state parameter. However, removing random_state will mean that the results might not be reproducible.\n",
    "\n",
    "``` python\n",
    "if plot_umap:\n",
    "    umap_alpha = .02\n",
    "\n",
    "    # Plotting UMAP components\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot in-distribution data\n",
    "    scatter1 = plt.scatter(umap_in_dist[train_labels == 0, 0], umap_in_dist[train_labels == 0, 1], c='blue', label='T-shirts (ID)', alpha=umap_alpha)\n",
    "    scatter2 = plt.scatter(umap_in_dist[train_labels == 1, 0], umap_in_dist[train_labels == 1, 1], c='red', label='Trousers (ID)', alpha=umap_alpha)\n",
    "    \n",
    "    # Plot OOD data\n",
    "    scatter3 = plt.scatter(umap_ood[:, 0], umap_ood[:, 1], c='green', label='OOD', edgecolor='k', alpha=alpha)\n",
    "    \n",
    "    # Create a single legend for all classes\n",
    "    plt.legend(handles=[scatter1, scatter2, scatter3], loc=\"upper right\")\n",
    "    plt.xlabel('First UMAP Component')\n",
    "    plt.ylabel('Second UMAP Component')\n",
    "    plt.title('UMAP of In-Distribution and OOD Data')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "## Train CNN\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Convert to PyTorch tensors and normalize\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "ood_data_tensor = torch.tensor(ood_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "ood_dataset = torch.utils.data.TensorDataset(ood_data_tensor, torch.zeros(ood_data_tensor.shape[0], dtype=torch.long))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 128)  # Updated this line\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64*5*5)  # Updated this line\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "```\n",
    "\n",
    "``` python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(labels, predictions, title):\n",
    "    cm = confusion_matrix(labels, predictions, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"T-shirt/top\", \"Trouser\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate model on a dataset\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "# Evaluate on train data\n",
    "train_labels, train_predictions = evaluate_model(model, train_loader, device)\n",
    "plot_confusion_matrix(train_labels, train_predictions, \"Confusion Matrix for Train Data\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_labels, test_predictions = evaluate_model(model, test_loader, device)\n",
    "plot_confusion_matrix(test_labels, test_predictions, \"Confusion Matrix for Test Data\")\n",
    "\n",
    "# Evaluate on OOD data\n",
    "ood_labels, ood_predictions = evaluate_model(model, ood_loader, device)\n",
    "plot_confusion_matrix(ood_labels, ood_predictions, \"Confusion Matrix for OOD Data\")\n",
    "```\n",
    "\n",
    "``` python\n",
    "from scipy.stats import gaussian_kde\n",
    "from pytorch_ood.detector import EnergyBased\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Compute softmax scores\n",
    "def get_softmax_scores(model, dataloader):\n",
    "    model.eval()\n",
    "    softmax_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            softmax = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            softmax_scores.extend(softmax.cpu().numpy())\n",
    "    return np.array(softmax_scores)\n",
    "\n",
    "id_softmax_scores = get_softmax_scores(model, test_loader)\n",
    "ood_softmax_scores = get_softmax_scores(model, ood_loader)\n",
    "\n",
    "# Initialize the energy-based OOD detector\n",
    "energy_detector = EnergyBased(model, t=1.0)\n",
    "\n",
    "# Compute energy scores\n",
    "def get_energy_scores(detector, dataloader):\n",
    "    scores = []\n",
    "    detector.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            score = detector.predict(inputs)\n",
    "            scores.extend(score.cpu().numpy())\n",
    "    return np.array(scores)\n",
    "\n",
    "id_energy_scores = get_energy_scores(energy_detector, test_loader)\n",
    "ood_energy_scores = get_energy_scores(energy_detector, ood_loader)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot PSDs\n",
    "\n",
    "# Function to plot PSD\n",
    "def plot_psd(id_scores, ood_scores, method_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    alpha = 0.3\n",
    "\n",
    "    # Plot PSD for ID scores\n",
    "    id_density = gaussian_kde(id_scores)\n",
    "    x_id = np.linspace(id_scores.min(), id_scores.max(), 1000)\n",
    "    plt.plot(x_id, id_density(x_id), label=f'ID ({method_name})', color='blue', alpha=alpha)\n",
    "\n",
    "    # Plot PSD for OOD scores\n",
    "    ood_density = gaussian_kde(ood_scores)\n",
    "    x_ood = np.linspace(ood_scores.min(), ood_scores.max(), 1000)\n",
    "    plt.plot(x_ood, ood_density(x_ood), label=f'OOD ({method_name})', color='red', alpha=alpha)\n",
    "\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Probability Density Distributions for {method_name} Scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot PSD for softmax scores\n",
    "plot_psd(id_softmax_scores[:, 1], ood_softmax_scores[:, 1], 'Softmax')\n",
    "\n",
    "# Plot PSD for energy scores\n",
    "plot_psd(id_energy_scores, ood_energy_scores, 'Energy')\n",
    "\n",
    "```\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define thresholds to evaluate\n",
    "thresholds = np.linspace(id_energy_scores.min(), id_energy_scores.max(), 50)\n",
    "\n",
    "# Store evaluation metrics for each threshold\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# True labels for OOD data (since they are not part of the original labels)\n",
    "ood_true_labels = np.full(len(ood_energy_scores), -1)\n",
    "\n",
    "# We need the test_labels to be aligned with the ID data\n",
    "id_true_labels = test_labels[:len(id_energy_scores)]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Classify OOD examples based on energy scores\n",
    "    ood_classifications = np.where(ood_energy_scores >= threshold, -1,  # classified as OOD\n",
    "                                   np.where(ood_energy_scores < threshold, 0, -1))  # classified as ID\n",
    "\n",
    "    # Classify ID examples based on energy scores\n",
    "    id_classifications = np.where(id_energy_scores >= threshold, -1,  # classified as OOD\n",
    "                                  np.where(id_energy_scores < threshold, id_true_labels, -1))  # classified as ID\n",
    "\n",
    "    # Combine OOD and ID classifications and true labels\n",
    "    all_predictions = np.concatenate([ood_classifications, id_classifications])\n",
    "    all_true_labels = np.concatenate([ood_true_labels, id_true_labels])\n",
    "\n",
    "    # Evaluate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels, all_predictions, labels=[0, 1], average='macro')#, zero_division=0)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the best thresholds for each metric\n",
    "best_f1_index = np.argmax(f1_scores)\n",
    "best_f1_threshold = thresholds[best_f1_index]\n",
    "\n",
    "best_precision_index = np.argmax(precisions)\n",
    "best_precision_threshold = thresholds[best_precision_index]\n",
    "\n",
    "best_recall_index = np.argmax(recalls)\n",
    "best_recall_threshold = thresholds[best_recall_index]\n",
    "\n",
    "print(f\"Best F1 threshold: {best_f1_threshold}, F1 Score: {f1_scores[best_f1_index]}\")\n",
    "print(f\"Best Precision threshold: {best_precision_threshold}, Precision: {precisions[best_precision_index]}\")\n",
    "print(f\"Best Recall threshold: {best_recall_threshold}, Recall: {recalls[best_recall_index]}\")\n",
    "\n",
    "# Plot metrics as functions of the threshold\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, precisions, label='Precision', color='g')\n",
    "plt.plot(thresholds, recalls, label='Recall', color='b')\n",
    "plt.plot(thresholds, f1_scores, label='F1 Score', color='r')\n",
    "\n",
    "# Add best threshold indicators\n",
    "plt.axvline(x=best_f1_threshold, color='r', linestyle='--', label=f'Best F1 Threshold: {best_f1_threshold:.2f}')\n",
    "plt.axvline(x=best_precision_threshold, color='g', linestyle='--', label=f'Best Precision Threshold: {best_precision_threshold:.2f}')\n",
    "plt.axvline(x=best_recall_threshold, color='b', linestyle='--', label=f'Best Recall Threshold: {best_recall_threshold:.2f}')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Evaluation Metrics as Functions of Threshold (Energy-Based OOD Detection)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def evaluate_ood_detection(id_scores, ood_scores, id_true_labels, id_predictions, ood_predictions, score_type='energy'):\n",
    "    \"\"\"\n",
    "    Evaluate OOD detection based on either energy scores or softmax scores.\n",
    "\n",
    "    Parameters:\n",
    "    - id_scores: np.array, scores for in-distribution (ID) data\n",
    "    - ood_scores: np.array, scores for out-of-distribution (OOD) data\n",
    "    - id_true_labels: np.array, true labels for ID data\n",
    "    - id_predictions: np.array, predicted labels for ID data\n",
    "    - ood_predictions: np.array, predicted labels for OOD data\n",
    "    - score_type: str, type of score used ('energy' or 'softmax')\n",
    "\n",
    "    Returns:\n",
    "    - Best thresholds for F1, Precision, and Recall\n",
    "    - Plots of Precision, Recall, and F1 Score as functions of the threshold\n",
    "    \"\"\"\n",
    "    # Define thresholds to evaluate\n",
    "    if score_type == 'softmax':\n",
    "        thresholds = np.linspace(0.5, 1.0, 200)\n",
    "    else:\n",
    "        thresholds = np.linspace(id_scores.min(), id_scores.max(), 50)\n",
    "\n",
    "    # Store evaluation metrics for each threshold\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # True labels for OOD data (since they are not part of the original labels)\n",
    "    if score_type == \"energy\":\n",
    "        ood_true_labels = np.full(len(ood_scores), -1)\n",
    "    else:\n",
    "        ood_true_labels = np.full(len(ood_scores[:,0]), -1)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Classify OOD examples based on scores\n",
    "        if score_type == 'energy':\n",
    "            ood_classifications = np.where(ood_scores >= threshold, -1, ood_predictions)\n",
    "            id_classifications = np.where(id_scores >= threshold, -1, id_predictions)\n",
    "        elif score_type == 'softmax':\n",
    "            ood_classifications = np.where(ood_scores[:,0] <= threshold, -1, ood_predictions)\n",
    "            id_classifications = np.where(id_scores[:,0] <= threshold, -1, id_predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid score_type. Use 'energy' or 'softmax'.\")\n",
    "\n",
    "        # Combine OOD and ID classifications and true labels\n",
    "        all_predictions = np.concatenate([ood_classifications, id_classifications])\n",
    "        all_true_labels = np.concatenate([ood_true_labels, id_true_labels])\n",
    "\n",
    "        # Evaluate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels, all_predictions, labels=[-1, 0], average='macro', zero_division=0)\n",
    "        accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Find the best thresholds for each metric\n",
    "    best_f1_index = np.argmax(f1_scores)\n",
    "    best_f1_threshold = thresholds[best_f1_index]\n",
    "\n",
    "    best_precision_index = np.argmax(precisions)\n",
    "    best_precision_threshold = thresholds[best_precision_index]\n",
    "\n",
    "    best_recall_index = np.argmax(recalls)\n",
    "    best_recall_threshold = thresholds[best_recall_index]\n",
    "\n",
    "    print(f\"Best F1 threshold: {best_f1_threshold}, F1 Score: {f1_scores[best_f1_index]}\")\n",
    "    print(f\"Best Precision threshold: {best_precision_threshold}, Precision: {precisions[best_precision_index]}\")\n",
    "    print(f\"Best Recall threshold: {best_recall_threshold}, Recall: {recalls[best_recall_index]}\")\n",
    "\n",
    "    # Plot metrics as functions of the threshold\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(thresholds, precisions, label='Precision', color='g')\n",
    "    plt.plot(thresholds, recalls, label='Recall', color='b')\n",
    "    plt.plot(thresholds, f1_scores, label='F1 Score', color='r')\n",
    "\n",
    "    # Add best threshold indicators\n",
    "    plt.axvline(x=best_f1_threshold, color='r', linestyle='--', label=f'Best F1 Threshold: {best_f1_threshold:.2f}')\n",
    "    plt.axvline(x=best_precision_threshold, color='g', linestyle='--', label=f'Best Precision Threshold: {best_precision_threshold:.2f}')\n",
    "    plt.axvline(x=best_recall_threshold, color='b', linestyle='--', label=f'Best Recall Threshold: {best_recall_threshold:.2f}')\n",
    "\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(f'Evaluation Metrics as Functions of Threshold ({score_type.capitalize()}-Based OOD Detection)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot confusion matrix\n",
    "    # Threshold value for the energy score\n",
    "    upper_threshold = best_f1_threshold  # Using the best F1 threshold from the previous calculation\n",
    "    if score_type == 'energy':\n",
    "        # Classifying OOD examples based on energy scores\n",
    "        ood_classifications = np.where(ood_energy_scores >= upper_threshold, -1,  # classified as OOD\n",
    "                                  np.where(ood_energy_scores < upper_threshold, 0, -1))  # classified as ID\n",
    "        # Classifying ID examples based on energy scores\n",
    "        id_classifications = np.where(id_energy_scores >= upper_threshold, -1,  # classified as OOD\n",
    "                                  np.where(id_energy_scores < upper_threshold, id_true_labels, -1))  # classified as ID\n",
    "    elif score_type == 'softmax':\n",
    "        # Classifying OOD examples based on softmax scores\n",
    "        ood_classifications = softmax_thresh_classifications(ood_scores, upper_threshold)\n",
    "\n",
    "        # Classifying ID examples based on softmax scores\n",
    "        id_classifications = softmax_thresh_classifications(id_scores, upper_threshold)\n",
    "    # Combine OOD and ID classifications and true labels\n",
    "    all_predictions = np.concatenate([ood_classifications, id_classifications])\n",
    "    all_true_labels = np.concatenate([ood_true_labels, id_true_labels])\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_true_labels, all_predictions, labels=[0, 1, -1])\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Shirt\", \"Pants\", \"OOD\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for OOD and ID Classification ({score_type.capitalize()}-Based)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return best_f1_threshold, best_precision_threshold, best_recall_threshold\n",
    "\n",
    "# Example usage\n",
    "# Assuming id_energy_scores, ood_energy_scores, id_true_labels, and test_labels are already defined\n",
    "best_f1_threshold, best_precision_threshold, best_recall_threshold = evaluate_ood_detection(id_energy_scores, ood_energy_scores, test_labels, test_predictions, ood_predictions, score_type='energy')\n",
    "best_f1_threshold, best_precision_threshold, best_recall_threshold = evaluate_ood_detection(id_softmax_scores, ood_softmax_scores, test_labels, test_predictions, ood_predictions, score_type='softmax')\n",
    "```\n",
    "\n",
    "## Limitations of our approach thus far\n",
    "\n",
    "-   Focus on single OOD class: More reliable/accurate thresholds can/should be obtained using a wider variety (more classes) and larger sample of OOD data. This is part of the challenge of OOD detection which is that space of OOD data is vast. **Possible exercise**: Redo thresholding using all remaining classes in dataset.\n",
    "\n",
    "## References and supplemental resources\n",
    "\n",
    "-   https://www.youtube.com/watch?v=hgLC9_9ZCJI\n",
    "-   Generalized Out-of-Distribution Detection: A Survey: https://arxiv.org/abs/2110.11334\n",
    "\n",
    "-   Energy-based OOD detection is a modern and more robust alternative to softmax-based methods, leveraging energy scores to improve separability between in-distribution and OOD data.\n",
    "-   By calculating an energy value for each input, these methods provide a more nuanced measure of compatibility between data and the model’s learned distribution.\n",
    "-   Non-linear visualizations, like UMAP, offer better insights into how OOD and ID data are represented in high-dimensional feature spaces compared to linear methods like PCA.\n",
    "-   PyTorch-OOD simplifies the implementation of energy-based and other OOD detection methods, making it accessible for real-world applications.\n",
    "-   While energy-based methods excel in many scenarios, challenges include tuning thresholds across diverse OOD classes and ensuring generalizability to unseen distributions.\n",
    "-   Transitioning to energy-based detection lays the groundwork for exploring training-time regularization and hybrid approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
