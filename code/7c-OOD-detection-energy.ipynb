{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc984f3",
   "metadata": {},
   "source": [
    "# OOD detection: energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805419b9",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- What are energy-based methods for out-of-distribution (OOD) detection, and how do they compare to softmax-based approaches?\n",
    "- How does the energy metric enhance separability between in-distribution and OOD data?\n",
    "- What are the challenges and limitations of energy-based OOD detection methods?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Understand the concept of energy-based OOD detection and its theoretical foundations.\n",
    "- Compare energy-based methods to softmax-based approaches, highlighting their strengths and limitations.\n",
    "- Learn how to implement energy-based OOD detection using tools like PyTorch-OOD.\n",
    "- Explore challenges in applying energy-based methods, including threshold tuning and generalizability to diverse OOD scenarios.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "# Example 2: Energy-Based OOD Detection\n",
    "\n",
    "Traditional approaches, such as softmax-based methods, rely on output probabilities to flag OOD data. While simple and intuitive, these methods often struggle to distinguish OOD data effectively in complex scenarios, especially in high-dimensional spaces.\n",
    "\n",
    "Energy-based OOD detection offers a modern and robust alternative. This \"output-based\" approach leverages the **energy score**, a scalar value derived from a model's output logits, to measure the compatibility between input data and the model's learned distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed89c5d-3d68-4405-9291-e12c6220c6c9",
   "metadata": {},
   "source": [
    "### Understanding energy scores\n",
    "\n",
    "To understand energy-based OOD detection, we start by defining the **energy function E(x)**, which measures how \"compatible\" an input x is with a model's learned distribution.\n",
    "\n",
    "### 1. Energy function\n",
    "For a given input x and output logits f(x) — the raw outputs of a neural network — the energy of x is defined as:  \n",
    "\n",
    "$$\n",
    "E(x) = -\\log \\left( \\sum_{k} \\exp(f_k(x)) \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- f_k(x) is the logit corresponding to class k,  \n",
    "- The sum is taken over all classes k.  \n",
    "\n",
    "This equation compresses the logits into a single scalar value: the energy score.  \n",
    "\n",
    "- Lower energy E(x) reflects **higher compatitibility**,  \n",
    "- Higher energy E(x) reflects **lower compatitibility**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f0469-7ca4-4e15-b9db-d553990e6418",
   "metadata": {},
   "source": [
    "### 2. Energy to probability\n",
    "Using the Gibbs distribution, the energy can be converted into a probability that reflects how likely x is under the model's learned distribution. The relationship is:  \n",
    "\n",
    "$$\n",
    "P(x) \\propto \\exp(-E(x))\n",
    "$$\n",
    "\n",
    "Here:  \n",
    "- Lower energy \\( E(x) \\) leads to a **higher probability**,  \n",
    "- Higher energy \\( E(x) \\) leads to a **lower probability**.  \n",
    "\n",
    "The exponential relationship ensures that even small differences in energy values translate to significant changes in probability. \n",
    "\n",
    "If your stakeholders or downstream tasks require interpretable confidence scores, a Gibbs-based probability might make the thresholding process more understandable and adaptable. However, the raw energy scores can be more sensitive to OOD data since they do not compress their values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45e474-a587-454a-bee9-7c487dcd6833",
   "metadata": {},
   "source": [
    "### 3. Why energy works better than softmax\n",
    "\n",
    "Softmax probabilities are computed as:  \n",
    "\n",
    "$$\n",
    "P(y = k \\mid x) = \\frac{\\exp(f_k(x))}{ \\sum_{j} \\exp(f_j(x))}\n",
    "$$\n",
    "\n",
    "The softmax function normalizes the logits \\( f(x) \\), squeezing the output into a range between 0 and 1. While this is useful for interpreting the model’s predictions as probabilities, it introduces **overconfidence** for OOD inputs. Specifically:\n",
    "- Even when none of the logits \\( f_k(x) \\) are strongly aligned with any class (e.g., low magnitudes for all logits), softmax still distributes the probabilities across the known classes.\n",
    "- The normalization ensures the total probability sums to 1, which can mask the uncertainty by making the scores appear confident for OOD inputs.\n",
    "\n",
    "Energy-based methods, on the other hand, do not normalize the logits into probabilities by default. Instead, the **energy score** summarizes the raw logits as:  \n",
    "\n",
    "$$\n",
    "E(x) = -\\log \\sum_{j} \\exp(f_j(x))\n",
    "$$\n",
    "\n",
    "#### Key difference: sensitivity to logits / no normalization\n",
    "\n",
    "- **Softmax**: The output probabilities are dominated by the largest logit relative to the others, even if all logits are small. This can produce overconfident predictions for OOD data because the softmax function distributes probabilities across known classes.\n",
    "- **Energy**: By summarizing the raw logits directly, energy scores provide a more nuanced view of the model’s uncertainty, without forcing outputs into an overconfident probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b8bc6-b160-44f8-8bab-32fd56ca0e04",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Energy E(x) directly measures compatibility with the model.  \n",
    "- Lower energy → Higher compatibility (in-distribution),  \n",
    "- Higher energy → Lower compatibility (OOD data).  \n",
    "- The exponential relationship ensures sensitivity to even small deviations, making energy-based detection more robust than softmax-based methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fd0dd-3158-45c7-a5b0-9b16326e1fb4",
   "metadata": {},
   "source": [
    "## Worked example: comparing softmax and energy\n",
    "In this hands-on example, we'll repeat the same investigation as before with a couple of adjustments:\n",
    "\n",
    "- Use CNN to train model\n",
    "- Compare both softmax and energy scores with respect to ID and OOD data. We can do this easily using the PyTorch-OOD library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6e5f5-8435-42b5-8df6-8cc056a991c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "def prep_ID_OOD_datasests(ID_class_labels, OOD_class_labels):\n",
    "    \"\"\"\n",
    "    Prepares in-distribution (ID) and out-of-distribution (OOD) datasets \n",
    "    from the Fashion MNIST dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - ID_class_labels: list or array-like, labels for the in-distribution classes.\n",
    "                       Example: [0, 1] for T-shirts (0) and Trousers (1).\n",
    "    - OOD_class_labels: list or array-like, labels for the out-of-distribution classes.\n",
    "                        Example: [5] for Sandals.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: np.array, training images for in-distribution classes.\n",
    "    - test_data: np.array, test images for in-distribution classes.\n",
    "    - ood_data: np.array, test images for out-of-distribution classes.\n",
    "    - train_labels: np.array, labels corresponding to the training images.\n",
    "    - test_labels: np.array, labels corresponding to the test images.\n",
    "    - ood_labels: np.array, labels corresponding to the OOD test images.\n",
    "\n",
    "    Notes:\n",
    "    - The function filters images based on provided class labels for ID and OOD.\n",
    "    - Outputs include images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    # Load Fashion MNIST dataset\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    \n",
    "    # Prepare OOD data: Sandals = 5\n",
    "    ood_filter = np.isin(test_labels, OOD_class_labels)\n",
    "    ood_data = test_images[ood_filter]\n",
    "    ood_labels = test_labels[ood_filter]\n",
    "    print(f'ood_data.shape={ood_data.shape}')\n",
    "    \n",
    "    # Filter data for T-shirts (0) and Trousers (1) as in-distribution\n",
    "    train_filter = np.isin(train_labels, ID_class_labels)\n",
    "    test_filter = np.isin(test_labels, ID_class_labels)\n",
    "    \n",
    "    train_data = train_images[train_filter]\n",
    "    train_labels = train_labels[train_filter]\n",
    "    print(f'train_data.shape={train_data.shape}')\n",
    "    \n",
    "    test_data = test_images[test_filter]\n",
    "    test_labels = test_labels[test_filter]\n",
    "    print(f'test_data.shape={test_data.shape}')\n",
    "\n",
    "    return train_data, test_data, ood_data, train_labels, test_labels, ood_labels\n",
    "\n",
    "\n",
    "def plot_data_sample(train_data, ood_data):\n",
    "    \"\"\"\n",
    "    Plots a sample of in-distribution and OOD data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: np.array, array of in-distribution data images\n",
    "    - ood_data: np.array, array of out-of-distribution data images\n",
    "\n",
    "    Returns:\n",
    "    - fig: matplotlib.figure.Figure, the figure object containing the plots\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    N_samples = 7\n",
    "    for i in range(N_samples):\n",
    "        plt.subplot(2, N_samples, i + 1)\n",
    "        plt.imshow(train_data[i], cmap='gray')\n",
    "        plt.title(\"In-Dist\")\n",
    "        plt.axis('off')\n",
    "    for i in range(N_samples):\n",
    "        plt.subplot(2, N_samples, i + N_samples+1)\n",
    "        plt.imshow(ood_data[i], cmap='gray')\n",
    "        plt.title(\"OOD\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, ood_data, train_labels, test_labels, ood_labels = prep_ID_OOD_datasests([0,1], [5]) #list(range(2,10)) use remaining 8 classes in dataset as OOD\n",
    "fig = plot_data_sample(train_data, ood_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b2422",
   "metadata": {},
   "source": [
    "## Visualizing OOD and ID data\n",
    "\n",
    "\n",
    "### UMAP (or similar)\n",
    "\n",
    "Recall in our previous example, we used PCA to visualize the ID and OOD data distributions. This was appropriate given that we were evaluating OOD/ID data in the context of a linear model. However, when working with nonlinear models such as CNNs, it makes more sense to investigate how the data is represented in a nonlinear space. Nonlinear embedding methods, such as Uniform Manifold Approximation and Projection (UMAP), are more suitable in such scenarios. \n",
    "\n",
    "UMAP  is a non-linear dimensionality reduction technique that preserves both the global structure and the local neighborhood relationships in the data. UMAP is often better at maintaining the continuity of data points that lie on non-linear manifolds. It can reveal nonlinear patterns and structures that PCA might miss, making it a valuable tool for analyzing ID and OOD distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap = True \n",
    "if plot_umap:\n",
    "    import umap\n",
    "    # Flatten images for PCA and logistic regression\n",
    "    train_data_flat = train_data.reshape((train_data.shape[0], -1))\n",
    "    test_data_flat = test_data.reshape((test_data.shape[0], -1))\n",
    "    ood_data_flat = ood_data.reshape((ood_data.shape[0], -1))\n",
    "    \n",
    "    print(f'train_data_flat.shape={train_data_flat.shape}')\n",
    "    print(f'test_data_flat.shape={test_data_flat.shape}')\n",
    "    print(f'ood_data_flat.shape={ood_data_flat.shape}')\n",
    "    \n",
    "    # Perform UMAP to visualize the data\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    combined_data = np.vstack([train_data_flat, ood_data_flat])\n",
    "    combined_labels = np.hstack([train_labels, np.full(ood_data_flat.shape[0], 2)])  # Use 2 for OOD class\n",
    "    \n",
    "    umap_results = umap_reducer.fit_transform(combined_data)\n",
    "    \n",
    "    # Split the results back into in-distribution and OOD data\n",
    "    umap_in_dist = umap_results[:len(train_data_flat)]\n",
    "    umap_ood = umap_results[len(train_data_flat):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb9cb2",
   "metadata": {},
   "source": [
    "The warning message indicates that UMAP has overridden the n_jobs parameter to 1 due to the random_state being set. This behavior ensures reproducibility by using a single job. If you want to avoid the warning and still use parallelism, you can remove the random_state parameter. However, removing random_state will mean that the results might not be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_umap:\n",
    "    umap_alpha = .1\n",
    "\n",
    "    # Plotting UMAP components\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot in-distribution data\n",
    "    scatter1 = plt.scatter(umap_in_dist[train_labels == 0, 0], umap_in_dist[train_labels == 0, 1], c='blue', label='T-shirts (ID)', alpha=umap_alpha)\n",
    "    scatter2 = plt.scatter(umap_in_dist[train_labels == 1, 0], umap_in_dist[train_labels == 1, 1], c='red', label='Trousers (ID)', alpha=umap_alpha)\n",
    "    \n",
    "    # Plot OOD data\n",
    "    scatter3 = plt.scatter(umap_ood[:, 0], umap_ood[:, 1], c='green', label='OOD', edgecolor='k', alpha=umap_alpha)\n",
    "    \n",
    "    # Create a single legend for all classes\n",
    "    plt.legend(handles=[scatter1, scatter2, scatter3], loc=\"upper right\")\n",
    "    plt.xlabel('First UMAP Component')\n",
    "    plt.ylabel('Second UMAP Component')\n",
    "    plt.title('UMAP of In-Distribution and OOD Data')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4145b74-df3a-4875-9e75-2d60304a4a43",
   "metadata": {},
   "source": [
    "With UMAP, we see our data clusters into more meaningful groups (compared to PCA). Our nonlinear model should hopefully have no problem separating these three clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d12c25",
   "metadata": {},
   "source": [
    "### Preparing data for CNN\n",
    "Next, we'll prepare our data for a pytorch (torch) CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ad1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to PyTorch tensors and normalize\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "ood_data_tensor = torch.tensor(ood_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# TensorDataset provides a convenient way to couple input data with their corresponding labels, making it easier to pass them into a DataLoader.\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "ood_dataset = torch.utils.data.TensorDataset(ood_data_tensor, torch.zeros(ood_data_tensor.shape[0], dtype=torch.long))\n",
    "\n",
    "# DataLoader is used to efficiently load and manage batches of data\n",
    "# - It provides iterators over the data for training/testing.\n",
    "# - Supports options like batch size, shuffling, and parallel data loading\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fcc7d-ebf8-482f-a722-45ff8b5e531a",
   "metadata": {},
   "source": [
    "### Define CNN class\n",
    "Next, we'll define a simple Convolutional Neural Network (CNN) to classify in-distribution (ID) data. This CNN will serve as the backbone for our experiments, enabling us to analyze its predictions on both ID and OOD data. The model will include convolutional layers for feature extraction and fully connected layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9307b-e3d1-4273-a26f-3a33d3620716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 128)  # Updated this line\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64*5*5)  # Updated this line\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910348fe-e853-4366-98f6-aa254c3170b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = SimpleCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffa2b2-e43d-4d30-99bb-dba2898603b5",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca1da4-1732-4541-a671-4a33de708578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    \"\"\"\n",
    "    Trains a given PyTorch model using a specified dataset, loss function, and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The neural network model to train.\n",
    "    - train_loader (DataLoader): DataLoader object providing the training dataset in batches.\n",
    "    - criterion (nn.Module): Loss function used for optimization (e.g., CrossEntropyLoss).\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for adjusting model weights (e.g., Adam, SGD).\n",
    "    - epochs (int): Number of training iterations over the entire dataset.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the loss for each epoch during training.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterate over the dataset for the given number of epochs.\n",
    "    2. For each batch, forward propagate inputs, compute the loss, and backpropagate gradients.\n",
    "    3. Update model weights using the optimizer and reset gradients after each step.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the appropriate device (CPU or GPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients from the previous step to avoid accumulation\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Compute model predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute the loss between predictions and true labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass: Compute gradients of the loss w.r.t. model parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update model weights using gradients and optimizer rules\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the batch loss for reporting\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d09c4b-27b4-4b38-b2df-9ed24572317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591769f-c784-4a5f-86c0-55cb27ff83e6",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e047d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(labels, predictions, title):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix for a classification task.\n",
    "\n",
    "    Parameters:\n",
    "    - labels (array-like): True labels for the dataset.\n",
    "    - predictions (array-like): Model-predicted labels.\n",
    "    - title (str): Title for the confusion matrix plot.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions, labels=[0, 1])\n",
    "    \n",
    "    # Create a display object for the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"T-shirt/top\", \"Trouser\"])\n",
    "    \n",
    "    # Plot the confusion matrix with a color map\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de011be7-1580-41b3-957f-6a1a1b746816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on a given dataset\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates a PyTorch model on a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained PyTorch model to evaluate.\n",
    "    - dataloader (torch.utils.data.DataLoader): DataLoader object providing the dataset in batches.\n",
    "    - device (torch.device): Device on which to perform the evaluation (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "    - all_labels (np.array): True labels for the entire dataset.\n",
    "    - all_predictions (np.array): Model predictions for the entire dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []  # To store true labels\n",
    "    all_predictions = []  # To store model predictions\n",
    "    \n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass to get model outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Get predicted class labels (index with the highest probability)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Append true labels and predictions to the lists\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Convert lists to NumPy arrays for easier processing\n",
    "    return np.array(all_labels), np.array(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c58161-159c-4aa3-b2c5-1152e07fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_labels, test_predictions = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Plot confusion matrix for test dataset\n",
    "plot_confusion_matrix(test_labels, test_predictions, \"Confusion Matrix for Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe1b53-8ff5-4900-8bbd-94f6b0179ed8",
   "metadata": {},
   "source": [
    "#### Comparing softmax vs energy scores\n",
    "Let's take a look at both the softmax and energy scores generated by both the ID test set and the OOD data we extracted earlier.\n",
    "\n",
    "With PyTorch-OOD, we can easily calculate both measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef1246-d8f5-4204-8553-4ef35f0908df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Computing softmax scores\n",
    "from pytorch_ood.detector import MaxSoftmax\n",
    "\n",
    "# Initialize the softmax-based OOD detector\n",
    "softmax_detector = MaxSoftmax(model)\n",
    "\n",
    "# Compute softmax scores\n",
    "def get_OOD_scores(detector, dataloader):\n",
    "    \"\"\"\n",
    "    Computes softmax-based scores for a given OOD detector and dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - detector: An initialized OOD detector (e.g., MaxSoftmax).\n",
    "    - dataloader: DataLoader providing the dataset for which scores are to be computed.\n",
    "\n",
    "    Returns:\n",
    "    - scores: A NumPy array of softmax scores for all data points.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    detector.model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)  # Move inputs to the correct device\n",
    "            score = detector.predict(inputs)  # Get the max softmax score\n",
    "            scores.extend(score.cpu().numpy())  # Move scores to CPU and convert to NumPy array\n",
    "    return np.array(scores)\n",
    "\n",
    "# Compute softmax scores for ID and OOD data\n",
    "id_softmax_scores = get_OOD_scores(softmax_detector, test_loader)\n",
    "ood_softmax_scores = get_OOD_scores(softmax_detector, ood_loader)\n",
    "\n",
    "id_softmax_scores # values are negative to align with other OOD measures, such as energy (more negative is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfda2a-82ec-4dfa-a91c-695f07ee4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Computing energy\n",
    "from pytorch_ood.detector import EnergyBased\n",
    "\n",
    "# Initialize the energy-based OOD detector\n",
    "energy_detector = EnergyBased(model)\n",
    "\n",
    "id_energy_scores = get_OOD_scores(energy_detector, test_loader)\n",
    "ood_energy_scores = get_OOD_scores(energy_detector, ood_loader)\n",
    "id_energy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2401f7b-1c3a-4d87-8d3f-15006c14ce71",
   "metadata": {},
   "source": [
    "### Plot probability densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab8ca9-3e96-4ea6-b6d0-1900b0bb5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot PSDs\n",
    "\n",
    "# Function to plot PSD\n",
    "def plot_psd(id_scores, ood_scores, method_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    alpha = 0.3\n",
    "\n",
    "    # Plot PSD for ID scores\n",
    "    id_density = gaussian_kde(id_scores)\n",
    "    x_id = np.linspace(id_scores.min(), id_scores.max(), 1000)\n",
    "    plt.plot(x_id, id_density(x_id), label=f'ID ({method_name})', color='blue', alpha=alpha)\n",
    "\n",
    "    # Plot PSD for OOD scores\n",
    "    ood_density = gaussian_kde(ood_scores)\n",
    "    x_ood = np.linspace(ood_scores.min(), ood_scores.max(), 1000)\n",
    "    plt.plot(x_ood, ood_density(x_ood), label=f'OOD ({method_name})', color='red', alpha=alpha)\n",
    "\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Probability Density Distributions for {method_name} Scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot PSD for softmax scores\n",
    "plot_psd(id_softmax_scores, ood_softmax_scores, 'Softmax')\n",
    "\n",
    "# Plot PSD for energy scores\n",
    "plot_psd(id_energy_scores, ood_energy_scores, 'Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bbd26",
   "metadata": {},
   "source": [
    "## Recap and limitations\n",
    "The energy-based approach for out-of-distribution (OOD) detection has several strengths, particularly its ability to effectively separate in-distribution (ID) and OOD data by leveraging the raw logits of a model. However, it is not without limitations. Here are the key drawbacks:\n",
    "\n",
    "1. Dependence on well-defined classes: Energy scores rely on logits that correspond to clearly defined classes in the model. If the model's logits are not well-calibrated or if the task involves ambiguous or overlapping classes, the energy scores may not provide reliable OOD separation.\n",
    "2. Energy thresholds tuned on one dataset may not generalize well to other datasets or domains (depending on how expansive/variable your OOD calibration set is)\n",
    "\n",
    "## References and supplemental resources\n",
    "\n",
    "* https://www.youtube.com/watch?v=hgLC9_9ZCJI\n",
    "* Generalized Out-of-Distribution Detection: A Survey: https://arxiv.org/abs/2110.11334\n",
    "\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- Energy-based OOD detection is a modern and more robust alternative to softmax-based methods, leveraging energy scores to improve separability between in-distribution and OOD data.\n",
    "- By calculating an energy value for each input, these methods provide a more nuanced measure of compatibility between data and the model's learned distribution.\n",
    "- Non-linear visualizations, like UMAP, offer better insights into how OOD and ID data are represented in high-dimensional feature spaces compared to linear methods like PCA.\n",
    "- PyTorch-OOD simplifies the implementation of energy-based and other OOD detection methods, making it accessible for real-world applications.\n",
    "- While energy-based methods excel in many scenarios, challenges include tuning thresholds across diverse OOD classes and ensuring generalizability to unseen distributions.\n",
    "- Transitioning to energy-based detection lays the groundwork for exploring training-time regularization and hybrid approaches.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustworthy_ML",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
