{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26afb7e-86b1-4333-b547-e20bae0192ab",
   "metadata": {},
   "source": [
    "-   Why is model sharing important in the context of reproducibility and responsible use?\n",
    "-   What are the challenges, risks, and ethical considerations related to sharing models?\n",
    "-   How can model-sharing best practices be applied using tools like model cards and the Hugging Face platform?\n",
    "\n",
    "-   Understand the importance of model sharing and best practices to ensure reproducibility and responsible use of models.\n",
    "-   Understand the challenges, risks, and ethical concerns associated with model sharing.\n",
    "-   Apply model-sharing best practices through using model cards and the Hugging Face platform.\n",
    "\n",
    "-   Model cards are the standard technique for communicating information about how machine learning systems were trained and how they should and should not be used.\n",
    "-   Models can be shared and reused via the Hugging Face platform.\n",
    "\n",
    "### Why should we share trained models?\n",
    "\n",
    "Discuss in small groups and report out: *Why do you believe it is or isn’t important to share ML models? How has model-sharing contributed to your experiences or projects?*\n",
    "\n",
    "### Solution\n",
    "\n",
    "-   **Accelerating research**: Sharing models allows researchers and practitioners to build upon existing work, accelerating the pace of innovation in the field.\n",
    "-   **Knowledge exchange**: Model sharing promotes knowledge exchange and collaboration within the machine learning community, fostering a culture of open science.\n",
    "-   **Reproducibility**: Sharing models, along with associated code and data, enhances reproducibility, enabling others to validate and verify the results reported in research papers.\n",
    "-   **Benchmarking**: Shared models serve as benchmarks for comparing new models and algorithms, facilitating the evaluation and improvement of state-of-the-art techniques.\n",
    "-   **Education / Accessibility to state-of-the-art architectures**: Shared models provide valuable resources for educational purposes, allowing students and learners to explore and experiment with advanced machine learning techniques.\n",
    "-   **Repurpose (transfer learning and finetuning)**: Some models (i.e., foundation models) can be repurposed for a wide variety of tasks. This is especially useful when working with limited data.\n",
    "    Data scarcity\n",
    "-   **Resource efficiency**: Instead of training a model from the ground up, practitioners can use existing models as a starting point, saving time, computational resources, and energy.\n",
    "\n",
    "### Challenges and risks of model sharing\n",
    "\n",
    "Discuss in small groups and report out: *What are some potential challenges, risks, or ethical concerns associated with model sharing and reproducing ML workflows?*\n",
    "\n",
    "### Solution\n",
    "\n",
    "-   **Privacy concerns**: Sharing models that were trained on sensitive or private data raises privacy concerns. The potential disclosure of personal information through the model poses a risk to individuals and can lead to unintended consequences.\n",
    "-   **Informed consent**: If models involve user data, ensuring informed consent is crucial. Sharing models trained on user-generated content without clear consent may violate privacy norms and regulations.\n",
    "-   **Intellectual property**: Models may be developed within organizations with proprietary data and methodologies. Sharing such models without proper consent or authorization may lead to intellectual property disputes and legal consequences.\n",
    "-   **Model robustness and generalization**: Reproduced models may not generalize well to new datasets or real-world scenarios. Failure to account for the limitations of the original model can result in reduced performance and reliability in diverse settings.\n",
    "-   **Lack of reproducibility**: Incomplete documentation, missing details, or changes in dependencies over time can hinder the reproducibility of ML workflows. This lack of reproducibility can impede scientific progress and validation of research findings.\n",
    "-   **Unintended use and misuse**: Shared models may be used in unintended ways, leading to ethical concerns. Developers should consider the potential consequences of misuse, particularly in applications with societal impact, such as healthcare or law enforcement.\n",
    "-   **Responsible AI considerations**: Ethical considerations, such as fairness, accountability, and transparency, should be addressed during model sharing. Failing to consider these aspects can result in models that inadvertently discriminate or lack interpretability. Models used for decision-making, especially in critical areas like healthcare or finance, should be ethically deployed. Transparent documentation and disclosure of how decisions are made are essential for responsible AI adoption.\n",
    "\n",
    "## Saving the model locally\n",
    "\n",
    "Let’s review the simplest method for sharing a model first — saving the model locally. When working with PyTorch, it’s important to know how to save and load models efficiently. This process ensures that you can pause your work, share your models, or deploy them for inference without having to retrain them from scratch each time.\n",
    "\n",
    "### Defining the model\n",
    "\n",
    "As an example, we’ll configure a simple perceptron (single hidden layer) in PyTorch. We’ll define a bare bones class for this just so we can initialize the model.\n",
    "\n",
    "``` python\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, config: Dict[str, int]):\n",
    "        super().__init__()\n",
    "        # Parameter is a trainable tensor initialized with random values\n",
    "        self.param = nn.Parameter(torch.rand(config[\"num_channels\"], config[\"hidden_size\"]))\n",
    "        # Linear layer (fully connected layer) for the output\n",
    "        self.linear = nn.Linear(config[\"hidden_size\"], config[\"num_classes\"])\n",
    "        # Store the configuration\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Forward pass: Add the input to the param tensor, then pass through the linear layer\n",
    "        return self.linear(x + self.param)\n",
    "```\n",
    "\n",
    "Initialize model by calling the class with configuration settings.\n",
    "\n",
    "``` python\n",
    "# Create model instance with specific configuration\n",
    "config = {\"num_channels\": 3, \"hidden_size\": 32, \"num_classes\": 10}\n",
    "model = MyModel(config=config)\n",
    "```\n",
    "\n",
    "We can then write a function to save out the model. We’ll need both the model weights and the model’s configuration (hyperparameter settings). We’ll save the configurations as a json since a key/value format is convenient here.\n",
    "\n",
    "``` python\n",
    "import json\n",
    "\n",
    "# Function to save model and config locally\n",
    "def save_model(model: nn.Module, model_path: str, config_path: str) -> None:\n",
    "    # Save model state dict (weights and biases) as a .pth file\n",
    "    torch.save(model.state_dict(), model_path) #\n",
    "    # Save config\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(model.config, f)\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Save the model and config locally\n",
    "save_model(model, \"my_awesome_model.pth\", \"my_awesome_model_config.json\")\n",
    "```\n",
    "\n",
    "To load the model back in, we can write another function\n",
    "\n",
    "``` python\n",
    "# Function to load model and config locally\n",
    "def load_model(model_class: Any, model_path: str, config_path: str) -> nn.Module:\n",
    "    # Load config\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    # Create model instance with config\n",
    "    model = model_class(config=config)\n",
    "    # Load model state dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Load the model and config locally\n",
    "loaded_model = load_model(MyModel, \"my_awesome_model.pth\", \"my_awesome_model_config.json\")\n",
    "\n",
    "# Verify the loaded model\n",
    "print(loaded_model)\n",
    "```\n",
    "\n",
    "## Saving a model to Hugging Face\n",
    "\n",
    "To share your model with a wider audience, we recommend uploading your model to Hugging Face. Hugging Face is a very popular machine learning (ML) platform and community that helps users build, deploy, share, and train machine learning models. It has quickly become the go-to option for sharing models with the public.\n",
    "\n",
    "### Create a Hugging Face account and access Token\n",
    "\n",
    "If you haven’t completed these steps from the setup, make sure to do this now.\n",
    "\n",
    "**Create account**: To create an account on Hugging Face, visit: [huggingface.co/join](https://huggingface.co/join). Enter an email address and password, and follow the instructions provided via Hugging Face (you may need to verify your email address) to complete the process.\n",
    "\n",
    "**Setup access token**: Once you have your account created, you’ll need to generate an access token so that you can upload/share models to your Hugging Face account during the workshop. To generate a token, visit the [Access Tokens setting page](https://huggingface.co/settings/tokens) after logging in.\n",
    "\n",
    "### Login to Hugging Face account\n",
    "\n",
    "To login, you will need to retrieve your access token from the [Access Tokens setting page](https://huggingface.co/settings/tokens)\n",
    "\n",
    "``` python\n",
    "!huggingface-cli login\n",
    "```\n",
    "\n",
    "You might get a message saying you cannot authenticate through git-credential as no helper is defined on your machine. This warning message should not stop you from being able to complete this episode, but it may mean that the token won’t be stored on your machine for future use.\n",
    "\n",
    "Once logged in, we will need to edit our model class definition to include Hugging Face’s “push_to_hub” attribute. To enable the push_to_hub functionality, you’ll need to include the PyTorchModelHubMixin “mixin class” provided by the huggingface_hub library. A mixin class is a type of class used in object-oriented programming to “mix in” additional properties and methods into a class. The PyTorchModelHubMixin class adds methods to your PyTorch model to enable easy saving and loading from the Hugging Face Model Hub.\n",
    "\n",
    "Here’s how you can adjust the code to incorporate both saving/loading locally and pushing the model to the Hugging Face Hub.\n",
    "\n",
    "``` python\n",
    "from huggingface_hub import PyTorchModelHubMixin # NEW\n",
    "\n",
    "class MyModel(nn.Module, PyTorchModelHubMixin): # PyTorchModelHubMixin is new\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        # Initialize layers and parameters\n",
    "        self.param = nn.Parameter(torch.rand(config[\"num_channels\"], config[\"hidden_size\"]))\n",
    "        self.linear = nn.Linear(config[\"hidden_size\"], config[\"num_classes\"])\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x + self.param)\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Create model instance with specific configuration\n",
    "config = {\"num_channels\": 3, \"hidden_size\": 32, \"num_classes\": 10}\n",
    "model = MyModel(config=config)\n",
    "print(model)\n",
    "```\n",
    "\n",
    "``` python\n",
    "# push to the hub\n",
    "model.push_to_hub(\"my-awesome-model\", config=config)\n",
    "```\n",
    "\n",
    "**Verifying**: To check your work, head back over to your Hugging Face account and click your profile icon in the top-right of the website. Click “Profile” from there to view all of your uploaded models. Alternatively, you can search for your username (or model name) from the [Model Hub](https://huggingface.co/models).\n",
    "\n",
    "#### Loading the model from Hugging Face\n",
    "\n",
    "``` python\n",
    "# reload\n",
    "model = MyModel.from_pretrained(\"your-username/my-awesome-model\")\n",
    "```\n",
    "\n",
    "## Uploading transformer models to Hugging Face\n",
    "\n",
    "Key Differences\n",
    "\n",
    "-   **Saving and Loading the Tokenizer**: Transformer models require a tokenizer that needs to be saved and loaded with the model. This is not necessary for custom PyTorch models that typically do not require a separate tokenizer.\n",
    "-   **Using Pre-trained Classes**: Transformer models use classes like AutoModelForSequenceClassification and AutoTokenizer from the transformers library, which are pre-built and designed for specific tasks (e.g., sequence classification).\n",
    "-   **Methods for Saving and Loading**: The transformers library provides save_pretrained and from_pretrained methods for both models and tokenizers, which handle the serialization and deserialization processes seamlessly.\n",
    "\n",
    "``` python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained(\"my_transformer_model\")\n",
    "tokenizer.save_pretrained(\"my_transformer_model\")\n",
    "\n",
    "# Load the model and tokenizer from the saved directory\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"my_transformer_model\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"my_transformer_model\")\n",
    "\n",
    "# Verify the loaded model and tokenizer\n",
    "print(loaded_model)\n",
    "print(loaded_tokenizer)\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Push the model and tokenizer to Hugging Face Hub\n",
    "model.push_to_hub(\"my-awesome-transformer-model\")\n",
    "tokenizer.push_to_hub(\"my-awesome-transformer-model\")\n",
    "\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "hub_model = AutoModelForSequenceClassification.from_pretrained(\"user-name/my-awesome-transformer-model\")\n",
    "hub_tokenizer = AutoTokenizer.from_pretrained(\"user-name/my-awesome-transformer-model\")\n",
    "\n",
    "# Verify the model and tokenizer loaded from the hub\n",
    "print(hub_model)\n",
    "print(hub_tokenizer)\n",
    "```\n",
    "\n",
    "### What pieces must be well-documented to ensure reproducible and responsible model sharing?\n",
    "\n",
    "Discuss in small groups and report out: *What type of information needs to be included in the documentation when sharing a model?*\n",
    "\n",
    "### Solution\n",
    "\n",
    "-   Environment setup\n",
    "-   Training data\n",
    "    -   How the data was collected\n",
    "    -   Who owns the data: data license and usage terms\n",
    "    -   Basic descriptive statistics: number of samples, features, classes, etc.\n",
    "    -   Note any class imbalance or general bias issues\n",
    "    -   Description of data distribution to help prevent out-of-distribution failures.\n",
    "-   Preprocessing steps.\n",
    "    -   Data splitting\n",
    "    -   Standardization method\n",
    "    -   Feature selection\n",
    "    -   Outlier detection and other filters\n",
    "-   Model architecture, hyperparameters and, training procedure (e.g., dropout or early stopping)\n",
    "-   Model weights\n",
    "-   Evaluation metrics. Results and performance. The more tasks/datasets you can evaluate on, the better.\n",
    "-   Ethical considerations: Include investigations of bias/fairness when applicable (i.e., if your model involves human data or affects decision-making involving humans)\n",
    "-   Contact info\n",
    "-   Acknowledgments\n",
    "-   Examples and demos (highly recommended)\n",
    "\n",
    "### Document your model\n",
    "\n",
    "For this challenge, you have two options:\n",
    "\n",
    "1.  Start writing a model card for a model you have created for your research. The solution from the previous challenge or this [template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md) from Hugging Face are good places to start, but note that not all fields may be relevant, depending on what your model does.\n",
    "\n",
    "2.  Find a model on [HuggingFace](https://huggingface.co/) that has a model card, for example, you could search for models using terms like “sentiment classification” or “medical”. Read the model card and evaluate whether the information is clear and complete. Would you be able to recreate the model based on the information presented? Do you feel that there is enough information to be able to evaluate you would be able to adapt this model for your purposes? You can refer to the previous challenge’s solution for ideas of what information should be included, but note that not all sections are relevant to all models.\n",
    "\n",
    "### Solution\n",
    "\n",
    "Pair up with a classmate and discuss what you wrote/read. Do model cards seem like a useful tool for you moving forwards?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
