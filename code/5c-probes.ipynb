{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac1dbcb",
   "metadata": {},
   "source": [
    "# Explainability methods: Linear Probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce6b26",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can probing classifiers help us understand what a model has learned?  \n",
    "- What are the limitations of probing classifiers, and how can they be addressed?  \n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Understand the concept of probing classifiers and how they assess the representations learned by models.  \n",
    "- Gain familiarity with the PyTorch and HuggingFace libraries, for using and evaluating language models.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "## What part of my model causes this prediction?\n",
    "\n",
    "When a model makes a correct prediction on a task it has been trained on (known as a 'downstream task'), \n",
    "**[Probing classifiers](https://direct.mit.edu/coli/article/48/1/207/107571/Probing-Classifiers-Promises-Shortcomings-and)** can be used to identify if the model actually contains the relevant information or knowledge required \n",
    "to make that prediction, or if it is just making a lucky guess.\n",
    "Furthermore, probes can be used to identify the specific components of the model that contain this relevant information, \n",
    "providing crucial insights for developing better models over time.\n",
    "\n",
    "#### Method and Examples\n",
    "\n",
    "A neural network takes its input as a series of vectors, or representations, and transforms them through a series of layers to produce an output.\n",
    "The job of the main body of the neural network is to develop representations that are as useful for the downstream task as possible, \n",
    "so that the final few layers of the network can make a good prediction.\n",
    "\n",
    "This essentially means that a good quality representation is one that _already_ contains all the information required to make a good prediction. \n",
    "In other words, the features or representations from the model are easily separable by a simple classifier. And that classifier is what we call \n",
    "a 'probe'. A probe is a simple model that uses the representations of the model as input, and tries to learn the downstream task from them.\n",
    "The probe itself is designed to be too easy to learn the task on its own. This means, that the only way the probe get perform well on this task is if \n",
    "the representations it is given are already good enough to make the prediction.\n",
    "\n",
    "These representations can be taken from any part of the model. Generally, using representations from the last layer of a neural network help identify if\n",
    "the model even contains the information to make predictions for the downstream task. \n",
    "However, this can be extended further: probing the representations from different layers of the model can help identify where in the model the\n",
    "information is stored, and how it is transformed through the model.\n",
    "\n",
    "Probes have been frequently used in the domain of NLP, where they have been used to check if language models contain certain kinds of linguistic information. \n",
    "These probes can be designed with varying levels of complexity. For example, simple probes have shown language models to contain information \n",
    "about simple syntactical features like [Part of Speech tags](https://aclanthology.org/D15-1246.pdf), and more complex probes have shown models to contain entire [Parse trees](https://aclanthology.org/N19-1419.pdf) of sentences.\n",
    "\n",
    "#### Limitations and Extensions\n",
    "\n",
    "One large challenge in using probes is identifying the correct architectural design of the probe. Too simple, and \n",
    "it may not be able to learn the downstream task at all. Too complex, and it may be able to learn the task even if the \n",
    "model does not contain the information required to make the prediction.\n",
    "\n",
    "Another large limitation is that even if a probe is able to learn the downstream task, it does not mean that the model\n",
    "is actually using the information contained in the representations to make the prediction. \n",
    "So essentially, a probe can only tell us if a part of the model _can_ make the prediction, not if it _does_ make the prediction.\n",
    "\n",
    "A new approach known as **[Causal Tracing](https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html)** \n",
    "addresses this limitation. The objective of this approach is similar to probes: attempting to understand which part of a model contains \n",
    "information relevant to a downstream task. The approach involves iterating through all parts of the model being examined (e.g. all layers\n",
    "of a model), and disrupting the information flow through that part of the model. (This could be as easy as adding some kind of noise on top of the \n",
    "weights of that model component). If the model performance on the downstream task suddenly drops on disrupting a specific model component, \n",
    "we know for sure that that component not only contains the information required to make the prediction, but that the model is actually using that\n",
    "information to make the prediction.\n",
    "\n",
    "## Implementing your own Probe\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # This is needed to avoid a warning from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899117b",
   "metadata": {},
   "source": [
    "Now, let's set the random seed to ensure reproducibility. Setting random seeds is like setting a starting point for your machine learning adventure. It ensures that every time you train your model, it starts from the same place, using the same random numbers, making your results consistent and comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility - pick any number of your choice to set the seed. We use 42, since that is the answer to everything, after all.\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1161b85",
   "metadata": {},
   "source": [
    "##### Loading the Dataset\n",
    "Let's load our data: the IMDB Movie Review dataset. The dataset contains text reviews and their corresponding sentiment labels (positive or negative). \n",
    "The label 1 corresponds to a positive review, and 0 corresponds to a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_dataset(keep_samples: int = 100) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    '''\n",
    "    Load the IMDB dataset from huggingface.\n",
    "    The dataset contains text reviews and their corresponding sentiment labels (positive or negative).\n",
    "    The label 1 corresponds to a positive review, and 0 corresponds to a negative review.\n",
    "    :param keep_samples: Number of samples to keep, for faster training.\n",
    "    :return: train, dev, test datasets. Each can be treated as a dictionary with keys 'text' and 'label'.\n",
    "    '''\n",
    "    dataset = load_dataset('imdb')\n",
    "\n",
    "    # Keep only a subset of the data for faster training\n",
    "    train_dataset = Dataset.from_dict(dataset['train'].shuffle(seed=42)[:keep_samples])\n",
    "    dev_dataset = Dataset.from_dict(dataset['test'].shuffle(seed=42)[:keep_samples])\n",
    "    test_dataset = Dataset.from_dict(dataset['test'].shuffle(seed=42)[keep_samples:2*keep_samples])\n",
    "\n",
    "    # train_dataset[0] will return {'text': ...., 'label': 0}\n",
    "    logging.info(f'Loaded IMDB dataset: {len(train_dataset)} training samples, {len(dev_dataset)} dev samples, {len(test_dataset)} test samples.')\n",
    "    return train_dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset, test_dataset = load_imdb_dataset(keep_samples=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bbabc",
   "metadata": {},
   "source": [
    "##### Loading the Model\n",
    "\n",
    "We will load a model from huggingface, and use this model to get the embeddings for the probe.\n",
    "We use distilBERT for this example, but feel free to explore other models from huggingface after the exercise.\n",
    "\n",
    "BERT is a transformer-based model, and is known to perform well on a variety of NLP tasks.\n",
    "The model is pre-trained on a large corpus of text, and can be fine-tuned for specific tasks.\n",
    "distilBERT is a lightweight version of the model, created through a process known as [distillation](https://en.wikipedia.org/wiki/Knowledge_distillation#:~:text=In%20machine%20learning%2C%20knowledge%20distillation,might%20not%20be%20fully%20utilized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e179a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str) -> Tuple[AutoModel, AutoTokenizer]:\n",
    "    '''\n",
    "    Load a model from huggingface.\n",
    "    :param model_name: Check huggingface for acceptable model names.\n",
    "    :return: Model and tokenizer.\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, config=config)\n",
    "    model.config.max_position_embeddings = 128  # Reducing from default 512 to 128 for computational efficiency\n",
    "\n",
    "    logging.info(f'Loaded model and tokenizer: {model_name} with {model.config.num_hidden_layers} layers, '\n",
    "                 f'hidden size {model.config.hidden_size} and sequence length {model.config.max_position_embeddings}.')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd15bb1",
   "metadata": {},
   "source": [
    "To play around with other models, find a list of models and their model_ids at: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3146ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model('distilbert-base-uncased') #'bert-base-uncased' has 12 layers and may take a while to process. We'll investigate distilbert instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd3cdf",
   "metadata": {},
   "source": [
    "Let's see what the model's architecture looks like. How many layers does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc54f5",
   "metadata": {},
   "source": [
    "Let's see if your answer matches the actual number of layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "print(f'The model has {num_layers} layers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43362245",
   "metadata": {},
   "source": [
    "##### Setting up the Probe\n",
    "Before we define the probing classifier or probe, let's set up some utility functions the probe will use. \n",
    "The probe will be trained from hidden representations from a specific layer of the BERT model. The `get_embeddings_from_model` function will retrieve the intermediate layer representations (also known as embeddings) from a user defined layer number.\n",
    "\n",
    "The `visualize_embeddings` method can be used to see what these high dimensional hidden embeddings would look like when converted into a 2D view. The visualization is not intended to be informative in itself, and is only an additional tool used to get a sense of what the inputs to the probing classifier may look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_model(model: AutoModel, tokenizer: AutoTokenizer, layer_num: int, data: list[str], batch_size : int) -> torch.Tensor:\n",
    "    '''\n",
    "    Get the embeddings from a model.\n",
    "    :param model: The model to use. This is needed to get the embeddings.\n",
    "    :param tokenizer: The tokenizer to use. This is needed to convert the data to input IDs.\n",
    "    :param layer_num: The layer to get embeddings from. 0 is the input embeddings, and the last layer is the output embeddings.\n",
    "    :param data: The data to get embeddings for. A list of strings.\n",
    "    :return: The embeddings. Shape is N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
    "    '''\n",
    "    logging.info(f'Getting embeddings from layer {layer_num} for {len(data)} samples...')\n",
    "\n",
    "    # Batch the data for computational efficiency\n",
    "    batch_num = 1\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        logging.debug(f'Getting embeddings for batch {batch_num}...')\n",
    "        batch_num += 1\n",
    "\n",
    "        # Tokenize the batch of data\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "\n",
    "        # Get the embeddings from the model\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Get the embeddings for the specific the layer\n",
    "        embeddings = outputs.hidden_states[layer_num]\n",
    "        logging.debug(f'Extracted hidden states of shape {embeddings.shape}')\n",
    "\n",
    "        # Concatenate the embeddings from each batch\n",
    "        if i == 0:\n",
    "            all_embeddings = embeddings\n",
    "        else:\n",
    "            all_embeddings = torch.cat([all_embeddings, embeddings], dim=0)\n",
    "\n",
    "    logging.info(f'Got embeddings for {len(data)} samples from layer {layer_num}. Shape: {all_embeddings.shape}')\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3943068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings: torch.Tensor, labels: list, layer_num: int, visualization_method: str = 't-SNE', save_plot: bool = False) -> None:\n",
    "    '''\n",
    "    Visualize the embeddings using t-SNE.\n",
    "    :param embeddings: The embeddings to visualize. Shape is N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
    "    :param labels: The labels for the embeddings. A list of integers.\n",
    "    :return: None\n",
    "    '''\n",
    "\n",
    "    # Since we are working with sentiment analysis, which is sentence based task, we can use sentence embeddings.\n",
    "    # The sentence embeddings are simply the mean of the token embeddings of that sentence.\n",
    "    sentence_embeddings = torch.mean(embeddings, dim=1)  # N, D\n",
    "\n",
    "    # Convert to numpy\n",
    "    sentence_embeddings = sentence_embeddings.detach().numpy()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    assert visualization_method in ['t-SNE', 'PCA'], \"visualization_method must be one of 't-SNE' or 'PCA'\"\n",
    "\n",
    "    # Visualize the embeddings\n",
    "    if visualization_method == 't-SNE':\n",
    "        tsne = TSNE(n_components=2, random_state=0)\n",
    "        embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
    "        xlabel = 't-SNE dimension 1'\n",
    "        ylabel = 't-SNE dimension 2'\n",
    "    if visualization_method == 'PCA':\n",
    "        pca = PCA(n_components=2, random_state=0)\n",
    "        embeddings_2d = pca.fit_transform(sentence_embeddings)\n",
    "        xlabel = 'First Principal Component'\n",
    "        ylabel = 'Second Principal Component'\n",
    "\n",
    "    negative_points = embeddings_2d[labels == 0]\n",
    "    positive_points = embeddings_2d[labels == 1]\n",
    "\n",
    "    # Plot the embeddings. We want to colour the datapoints by label.\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(negative_points[:, 0], negative_points[:, 1], label='Negative', color='red', marker='o', s=10, alpha=0.7)\n",
    "    ax.scatter(positive_points[:, 0], positive_points[:, 1], label='Positive', color='blue', marker='o', s=10, alpha=0.7)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f'{visualization_method} of Sentence Embeddings - Layer{layer_num}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot if needed, then display it\n",
    "    if save_plot:\n",
    "        plt.savefig(f'{visualization_method}_layer_{layer_num}.png')\n",
    "    plt.show()\n",
    "\n",
    "    logging.info(f'Visualized embeddings using {visualization_method}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac3469",
   "metadata": {},
   "source": [
    "Now, it's finally time to define our probe! We set this up as a class, where the probe itself is an object of this class. \n",
    "The class also contains methods used to train and evaluate the probe. \n",
    "\n",
    "Read through this code block in a bit more detail - from this whole exercise, this part provides you with the most useful takeaways on ways to define and train neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daed9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe():\n",
    "    def __init__(self, hidden_dim: int = 768, class_size: int = 2)  -> None:\n",
    "        '''\n",
    "        Initialize the probe.\n",
    "        :param hidden_dim: The dimensionality of the hidden layer of the probe.\n",
    "        :param num_layers: The number of layers in the probe.\n",
    "        :return: None\n",
    "        '''\n",
    "\n",
    "        # The probe is a simple linear classifier, with a hidden layer and an output layer.\n",
    "        # The input to the probe is the embeddings from the model, and the output is the predicted class.\n",
    "\n",
    "        # Exercise: Try playing around with the hidden_dim and num_layers to see how it affects the probe's performance.\n",
    "        # But watch out: if a complex probe performs well on the task, we don't know if the performance\n",
    "        # is because of the model embeddings, or the probe itself learning the task!\n",
    "\n",
    "        self.probe = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, class_size),\n",
    "\n",
    "            # Add more layers here if needed\n",
    "\n",
    "            # Sigmoid is used to convert the hidden states into a probability distribution over the classes\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, data_embeddings: torch.Tensor, labels: torch.Tensor, num_epochs: int = 10,\n",
    "              learning_rate: float = 0.001, batch_size: int = 32) -> None:\n",
    "        '''\n",
    "        Train the probe on the embeddings of data from the model.\n",
    "        :param data_embeddings: A tensor of shape N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
    "        :param labels: A tensor of shape N, where N is the number of samples. Each element is the label for the corresponding sample.\n",
    "        :param num_epochs: The number of epochs to train the probe for. An epoch is one pass through the entire dataset.\n",
    "        :param learning_rate: How fast the probe learns. A hyperparameter.\n",
    "        :param batch_size: Used to batch the data for computational efficiency. A hyperparameter.\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # Setup the loss function (training objective) for the training process.\n",
    "        # The cross-entropy loss is used for multi-class classification, and represents the negative log likelihood of the true class.\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Setup the optimization algorithm to update the probe's parameters during training.\n",
    "        # The Adam optimizer is an extension to stochastic gradient descent, and is a popular choice.\n",
    "        optimizer = torch.optim.Adam(self.probe.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the probe\n",
    "        logging.info('Training the probe...')\n",
    "        for epoch in range(num_epochs):  # Pass over the data num_epochs times\n",
    "\n",
    "            for i in range(0, len(data_embeddings), batch_size):\n",
    "\n",
    "                # Iterate through one batch of data at a time\n",
    "                batch_embeddings = data_embeddings[i:i+batch_size].detach()\n",
    "                batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "                # Convert to sentence embeddings, since we are performing a sentence classification task\n",
    "                batch_embeddings = torch.mean(batch_embeddings, dim=1)  # N, D\n",
    "\n",
    "                # Get the probe's predictions, given the embeddings from the model\n",
    "                outputs = self.probe(batch_embeddings)\n",
    "\n",
    "                # Calculate the loss of the predictions, against the true labels\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "\n",
    "                # Backward pass - update the probe's parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        logging.info('Done.')\n",
    "\n",
    "\n",
    "    def predict(self, data_embeddings: torch.Tensor, batch_size: int = 32) -> torch.Tensor:\n",
    "        '''\n",
    "        Get the probe's predictions on the embeddings from the model, for unseen data.\n",
    "        :param data_embeddings: A tensor of shape N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
    "        :param batch_size: Used to batch the data for computational efficiency.\n",
    "        :return: A tensor of shape N, where N is the number of samples. Each element is the predicted class for the corresponding sample.\n",
    "        '''\n",
    "\n",
    "        # Iterate through batches\n",
    "        for i in range(0, len(data_embeddings), batch_size):\n",
    "\n",
    "            # Iterate through one batch of data at a time\n",
    "            batch_embeddings = data_embeddings[i:i+batch_size]\n",
    "\n",
    "            # Convert to sentence embeddings, since we are performing a sentence classification task\n",
    "            batch_embeddings = torch.mean(batch_embeddings, dim=1)  # N, D\n",
    "\n",
    "            # Get the probe's predictions\n",
    "            outputs = self.probe(batch_embeddings)\n",
    "\n",
    "            # Get the predicted class for each sample\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Concatenate the predictions from each batch\n",
    "            if i == 0:\n",
    "                all_predicted = predicted\n",
    "            else:\n",
    "                all_predicted = torch.cat([all_predicted, predicted], dim=0)\n",
    "\n",
    "        return all_predicted\n",
    "\n",
    "\n",
    "    def evaluate(self, data_embeddings: torch.tensor, labels: torch.tensor, batch_size: int = 32) -> float:\n",
    "        '''\n",
    "        Evaluate the probe's performance by testing it on unseen data.\n",
    "        :param data_embeddings: A tensor of shape N, L, D, where N is the number of samples, L is the length of the sequence, and D is the dimensionality of the embeddings.\n",
    "        :param labels: A tensor of shape N, where N is the number of samples. Each element is the label for the corresponding sample.\n",
    "        :return: The accuracy of the probe on the unseen data.\n",
    "        '''\n",
    "\n",
    "        # Iterate through batches\n",
    "        for i in range(0, len(data_embeddings), batch_size):\n",
    "\n",
    "            # Iterate through one batch of data at a time\n",
    "            batch_embeddings = data_embeddings[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "            # Convert to sentence embeddings, since we are performing a sentence classification task\n",
    "            batch_embeddings = torch.mean(batch_embeddings, dim=1)  # N, D\n",
    "\n",
    "            # Get the probe's predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.probe(batch_embeddings)\n",
    "\n",
    "            # Get the predicted class for each sample\n",
    "            _, predicted = torch.max(outputs, dim=-1)\n",
    "\n",
    "            # Concatenate the predictions from each batch\n",
    "            if i == 0:\n",
    "                all_predicted = predicted\n",
    "                all_labels = batch_labels\n",
    "            else:\n",
    "                all_predicted = torch.cat([all_predicted, predicted], dim=0)\n",
    "                all_labels = torch.cat([all_labels, batch_labels], dim=0)\n",
    "\n",
    "        # Calculate the accuracy of the probe\n",
    "        correct = (all_predicted == all_labels).sum().item()\n",
    "        accuracy = correct / all_labels.shape[0]\n",
    "        logging.info(f'Probe accuracy: {accuracy:.2f}')\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the probing classifier (or probe)\n",
    "probe = Probe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbeda1",
   "metadata": {},
   "source": [
    "##### Analysing the model using Probes\n",
    "\n",
    "Time to start evaluating the model using our probing tool! Let's see which layer has most information about sentiment analysis on IMDB.\n",
    "For this, we will train the probe on embeddings from each layer of the model, and see which layer performs the best on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3053545",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_wise_accuracies = []\n",
    "best_probe, best_layer, best_accuracy = None, -1, 0\n",
    "batch_size = 32\n",
    "\n",
    "for layer_num in range(num_layers):\n",
    "    logging.info(f'Evaluating representations of layer {layer_num}:\\n')\n",
    "\n",
    "    train_embeddings = get_embeddings_from_model(model, tokenizer, layer_num=layer_num, data=train_dataset['text'], batch_size=batch_size)\n",
    "    dev_embeddings = get_embeddings_from_model(model, tokenizer, layer_num=layer_num, data=dev_dataset['text'], batch_size=batch_size)\n",
    "    train_labels, dev_labels = torch.tensor(train_dataset['label'],  dtype=torch.long), torch.tensor(dev_dataset['label'],  dtype=torch.long)\n",
    "\n",
    "    # Now, let's train the probe on the embeddings from the model.\n",
    "    # Feel free to play around with the training hyperparameters, and see what works best for your probe.\n",
    "    probe = Probe()\n",
    "    probe.train(data_embeddings=train_embeddings, labels=train_labels,\n",
    "                num_epochs=5, learning_rate=0.001, batch_size=8)\n",
    "\n",
    "    # Let's see how well our probe does on a held out dev set\n",
    "    accuracy = probe.evaluate(data_embeddings=dev_embeddings, labels=dev_labels)\n",
    "    layer_wise_accuracies.append(accuracy)\n",
    "\n",
    "    # Keep track of the best probe\n",
    "    if accuracy > best_accuracy:\n",
    "        best_probe, best_layer, best_accuracy = probe, layer_num, accuracy\n",
    "\n",
    "logging.info(f'DONE.\\n Best accuracy of {best_accuracy*100}% from layer {best_layer}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbab118",
   "metadata": {},
   "source": [
    "Seeing a list of accuracies can be hard to interpret. Let's plot the layer-wise accuracies to see which layer is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(layer_wise_accuracies)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Probe Accuracy by Layer')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e274dd7",
   "metadata": {},
   "source": [
    "Which layer has the best accuracy? What does this tell us about the model?\n",
    "\n",
    "Is the last layer of every model the most informative? Not necessarily! With larger models, many semantic tasks are encoded in the intermediate layers, while the last layers focus more on next token prediction.\n",
    "\n",
    "##### Visualizing Embeddings\n",
    "\n",
    "We've seen that the last layer of the model is most informative for the sentiment analysis task. Can we \"see\" what embedding structure the probe saw to say that the last layer's embeddings were most separable? \n",
    "\n",
    "Let's use the `visualize_embeddings` method from before. We'll also use two different kinds of visualization strategies:\n",
    "- PCA: A linear method using SVD to highlight the largest variances in the data.\n",
    "- t-SNE: A non-linear method that emphasizes local patterns and clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = ...\n",
    "embeddings=get_embeddings_from_model(model, tokenizer, layer_num=layer_num, data=train_dataset['text'], batch_size=batch_size)\n",
    "labels=torch.tensor(train_dataset['label'],  dtype=torch.long).numpy().tolist()\n",
    "visualize_embeddings(embeddings=embeddings, labels=labels, layer_num=layer_num, visualization_method='t-SNE')\n",
    "visualize_embeddings(embeddings=embeddings, labels=labels, layer_num=layer_num, visualization_method='PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd866c1c",
   "metadata": {},
   "source": [
    "Not very informative, was it? Because these embeddings exist in such high dimentions, it is not always possible to extract useful structure in them to simple 2D spaces. For this reason, visualizations are better treated as additional sources of information, rather than primary ones.\n",
    "\n",
    "#### Testing the best layer on OOD data\n",
    "\n",
    "Let's go ahead and stress test our probe's finding. Is the best layer able to predict sentiment for sentences outside the IMDB dataset?\n",
    "\n",
    "For answering this question, you are the test set! Try to think of challenging sequences for which the model may not be able to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_layer = ...\n",
    "test_sequences = ['Your sentence here', 'Here is another sentence']\n",
    "embeddings = get_embeddings_from_model(model=model, tokenizer=tokenizer, layer_num=best_layer, data=test_sequences, batch_size=batch_size)\n",
    "preds = probe.predict(data_embeddings=embeddings)\n",
    "predictions = ['Positive' if pred == 1 else 'Negative' for pred in preds]\n",
    "print(f'Predictions for test sequences: {predictions}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
