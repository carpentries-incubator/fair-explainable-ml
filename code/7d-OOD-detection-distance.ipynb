{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf03e7e",
   "metadata": {},
   "source": [
    "# OOD Detection: Distance-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cb7fd-ee8b-4955-885d-49b455a7a1d2",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::::: questions\n",
    "- How do distance-based methods like Mahalanobis distance and KNN work for OOD detection?\n",
    "- What is contrastive learning and how does it improve feature representations?\n",
    "- How does contrastive learning enhance the effectiveness of distance-based OOD detection methods?\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "::::::::::::::::::::::::::::::::::::::: objectives\n",
    "- Gain a thorough understanding of distance-based OOD detection methods, including Mahalanobis distance and KNN.\n",
    "- Learn the principles of contrastive learning and its role in improving feature representations.\n",
    "- Explore the synergy between contrastive learning and distance-based OOD detection methods to enhance detection performance.\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede6205-5cc1-4aee-8e7c-fb8029e5f627",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Distance-based Out-of-Distribution (OOD) detection relies on measuring the proximity of a data point to the training data's feature space. Unlike threshold-based methods such as softmax or energy, distance-based approaches compute the similarity of the feature representation of an input to the known classes' clusters. \n",
    "\n",
    "### Advantages\n",
    "- **Class-agnostic**: Can detect OOD data regardless of the class label.\n",
    "- **Highly interpretable**: Uses well-defined mathematical distances like Euclidean or Mahalanobis.\n",
    "\n",
    "### Disadvantages\n",
    "- **Requires feature extraction**: Needs a model that produces meaningful embeddings.\n",
    "- **Computationally intensive**: Calculating distances can be expensive, especially with high-dimensional embeddings.\n",
    "\n",
    "We will use the Mahalanobis distance as the core metric in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10f728",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance\n",
    "The Mahalanobis distance measures the distance of a point from a distribution, accounting for the variance and correlations of the data:\n",
    "\n",
    "$$\n",
    "D_M(x) = \\sqrt{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\n",
    "$$\n",
    "where:\n",
    "- x: The input data point.\n",
    "- \\(mu\\): The mean vector of the distribution.\n",
    "- Sigma: The covariance matrix of the distribution. The inverse of the covariance matrix is used to \"whiten\" the feature space, ensuring that features with larger variances do not dominate the distance computation. This adjustment also accounts for correlations between features, transforming the data into a space where all features are uncorrelated and standardized.\n",
    "This approach is robust for high-dimensional data as it accounts for correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c42e6-acd1-46a1-8798-dc1ef31ad87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "def prep_ID_OOD_datasests(ID_class_labels, OOD_class_labels):\n",
    "    \"\"\"\n",
    "    Prepares in-distribution (ID) and out-of-distribution (OOD) datasets \n",
    "    from the Fashion MNIST dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - ID_class_labels: list or array-like, labels for the in-distribution classes.\n",
    "                       Example: [0, 1] for T-shirts (0) and Trousers (1).\n",
    "    - OOD_class_labels: list or array-like, labels for the out-of-distribution classes.\n",
    "                        Example: [5] for Sandals.\n",
    "\n",
    "    Returns:\n",
    "    - train_data: np.array, training images for in-distribution classes.\n",
    "    - test_data: np.array, test images for in-distribution classes.\n",
    "    - ood_data: np.array, test images for out-of-distribution classes.\n",
    "    - train_labels: np.array, labels corresponding to the training images.\n",
    "    - test_labels: np.array, labels corresponding to the test images.\n",
    "    - ood_labels: np.array, labels corresponding to the OOD test images.\n",
    "\n",
    "    Notes:\n",
    "    - The function filters images based on provided class labels for ID and OOD.\n",
    "    - Outputs include images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    # Load Fashion MNIST dataset\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    \n",
    "    # Prepare OOD data: Sandals = 5\n",
    "    ood_filter = np.isin(test_labels, OOD_class_labels)\n",
    "    ood_data = test_images[ood_filter]\n",
    "    ood_labels = test_labels[ood_filter]\n",
    "    print(f'ood_data.shape={ood_data.shape}')\n",
    "    \n",
    "    # Filter data for T-shirts (0) and Trousers (1) as in-distribution\n",
    "    train_filter = np.isin(train_labels, ID_class_labels)\n",
    "    test_filter = np.isin(test_labels, ID_class_labels)\n",
    "    \n",
    "    train_data = train_images[train_filter]\n",
    "    train_labels = train_labels[train_filter]\n",
    "    print(f'train_data.shape={train_data.shape}')\n",
    "    \n",
    "    test_data = test_images[test_filter]\n",
    "    test_labels = test_labels[test_filter]\n",
    "    print(f'test_data.shape={test_data.shape}')\n",
    "\n",
    "    return train_data, test_data, ood_data, train_labels, test_labels, ood_labels\n",
    "\n",
    "\n",
    "def plot_data_sample(train_data, ood_data):\n",
    "    \"\"\"\n",
    "    Plots a sample of in-distribution and OOD data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: np.array, array of in-distribution data images\n",
    "    - ood_data: np.array, array of out-of-distribution data images\n",
    "\n",
    "    Returns:\n",
    "    - fig: matplotlib.figure.Figure, the figure object containing the plots\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    N_samples = 7\n",
    "    for i in range(N_samples):\n",
    "        plt.subplot(2, N_samples, i + 1)\n",
    "        plt.imshow(train_data[i], cmap='gray')\n",
    "        plt.title(\"In-Dist\")\n",
    "        plt.axis('off')\n",
    "    for i in range(N_samples):\n",
    "        plt.subplot(2, N_samples, i + N_samples+1)\n",
    "        plt.imshow(ood_data[i], cmap='gray')\n",
    "        plt.title(\"OOD\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "train_data, test_data, ood_data, train_labels, test_labels, ood_labels = prep_ID_OOD_datasests([0,1], [5]) #list(range(2,10)) use remaining 8 classes in dataset as OOD\n",
    "fig = plot_data_sample(train_data, ood_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0e412-427c-47dc-b683-56cdcc9dd5c7",
   "metadata": {},
   "source": [
    "### Preparing data for CNN\n",
    "Next, we'll prepare our data for a pytorch (torch) CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da0ab1-194e-445c-97f7-d4d199ae0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to PyTorch tensors and normalize\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "ood_data_tensor = torch.tensor(ood_data, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# TensorDataset provides a convenient way to couple input data with their corresponding labels, making it easier to pass them into a DataLoader.\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "ood_dataset = torch.utils.data.TensorDataset(ood_data_tensor, torch.zeros(ood_data_tensor.shape[0], dtype=torch.long))\n",
    "\n",
    "# DataLoader is used to efficiently load and manage batches of data\n",
    "# - It provides iterators over the data for training/testing.\n",
    "# - Supports options like batch size, shuffling, and parallel data loading\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce6f81-cff0-4ae5-ae54-0697a553326d",
   "metadata": {},
   "source": [
    "### Define CNN class\n",
    "Next, we'll define a simple Convolutional Neural Network (CNN) to classify in-distribution (ID) data. This CNN will serve as the backbone for our experiments, enabling us to analyze its predictions on both ID and OOD data. The model will include convolutional layers for feature extraction and fully connected layers for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc54eb0-ca25-400b-a18e-af38b0ae2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer:\n",
    "        # Input channels = 1 (grayscale images), output channels = 32, kernel size = 3x3\n",
    "        # Output size after conv1: (32, H-2, W-2) due to 3x3 kernel (reduces spatial dimensions by 2 in each direction)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "\n",
    "        # Second convolutional layer:\n",
    "        # Input channels = 32, output channels = 64, kernel size = 3x3\n",
    "        # Output size after conv2: (64, H-4, W-4) due to two 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "\n",
    "        # Fully connected layer (penultimate layer):\n",
    "        # Input size = 64 * 5 * 5, output size = 30\n",
    "        # 5x5 is derived from input image size (28x28) reduced by two 3x3 kernels and two 2x2 max-pooling operations\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 30)\n",
    "\n",
    "        # Final fully connected layer (classification layer):\n",
    "        # Input size = 128 (penultimate layer output), output size = 2 (binary classification)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, channels, height, width), e.g., (64, 1, 28, 28) for grayscale images.\n",
    "        \n",
    "        Returns:\n",
    "        - logits: Output tensor of shape (batch_size, num_classes), e.g., (64, 2).\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply first convolutional layer followed by ReLU and 2x2 max-pooling\n",
    "        # Input size: (batch_size, 1, 28, 28)\n",
    "        # Output size after conv1: (batch_size, 32, 26, 26)\n",
    "        # Output size after max-pooling: (batch_size, 32, 13, 13)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "        # Apply second convolutional layer followed by ReLU and 2x2 max-pooling\n",
    "        # Input size: (batch_size, 32, 13, 13)\n",
    "        # Output size after conv2: (batch_size, 64, 11, 11)\n",
    "        # Output size after max-pooling: (batch_size, 64, 5, 5)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        # Input size: (batch_size, 64, 5, 5)\n",
    "        # Output size after flattening: (batch_size, 64*5*5)\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "\n",
    "        # Apply the first fully connected layer (penultimate layer) with ReLU\n",
    "        # Input size: (batch_size, 64*5*5)\n",
    "        # Output size: (batch_size, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Apply the final fully connected layer (classification layer)\n",
    "        # Input size: (batch_size, 128)\n",
    "        # Output size: (batch_size, 2)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def extract_penultimate(self, x):\n",
    "        \"\"\"\n",
    "        Extracts embeddings from the penultimate layer of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input tensor of shape (batch_size, channels, height, width), e.g., (64, 1, 28, 28).\n",
    "        \n",
    "        Returns:\n",
    "        - embeddings: Output tensor from the penultimate layer of shape (batch_size, 128).\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply convolutional layers and max-pooling (same as in forward)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "\n",
    "        # Stop at the penultimate layer (fc1) and return the output\n",
    "        embeddings = F.relu(self.fc1(x))\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7939634-e397-488b-9d28-b9e88797ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = SimpleCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2283a60-9ce7-42f8-80e6-4cb6fe8f8cfb",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685641d3-7003-483f-b699-082caf605329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    \"\"\"\n",
    "    Trains a given PyTorch model using a specified dataset, loss function, and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): The neural network model to train.\n",
    "    - train_loader (DataLoader): DataLoader object providing the training dataset in batches.\n",
    "    - criterion (nn.Module): Loss function used for optimization (e.g., CrossEntropyLoss).\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for adjusting model weights (e.g., Adam, SGD).\n",
    "    - epochs (int): Number of training iterations over the entire dataset.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints the loss for each epoch during training.\n",
    "\n",
    "    Workflow:\n",
    "    1. Iterate over the dataset for the given number of epochs.\n",
    "    2. For each batch, forward propagate inputs, compute the loss, and backpropagate gradients.\n",
    "    3. Update model weights using the optimizer and reset gradients after each step.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move inputs and labels to the appropriate device (CPU or GPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Reset gradients from the previous step to avoid accumulation\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Compute model predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute the loss between predictions and true labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass: Compute gradients of the loss w.r.t. model parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update model weights using gradients and optimizer rules\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate the batch loss for reporting\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910eebb-49a0-46f6-84d4-1f8161c9ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extracts embeddings from the penultimate layer of the model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained PyTorch model.\n",
    "    - dataloader: DataLoader providing the dataset.\n",
    "    - device: Torch device (e.g., 'cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - features: NumPy array of embeddings from the penultimate layer.\n",
    "    - labels: NumPy array of corresponding labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            embeddings = model.extract_penultimate(inputs)  # Extract embeddings\n",
    "            # embeddings = model(inputs)  # Extract embeddings from output neurons (N= number of classes; limited feature representation)\n",
    "\n",
    "            features.append(embeddings.cpu().numpy())\n",
    "            labels.append(targets.cpu().numpy())\n",
    "\n",
    "    # Combine features and labels into arrays\n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    # Report shape as a sanity check\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Mahalanobis distance\n",
    "def compute_mahalanobis_distance(features, mean, covariance):\n",
    "    inv_covariance = np.linalg.inv(covariance)\n",
    "    distances = []\n",
    "    for x in features:\n",
    "        diff = x - mean\n",
    "        distance = np.sqrt(np.dot(np.dot(diff, inv_covariance), diff.T))\n",
    "        distances.append(distance)\n",
    "    return np.array(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and covariance of ID features\n",
    "id_features, id_labels = extract_features(model, train_loader, device)\n",
    "mean = np.mean(id_features, axis=0)\n",
    "\n",
    "# from sklearn.covariance import EmpiricalCovariance\n",
    "# covariance = EmpiricalCovariance().fit(id_features).covariance_\n",
    "\n",
    "from sklearn.covariance import LedoitWolf\n",
    "# Use a shrinkage estimator for covariance\n",
    "covariance = LedoitWolf().fit(id_features).covariance_\n",
    "\n",
    "# Compute Mahalanobis distances for ID and OOD data\n",
    "ood_features, _ = extract_features(model, ood_loader, device)\n",
    "id_distances = compute_mahalanobis_distance(id_features, mean, covariance)\n",
    "ood_distances = compute_mahalanobis_distance(ood_features, mean, covariance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mahalanobis distances\n",
    "plt.hist(id_distances, bins=50, alpha=0.5, label='ID')\n",
    "plt.hist(ood_distances, bins=50, alpha=0.5, label='OOD')\n",
    "plt.xlabel('Mahalanobis Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Mahalanobis Distances for ID and OOD Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29d588-4619-47ec-b5e2-ac65a1421b87",
   "metadata": {},
   "source": [
    "### Discussion: Overlapping Mahalanobis distance distributions\n",
    "After plotting the Mahalanobis distances for in-distribution (ID) and out-of-distribution (OOD) data, we may observe some overlap between the two distributions. This overlap reveals one of the limitations of distance-based methods: **the separability of ID and OOD data is highly dependent on the quality of the feature representations**. The model's learned features might not adequately distinguish between ID and OOD data, especially when OOD samples share semantic or structural similarities with ID data.\n",
    "\n",
    "### A solution? Contrastive learning\n",
    "In classical training regimes, models are trained with a *limited worldview*. They learn to distinguish between pre-defined classes based only on the data they’ve seen during training. You don't know what you don't know!\n",
    "\n",
    "For instance, consider a child learning to identify animals based on a set of flashcards with pictures of cats, dogs, and birds. If you show them a picture of a fox or a turtle, they might struggle because their understanding is constrained by the categories they’ve been explicitly taught. This is analogous to the way models trained with supervised learning approach classification—they build decision boundaries tailored to the training classes but struggle with new, unseen data.\n",
    "\n",
    "Now, consider teaching the child differently. Instead of focusing solely on identifying \"cat\" or \"dog,\" you teach them to group animals by broader characteristics—like furry vs. scaly or walking vs. swimming. This approach helps the child form a more generalized understanding of the world, enabling them to recognize new animals by connecting them to familiar patterns. Contrastive learning aims to achieve something similar for machine learning models.\n",
    "\n",
    "**Contrastive learning** creates feature spaces that are less dependent on specific classes and more attuned to broader semantic relationships. By learning to pull similar data points closer in feature space and push dissimilar ones apart, contrastive methods generate representations that are robust to shifts in data and can naturally cluster unseen categories. This makes contrastive learning particularly promising for improving OOD detection, as it helps models generalize beyond their training distribution.\n",
    "\n",
    "Unlike traditional training methods that rely heavily on explicit class labels, contrastive learning optimizes the feature space itself, encouraging the model to group similar data points together and push dissimilar ones apart. For example:\n",
    "\n",
    "- Positive pairs (e.g., augmented views of the same image) are encouraged to be close in the feature space.\n",
    "- Negative pairs (e.g., different images or samples from different distributions) are pushed apart.\n",
    "\n",
    "This results in a *feature space with semantic clusters*, where data points with similar meanings are grouped together, even across unseen distributions.\n",
    "\n",
    "#### Challenges and trade-offs\n",
    "- **Training complexity:** Contrastive learning requires large amounts of diverse data and careful design of augmentations or sampling strategies.\n",
    "- **Unsupervised nature:** While contrastive learning does not rely on explicit labels, defining meaningful positive and negative pairs is non-trivial.\n",
    "\n",
    "### Concluding thoughts and future directions\n",
    "\n",
    "While contrastive learning provides an exciting opportunity to improve OOD detection, it represents a shift from the traditional threshold- or distance-based approaches we have discussed so far. By learning a feature space that is inherently more generalizable and robust, contrastive learning offers a promising solution to the challenges posed by overlapping Mahalanobis distance distributions.\n",
    "\n",
    "If you're interested, we can explore specific contrastive learning methods like **SimCLR** or **MoCo** in future sessions, diving into how their objectives help create robust feature spaces!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustworthy_ML",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
