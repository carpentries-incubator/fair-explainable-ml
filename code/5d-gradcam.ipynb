{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df165fb9-a54b-494b-be11-975d65482044",
   "metadata": {},
   "source": [
    "-   TODO\n",
    "\n",
    "-   TODO\n",
    "\n",
    "``` python\n",
    "# Let's begin by installing the grad-cam package - this will significantly simplify our implementation\n",
    "!pip install grad-cam\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Packages to download test images\n",
    "import requests\n",
    "\n",
    "# Packages to view and process images\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Packages to load the model\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# GradCAM Packaes\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "```\n",
    "\n",
    "``` python\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "```\n",
    "\n",
    "##### Load Model\n",
    "\n",
    "We’ll load the ResNet-50 model from torchvision. This model is pre-trained on the ImageNet dataset, which contains 1.2 million images across 1000 classes.\n",
    "ResNet-50 is popular model that is a type of convolutional neural network. You can learn more about it here: https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "\n",
    "``` python\n",
    "model = resnet50(pretrained=True).to(device).eval()\n",
    "```\n",
    "\n",
    "##### Load Test Image\n",
    "\n",
    "``` python\n",
    "# Let's first take a look at the image, which we source from the GradCAM package\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/both.png\"\n",
    "Image.open(requests.get(url, stream=True).raw)\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Cute, isn't it? Do you prefer dogs or cats?\n",
    "\n",
    "# We will need to convert the image into a tensor to feed it into the model.\n",
    "# Let's create a function to do this for us.\n",
    "def load_image(url):\n",
    "    rgb_img = np.array(Image.open(requests.get(url, stream=True).raw))\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img).to(device)\n",
    "    return input_tensor, rgb_img\n",
    "```\n",
    "\n",
    "``` python\n",
    "input_tensor, rgb_image = load_image(url)\n",
    "```\n",
    "\n",
    "### Grad-CAM Time!\n",
    "\n",
    "``` python\n",
    "# Let's start by selecting which layers of the model we want to use to generate the CAM.\n",
    "# For that, we will need to inspect the model architecture.\n",
    "# We can do that by simply printing the model object.\n",
    "print(model)\n",
    "```\n",
    "\n",
    "Here we want to interpret what the model as a whole is doing (not what a specific layer is doing).\n",
    "That means that we want to use the embeddings of the last layer before the final classification layer.\n",
    "This is the layer that contains the information about the image encoded by the model as a whole.\n",
    "\n",
    "Looking at the model, we can see that the last layer before the final classification layer is `layer4`.\n",
    "\n",
    "``` python\n",
    "target_layers = [model.layer4]\n",
    "```\n",
    "\n",
    "We also want to pick a label for the CAM - this is the class we want to visualize the activation for.\n",
    "Essentially, we want to see what the model is looking at when it is predicting a certain class.\n",
    "\n",
    "Since ResNet was trained on the ImageNet dataset with 1000 classes, let’s get an indexed list of those classes. We can then pick the index of the class we want to visualize.\n",
    "\n",
    "``` python\n",
    "imagenet_categories_url = \\\n",
    "     \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
    "labels = eval(requests.get(imagenet_categories_url).text)\n",
    "labels\n",
    "```\n",
    "\n",
    "Well, that’s a lot! To simplify things, we have already picked out the indices of a few interesting classes.\n",
    "\n",
    "-   157: Siberian Husky\n",
    "-   162: Beagle\n",
    "-   245: French Bulldog\n",
    "-   281: Tabby Cat\n",
    "-   285: Egyptian cat\n",
    "-   360: Otter\n",
    "-   537: Dog Sleigh\n",
    "-   799: Sliding Door\n",
    "-   918: Street Sign\n",
    "\n",
    "``` python\n",
    "# Specify the target class for visualization here. If you set this to None, the class with the highest score from the model will automatically be used.\n",
    "visualized_class_id = 245\n",
    "```\n",
    "\n",
    "``` python\n",
    "def viz_gradcam(model, target_layers, class_id):\n",
    "\n",
    "  if class_id is None:\n",
    "    targets = None\n",
    "  else:\n",
    "    targets = [ClassifierOutputTarget(class_id)]\n",
    "\n",
    "  cam_algorithm = GradCAM\n",
    "  with cam_algorithm(model=model, target_layers=target_layers) as cam:\n",
    "      grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                          targets=targets)\n",
    "\n",
    "      grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "      cam_image = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True)\n",
    "      cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "  cv2_imshow(cam_image)\n",
    "```\n",
    "\n",
    "Finally, we can start visualizing! Let’s begin by seeing what parts of the image the model looks at to make its most confident prediction.\n",
    "\n",
    "``` python\n",
    "viz_gradcam(model=model, target_layers=target_layers, class_id=None)\n",
    "```\n",
    "\n",
    "Interesting, it looks like the model totally ignores the cat and makes a prediction based on the dog.\n",
    "If we set the output class to “French Bulldog” (`class_id=245`), we see the same visualization - meaning that the model is indeed looking at the correct part of the image to make the correct prediction.\n",
    "\n",
    "Let’s see what the heatmap looks like when we force the model to look at the cat.\n",
    "\n",
    "``` python\n",
    "viz_gradcam(model=model, target_layers=target_layers, class_id=281)\n",
    "```\n",
    "\n",
    "The model is indeed looking at the cat when asked to predict the class “Tabby Cat” (`class_id=281`)!\n",
    "But why is it still predicting the dog? Well, the model was trained on the ImageNet dataset, which contains a lot of images of dogs and cats.\n",
    "The model has learned that the dog is a better indicator of the class “Tabby Cat” than the cat itself.\n",
    "\n",
    "Let’s see another example of this. The image has not only a dog and a cat, but also a items in the background. Can the model correctly identify the door?\n",
    "\n",
    "``` python\n",
    "viz_gradcam(model=model, target_layers=target_layers, class_id=799)\n",
    "```\n",
    "\n",
    "It can! However, it seems to also think of the shelf behind the dog as a door.\n",
    "\n",
    "Let’s try an unrelated object now. Where in the image does the model see a street sign?\n",
    "\n",
    "``` python\n",
    "viz_gradcam(model=model, target_layers=target_layers, class_id=918)\n",
    "```\n",
    "\n",
    "Looks like our analysis has revealed a shortcoming of the model! It seems to percieve cats and street signs similarly.\n",
    "\n",
    "Ideally, when the target class is some unrelated object, a good model will look at no significant part of the image. For example, the model does a good job with the class for Dog Sleigh.\n",
    "\n",
    "``` python\n",
    "viz_gradcam(model=model, target_layers=target_layers, class_id=537)\n",
    "```\n",
    "\n",
    "Explaining model predictions though visualization techniques like this can be very subjective and prone to error. However, this still provides some degree of insight a completely black box model would not provide.\n",
    "\n",
    "Spend some time playing around with different classes and seeing which part of the image the model looks at. Feel free to play around with other base images as well. Have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
