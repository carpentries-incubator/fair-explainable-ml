<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Trustworthy AI: Explainability, Bias, and Fairness: Model evaluation and fairness</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="manifest" href="../site.webmanifest"><link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../3-model-eval-and-fairness.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Trustworthy AI: Explainability, Bias, and Fairness
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Trustworthy AI: Explainability, Bias, and Fairness
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Trustworthy AI: Explainability, Bias, and Fairness
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 94%" class="percentage">
    94%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 94%" aria-valuenow="94" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../3-model-eval-and-fairness.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="0-introduction.html">1. Overview</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="1-preparing-to-train.html">2. Preparing to train a model</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="2-model-fitting.html">3. Scientific validity in the modeling process</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        4. Model evaluation and fairness
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#accuracy-metrics">Accuracy metrics</a></li>
<li><a href="#how-do-we-measure-fairness">How do we measure fairness?</a></li>
<li><a href="#fairness-in-generative-ai">Fairness in generative AI</a></li>
<li><a href="#improving-fairness-of-models">Improving fairness of models</a></li>
<li><a href="#mitigate-bias-with-in-processing">Mitigate bias with in-processing</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="4-explainability-vs-interpretability.html">5. Interpretablility versus explainability</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="5a-explainable-AI-method-overview.html">6. Explainability methods overview</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="5b-deep-dive-into-methods.html">7. Explainability methods: deep dive</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="5c-probes.html">8. Explainability methods: linear probe</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="5d-gradcam.html">9. Explainability methods: GradCAM</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="6-confidence-intervals.html">10. Estimating model uncertainty</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="7a-OOD-detection-output-based.html">11. Distribution Shift and Out-of-Distribution (OOD) Detection: Output-Based Methods</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush13">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading13">
        <a href="7b-OOD-detection-distance-based.html">12. OOD Detection: Distance-Based and Contrastive Learning</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush14">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading14">
        <a href="7c-OOD-detection-algo-design.html">13. OOD Detection: Training-Time Regularization</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush15">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading15">
        <a href="8-releasing-a-model.html">14. Documenting and releasing a model</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/2-model-fitting.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/4-explainability-vs-interpretability.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/2-model-fitting.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Scientific validity
        </a>
        <a class="chapter-link float-end" href="../instructor/4-explainability-vs-interpretability.html" rel="next">
          Next: Interpretablility...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Model evaluation and fairness</h1>
        <p>Last updated on 2024-06-19 |

        <a href="https://github.com/carpentries-incubator/fair-explainable-ml/edit/main/episodes/3-model-eval-and-fairness.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 0 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How do we define fairness and bias in machine learning
outcomes?</li>
<li>What types of bias and unfairness can occur in generative AI?</li>
<li>How can we improve the fairness of machine learning models?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Reason about model performance through standard evaluation
metrics.</li>
<li>Understand and distinguish between various notions of fairness in
machine learning.</li>
<li>Describe and implement two different ways of modifying the machine
learning modeling process to improve the fairness of a model.</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="accuracy-metrics">Accuracy metrics<a class="anchor" aria-label="anchor" href="#accuracy-metrics"></a>
<a class="anchor" aria-label="anchor" href="#accuracy-metrics"></a></h2>
<hr class="half-width"><p>Stakeholders often want to know the accuracy of a machine learning
model – what percent of predictions are correct? Accuracy can be
decomposed into further metrics: e.g., in a binary prediction setting,
recall (the fraction of positive samples that are classified correctly)
and precision (the fraction of samples classified as positive that
actually are positive) are commonly-used metrics.</p>
<p>Suppose we have a model that performs binary classification (+, -) on
a test dataset of 1000 samples (let <span class="math inline">\(n\)</span>=1000). A <em>confusion matrix</em>
defines how many predictions we make in each of four quadrants: true
positive with positive prediction (++), true positive with negative
prediction (+-), true negative with positive prediction (-+), and true
negative with negative prediction (–).</p>
<table class="table"><thead><tr class="header"><th></th>
<th>True +</th>
<th>True -</th>
</tr></thead><tbody><tr class="odd"><td>Predicted +</td>
<td>300</td>
<td>80</td>
</tr><tr class="even"><td>Predicted -</td>
<td>25</td>
<td>595</td>
</tr></tbody></table><p>So, for instance, 80 samples have a true class of + but get predicted
as members of -.</p>
<p>We can compute the following metrics:</p>
<ul><li>Accuracy: What fraction of predictions are correct?
<ul><li>(300 + 595) / 100 = 0.895</li>
<li>Accuracy is 89.5%</li>
</ul></li>
<li>Precision: What fraction of predicted positives are true positives?
<ul><li>300 / (300 + 80) = 0.789</li>
<li>Precision is 78.9%</li>
</ul></li>
<li>Recall: What fraction of true positives are classified as positive?
<ul><li>300 / (300 + 25) = 0.923</li>
<li>Recall is 92.3%</li>
</ul></li>
</ul><div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>We’ve discussed binary classification but for other types of tasks
there are different metrics. For example,</p>
<ul><li>Multi-class problems often use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html" class="external-link">Top-K
accuracy</a>, a metric of how often the true response appears in their
top-K guesses.</li>
<li>Regression tasks often use the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" class="external-link">Area
Under the ROC curve (AUC ROC)</a> as a measure of how well the
classifier performs at different thresholds.</li>
</ul></div>
</div>
</div>
<div id="what-accuracy-metric-to-use" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="what-accuracy-metric-to-use" class="callout-inner">
<h3 class="callout-title">What accuracy metric to use?<a class="anchor" aria-label="anchor" href="#what-accuracy-metric-to-use"></a>
</h3>
<div class="callout-content">
<p>Different accuracy metrics may be more relevant in different
situations. Discuss with a partner or small groups whether precision,
recall, or some combination of the two is most relevant in the following
prediction tasks:</p>
<ol style="list-style-type: decimal"><li>Deciding what patients are high risk for a disease and who should
get additional low-cost screening.</li>
<li>Deciding what patients are high risk for a disease and should start
taking medication to lower the disease risk. The medication is expensive
and can have unpleasant side effects.</li>
</ol></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li><p>It is best if all patients who need the screening get it, and
there is little downside for doing screenings unnecessarily because the
screening costs are low. Thus, a high recall score is optimal.</p></li>
<li><p>Given the costs and side effects of the medicine, we do not want
patients not at risk for the disease to take the medication. So, a high
precision score is ideal.</p></li>
</ol></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="how-do-we-measure-fairness">How do we measure fairness?<a class="anchor" aria-label="anchor" href="#how-do-we-measure-fairness"></a>
<a class="anchor" aria-label="anchor" href="#how-do-we-measure-fairness"></a></h2>
<hr class="half-width"><p>What does it mean for a machine learning model to be fair or
unbiased? There is no single definition of fairness, and we can talk
about fairness at several levels (ranging from training data, to model
internals, to how a model is deployed in practice). Similarly, bias is
often used as a catch-all term for any behavior that we think is unfair.
Even though there is no tidy definition of unfairness or bias, we can
use aggregate model outputs to gain an overall understanding of how
models behave with respect to different demographic groups – an approach
called group fairness.</p>
<p>In general, if there are no differences between groups in the real
world (e.g., if we lived in a utopia with no racial or gender gaps),
achieving fairness is easy. But, in practice, in many social settings
where prediction tools are used, there are differences between groups,
e.g., due to historical and current discrimination.</p>
<p>For instance, in a loan prediction setting in the United States, the
average white applicant may be better positioned to repay a loan than
the average Black applicant due to differences in generational wealth,
education opportunities, and other factors stemming from anti-Black
racism. Suppose that a bank uses a machine learning model to decide who
gets a loan. Suppose that 50% of white applicants are granted a loan,
with a precision of 90% and a recall of 70% – in other words, 90% of
white people granted loans end up repaying them, and 70% of all people
who would have repaid the loan, if given the opportunity, get the loan.
Consider the following scenarios:</p>
<ul><li>(Demographic parity) We give loans to 50% of Black applicants in a
way that maximizes overall accuracy</li>
<li>(Equalized odds) We give loans to X% of Black applicants, where X is
chosen to maximize accuracy subject to keeping precision equal to
90%.</li>
<li>(Group level calibration) We give loans to X% of Black applicants,
where X is chosen to maximize accuracy while keeping recall equal to
70%.</li>
</ul><p>There are <em>many</em> notions of statistical group fairness, but
most boil down to one of the three above options: demographic parity,
equalized odds, and group-level calibration. All three are forms of
<em>distributional</em> (or <em>outcome</em>) fairness. Another
dimension, though, is <em>procedural</em> fairness: whether decisions
are made in a just way, regardless of final outcomes. Procedural
fairness contains many facets, but one way to operationalize it is to
consider individual fairness (also called counterfactual fairness),
which was suggested in 2012 by <a href="https://dl.acm.org/doi/abs/10.1145/2090236.2090255" class="external-link">Dwork et
al.</a> as a way to ensure that “similar individuals [are treated]
similarly”. For instance, if two individuals differ only on their race
or gender, they should receive the same outcome from an algorithm that
decides whether to approve a loan application.</p>
<p>In practice, it’s hard to use individual fairness because defining a
complete set of rules about when two individuals are sufficiently
“similar” is challenging.</p>
<div id="matching-fairness-terminology-with-definitions" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="matching-fairness-terminology-with-definitions" class="callout-inner">
<h3 class="callout-title">Matching fairness terminology with definitions<a class="anchor" aria-label="anchor" href="#matching-fairness-terminology-with-definitions"></a>
</h3>
<div class="callout-content">
<p>Match the following types of formal fairness with their definitions.
(A) Individual fairness, (B) Equalized odds, (C) Demographic parity, and
(D) Group-level calibration</p>
<ol style="list-style-type: decimal"><li>The model is equally accurate across all demographic groups.</li>
<li>Different demographic groups have the same true positive rates and
false positive rates.</li>
<li>Similar people are treated similarly.</li>
<li>People from different demographic groups receive each outcome at the
same rate.</li>
</ol></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A - 3, B - 2, C - 4, D - 1</p>
</div>
</div>
</div>
</div>
<p>But some types of unfairness cannot be directly measured by
group-level statistical data. In particular, generative AI opens up new
opportunities for bias and unfairness. Bias can occur through
representational harms (e.g., creating content that over-represents one
population subgroup at the expense of another), or through stereotypes
(e.g., creating content that reinforces real-world stereotypes about a
group of people). We’ll discuss some specific examples of bias in
generative models next.</p>
</section><section><h2 class="section-heading" id="fairness-in-generative-ai">Fairness in generative AI<a class="anchor" aria-label="anchor" href="#fairness-in-generative-ai"></a>
<a class="anchor" aria-label="anchor" href="#fairness-in-generative-ai"></a></h2>
<hr class="half-width"><p>Generative models learn from statistical patterns in real-world data.
These statistical patterns reflect instances of bias in real-world data
- what data is available on the internet, what stereotypes does it
reinforce, and what forms of representation are missing?</p>
<div class="section level3">
<h3 id="natural-language">Natural language<a class="anchor" aria-label="anchor" href="#natural-language"></a></h3>
<p>One set of social stereotypes that large AI models can learn is
gender based. For instance, certain occupations are associated with men,
and others with women. For instance, in the U.S., doctors are
historically and stereotypically usually men.</p>
<p>In 2016, Caliskan et al. <a href="https://www.fatml.org/schedule/2016/presentation/semantics-derived-automatically-language-corpora" class="external-link">showed
that machine translation systems exhibit gender bias</a>, for instance,
by reverting to stereotypical gendered pronouns in ambiguous
translations, like in Turkish – a language without gendered pronouns –
to English.</p>
<p>In response, Google <a href="https://blog.research.google/2018/12/providing-gender-specific-translations.html" class="external-link">tweaked
their translator algorithms</a> to identify and correct for gender
stereotypes in Turkish and several other widely-spoken languages. So
when we repeat a similar experiment today, we get the following
output:</p>
<figure><img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-turkish-nlp-stereotypes.png" alt='Screenshot of Google Translate output. The English sentence "The doctor is on her lunch break" is translated to Turkish, and then the Turkish output is translated back to English as either "The doctor is on his lunch break" or "The doctor is on his lunch break".' class="figure mx-auto d-block"><div class="figcaption">Turkish Google Translate example (screenshot
from 1/9/2024)</div>
</figure><p>But for other, less widely-spoken languages, the original problem
persists:</p>
<figure><img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-norwegian-nlp-stereotypes.png" alt='Screenshot of Google Translate output. The English sentence "The doctor is on her lunch break" is translated to Norwegian, and then the Norwegian output is translated back to English as "The doctor is on his lunch break".' class="figure mx-auto d-block"><div class="figcaption">Norwegian Google Translate example (screenshot
from 1/9/2024)</div>
</figure><p>We’re not trying to slander Google Translate here – the translation,
without additional context, is ambiguous. And even if they extended the
existing solution to Norwegian and other languages, the underlying
problem (stereotypes in the training data) still exists. And with
generative AI such as ChatGPT, the problem can be even more
pernicious.</p>
<div id="red-teaming-large-language-models" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="red-teaming-large-language-models" class="callout-inner">
<h3 class="callout-title">Red-teaming large language models<a class="anchor" aria-label="anchor" href="#red-teaming-large-language-models"></a>
</h3>
<div class="callout-content">
<p>In cybersecurity, “red-teaming” is when well-intentioned people think
like a hacker in order to make a system safer. In the context of Large
Language Models (LLMs), red-teaming is used to try to get LLMs to output
offensive, inaccurate, or unsafe content, with the goal of understanding
the limitations of the LLM.</p>
<p>Try out red-teaming with ChatGPT or another LLM. Specifically, can
you construct a prompt that causes the LLM to output stereotypes? Here
are some example prompts, but feel free to get creative!</p>
<ul><li><p>“Tell me a story about a doctor” (or other profession with
gender)</p></li>
<li><p>If you speak a language other than English, how does are
ambiguous gendered pronouns handled? For instance, try the prompt
“Translate ‘The doctor is here’ to Spanish”. Is a masculine or feminine
pronoun used for the doctor in Spanish?</p></li>
</ul><p>If you use LLMs in your research, consider whether any of these
issues are likely to be present for your use cases. If you do not use
LLMs in your research, consider how these biases can affect downstream
uses of the LLM’s output.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Most publicly-available LLM providers set up guardrails to avoid
propagating biases present in their training data. For instance, as of
the time of this writing (January 2024), the first suggested prompt,
“Tell me a story about a doctor,” consistently creates a story about a
woman doctor. Similarly, substituting other professions that have strong
associations with men for “doctor” (e.g., “electrical engineer,”
“garbage collector,” and “US President”) yield stories with female or
gender-neutral names and pronouns.</p>
</div>
</div>
</div>
</div>
<div id="discussing-other-fairness-issues" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussing-other-fairness-issues" class="callout-inner">
<h3 class="callout-title">Discussing other fairness issues<a class="anchor" aria-label="anchor" href="#discussing-other-fairness-issues"></a>
</h3>
<div class="callout-content">
<p>If you use LLMs in your research, consider whether any of these
issues are likely to be present for your use cases. Share your thoughts
in small groups with other workshop participants.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-generation">Image generation<a class="anchor" aria-label="anchor" href="#image-generation"></a></h3>
<p>The same problems that language modeling face also affect image
generation. Consider, for instance, Melon et al. <a href="https://arxiv.org/pdf/2003.03808.pdf" class="external-link">developed an algorithm
called Pulse</a> that can convert blurry images to higher resolution.
But, biases were quickly unearthed and <a href="https://twitter.com/Chicken3gg/status/1274314622447820801?s=20&amp;t=_oORPJBJRaBW_J0zresFJQ" class="external-link">shared
via social media</a>.</p>
<div id="discussion5" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion5"></a>
</h3>
<div class="callout-content">
<p>Who is shown in this blurred picture? <img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-obama.png" alt="blurry image of Barack Obama" class="figure"></p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>While the picture is of Barack Obama, the upsampled image shows a
white face. <img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-obama-upsampled.png" alt="Unblurred version of the pixelated picture of Obama. Instead of showing Obama, it shows a white man." class="figure"></p>
<p>You can <a href="https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb#scrollTo=fU0aGtD4Nl4W" class="external-link">try
the model here</a>.</p>
</div>
</div>
</div>
</div>
<p>Menon and colleagues subsequently updated their paper to discuss this
issue of bias. They assert that the problems inherent in the PULSE model
are largely a result of the <a href="https://arxiv.org/abs/1812.04948" class="external-link">underlying StyleGAN model</a>,
which they had used in their work.</p>
<blockquote>
<p>Overall, it seems that sampling from StyleGAN yields white faces much
more frequently than faces of people of color … This bias extends to any
downstream application of StyleGAN, including the implementation of
PULSE using StyleGAN.</p>
<p>…</p>
<p>Results indicate a racial bias among the generated pictures, with
close to three-fourths (72.6%) of the pictures representing White
people. Asian (13.8%) and Black (10.1%) are considerably less frequent,
while Indians represent only a minor fraction of the pictures
(3.4%).</p>
</blockquote>
<p>These remarks get at a central issue: biases in any building block of
a system (data, base models, etc.) get propagated forwards. In
generative AI, such as text-to-image systems, this can result in
representational harms, <a href="https://arxiv.org/pdf/2211.03759.pdf" class="external-link">as documented by Bianchi et
al.</a> Fixing these issues of bias is still an active area of research.
One important step is to be careful in data collection, and try to get a
balanced dataset that does not contain harmful stereotypes. But large
language models use massive training datasets, so it is not possible to
manually verify data quality. Instead, researchers use heuristic
approaches to improve data quality, and then rely on various techniques
to improve models’ fairness, which we discuss next.</p>
</div>
</section><section><h2 class="section-heading" id="improving-fairness-of-models">Improving fairness of models<a class="anchor" aria-label="anchor" href="#improving-fairness-of-models"></a>
<a class="anchor" aria-label="anchor" href="#improving-fairness-of-models"></a></h2>
<hr class="half-width"><p>Model developers frequently try to improve the fairness of there
model by intervening at one of three stages: pre-processing,
in-processing, or post-processing. We’ll cover techniques within each of
these paradigms in turn.</p>
<p>We start, though, by discussing why removing the sensitive
attribute(s) is not sufficient. Consider the task of deciding which loan
applicants are funded. Suppose we are concerned with racial bias in the
model outputs. If we remove race from the set of attributes available to
the model, the model cannot make <em>overly</em> racist decisions.
However, it could instead make decisions based on zip code, which in the
US is a very good proxy for race.</p>
<p>Can we simply remove all proxy variables? We could likely remove zip
code, if we cannot identify a causal relationship between where someone
lives and whether they will be able to repay a loan. But what about an
attribute like educational achievement? Someone with a college degree
(compared with someone with, say, less than a high school degree) has
better employment opportunities and therefore might reasonably be
expected to be more likely to be able to repay a loan. However,
educational attainment is still a proxy for race in the United States
due to historical (and ongoing) discrimination.</p>
<p><strong>Pre-processing</strong> generally modifies the dataset used
for learning. Techniques in this category include:</p>
<ul><li><p>Oversampling/undersampling: instead of training a machine
learning model on all of the data, <em>undersample</em> the majority
class by removing some of the majority class samples from the dataset in
order to have a more balanced dataset. Alternatively,
<em>oversample</em> the minority class by duplicating samples belonging
to this group.</p></li>
<li><p>Data augmentation: the number of samples from minority groups may
be increased by generating synthetic data with a generative adversarial
network (GAN). We won’t cover this method in this workshop (using a GAN
can be more computationally expensive than other techniques). If you’re
interested, you can learn more about this method from the paper <a href="https://link.springer.com/chapter/10.1007/978-3-030-58542-6_23" class="external-link">Inclusive
GAN: Improving Data and Minority Coverage in Generative
Models</a>.</p></li>
<li><p>Changing feature representations: various techniques have been
proposed to increase fairness by removing unfairness from the data
directly. To do so, the data is converted into an alternate
representation so that differences between demographic groups are
minimized, yet enough information is maintained in order to be able to
learn a model that performs well. An advantage of this method is that it
is model-agnostic, however, a challenge is it reduces the
interpretability of interpretable models and makes post-hoc
explainability less meaningful for black-box models.</p></li>
</ul><div id="pros-and-cons-of-preprocessing-options" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="pros-and-cons-of-preprocessing-options" class="callout-inner">
<h3 class="callout-title">Pros and cons of preprocessing options<a class="anchor" aria-label="anchor" href="#pros-and-cons-of-preprocessing-options"></a>
</h3>
<div class="callout-content">
<p>Discuss what you think the pros and cons of the different
pre-processing options are. What techniques might work better in
different settings?</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>A downside of oversampling is that it may violate statistical
assumptions about independence of samples. A downside of undersampling
is that the total amount of data is reduced, potentially resulting in
models that perform less well overall.</p>
<p>A downside of using GANs to generate additional data is that this
process may be expensive and require higher levels of ML expertise.</p>
<p>A challenge with all techniques is that if there is not sufficient
data from minority groups, it may be hard to achieve good performance on
the groups without simply collecting more or higher-quality data.</p>
</div>
</div>
</div>
</div>
<p><strong>In-processing</strong> modifies the learning algorithm. Some
specific in-processing techniques include:</p>
<ul><li><p>Reweighting samples: many machine learning models allow for
reweighting individual samples, i.e., indicating that misclassifying
certain, rarer, samples should be penalized more severely in the loss
function. In the code example, we show how to reweight samples using
AIF360’s <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html" class="external-link">Reweighting</a>
function.</p></li>
<li><p>Incorporating fairness into the loss function: reweighting
explicitly instructs the loss function to penalize the misclassification
of certain samples more harshly. However, another option is to add a
term to the loss function corresponding to the fairness metric of
interest.</p></li>
</ul><p><strong>Post-processing</strong> modifies an existing model to
increase its fairness. Techniques in this category often compute a
custom <em>threshold</em> for each demographic group in order to satisfy
a specific notion of group fairness. For instance, if a machine learning
model for a binary prediction task uses 0.5 as a cutoff (e.g., raw
scores less than 0.5 get a prediction of 0 and others get a prediction
of 1), fair post-processing techniques may select different thresholds,
e.g., 0.4 or 0.6 for different demographic groups.</p>
<p>In the code, we explore two different bias mitigations strategies
implemented in the <a href="https://aif360.readthedocs.io/en/stable/" class="external-link">AIF360 Fairness
Toolkit</a>.</p>
<hr><div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown, display</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">from</span> aif360.metrics <span class="im">import</span> BinaryLabelDatasetMetric</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="im">from</span> aif360.metrics <span class="im">import</span> ClassificationMetric</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="im">from</span> aif360.explainers <span class="im">import</span> MetricTextExplainer</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> Reweighing</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="im">from</span> aif360.algorithms.preprocessing <span class="im">import</span> OptimPreproc</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="im">from</span> aif360.datasets <span class="im">import</span> MEPSDataset19</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="im">from</span> fairlearn.postprocessing <span class="im">import</span> ThresholdOptimizer</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span></code></pre>
</div>
<p>This notebook is adapted from AIF360’s <a href="https://github.com/Trusted-AI/AIF360/blob/master/examples/tutorial_medical_expenditure.ipynb" class="external-link">Medical
Expenditure Tutorial</a>.</p>
<p>The tutorial uses data from the <a href="https://meps.ahrq.gov/mepsweb/" class="external-link">Medical Expenditure Panel
Survey</a>. We include a short description of the data below. For more
details, especially on the preprocessing, please see the AIF360
tutorial. ## Scenario and data</p>
<p>The goal is to develop a healthcare utilization scoring model – i.e.,
to predict which patients will have the highest utilization of
healthcare resources.</p>
<p>The original dataset contains information about various types of
medical visits; the AIF360 preprocessing created a single output feature
‘UTILIZATION’ that combines utilization across all visit types. Then,
this feature is binarized based on whether utilization is high, defined
as &gt;= 10 visits. Around 17% of the dataset has high utilization.</p>
<p>The sensitive feature (that we will base fairness scores on) is
defined as race. Other predictors include demographics, health
assessment data, past diagnoses, and physical/mental limitations.</p>
<p>The data is divided into years (we follow the lead of AIF360’s
tutorial and use 2015), and further divided into Panels. We use Panel 19
(the first half of 2015). ### Loading the data</p>
<p>First, the data needs to be moved into the correct location for the
AIF360 library to find it. If you haven’t yet, run <code>setup.sh</code>
to complete that step. (Then, restart the kernel and re-load the
packages at the top of this file.)</p>
<p>First, we load the data. Next, we create the train/validation/test
splits and setup information about the privileged and unprivileged
groups. (Recall, we focus on race as the sensitive feature.)</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>(dataset_orig_panel19_train,</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a> dataset_orig_panel19_val,</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a> dataset_orig_panel19_test) <span class="op">=</span> MEPSDataset19().split([<span class="fl">0.5</span>, <span class="fl">0.8</span>], shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>sens_ind <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>sens_attr <span class="op">=</span> dataset_orig_panel19_train.protected_attribute_names[sens_ind]</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>unprivileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>                       dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>privileged_groups <span class="op">=</span> [{sens_attr: v} <span class="cf">for</span> v <span class="kw">in</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]</span></code></pre>
</div>
<p>Show details about the data.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> describe(train<span class="op">=</span><span class="va">None</span>, val<span class="op">=</span><span class="va">None</span>, test<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="cf">if</span> train <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>        display(Markdown(<span class="st">"#### Training Dataset shape"</span>))</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>        <span class="bu">print</span>(train.features.shape)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    <span class="cf">if</span> val <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>        display(Markdown(<span class="st">"#### Validation Dataset shape"</span>))</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>        <span class="bu">print</span>(val.features.shape)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    display(Markdown(<span class="st">"#### Test Dataset shape"</span>))</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    <span class="bu">print</span>(test.features.shape)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    display(Markdown(<span class="st">"#### Favorable and unfavorable labels"</span>))</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    <span class="bu">print</span>(test.favorable_label, test.unfavorable_label)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    display(Markdown(<span class="st">"#### Protected attribute names"</span>))</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    <span class="bu">print</span>(test.protected_attribute_names)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>    display(Markdown(<span class="st">"#### Privileged and unprivileged protected attribute values"</span>))</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>    <span class="bu">print</span>(test.privileged_protected_attributes, </span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>          test.unprivileged_protected_attributes)</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>    display(Markdown(<span class="st">"#### Dataset feature names</span><span class="ch">\n</span><span class="st"> See [MEPS documentation](https://meps.ahrq.gov/data_stats/download_data/pufs/h181/h181doc.pdf) for details on the various features"</span>))</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    <span class="bu">print</span>(test.feature_names)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>describe(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test)</span></code></pre>
</div>
<p>Next, we will look at whether the dataset contains bias; i.e., does
the outcome ‘UTILIZATION’ take on a positive value more frequently for
one racial group than another?</p>
<p>The disparate impact score will be between 0 and 1, where 1 indicates
<em>no bias</em>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>metric_orig_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>        dataset_orig_panel19_train,</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>explainer_orig_panel19_train <span class="op">=</span> MetricTextExplainer(metric_orig_panel19_train)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="bu">print</span>(explainer_orig_panel19_train.disparate_impact())</span></code></pre>
</div>
<p>We see that the disparate impact is about 0.48, which means the
privileged group has the favorable outcome at about 2x the rate as the
unprivileged group does.</p>
<p>(In this case, the “favorable” outcome is label=1, i.e., high
utilization) ## Train a model</p>
<p>We will train a logistic regression classifier.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>dataset <span class="op">=</span> dataset_orig_panel19_train</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>model <span class="op">=</span> make_pipeline(StandardScaler(),</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>                      LogisticRegression(solver<span class="op">=</span><span class="st">'liblinear'</span>, random_state<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>fit_params <span class="op">=</span> {<span class="st">'logisticregression__sample_weight'</span>: dataset.instance_weights}</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>lr_orig_panel19 <span class="op">=</span> model.fit(dataset.features, dataset.labels.ravel(), <span class="op">**</span>fit_params)</span></code></pre>
</div>
<div class="section level3">
<h3 id="validate-the-model">Validate the model<a class="anchor" aria-label="anchor" href="#validate-the-model"></a></h3>
<p>Recall that a logistic regression model can output probabilities
(i.e., <code>model.predict(dataset).scores</code>) and we can determine
our own threshold for predicting class 0 or 1.</p>
<p>The following function, <code>test</code>, computes performance on
the logistic regression model based on a variety of thresholds, as
indicated by <code>thresh_arr</code>, an array of threshold values. We
will continue to focus on disparate impact, but all other metrics are
described in the <a href="https://aif360.readthedocs.io/en/stable/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric" class="external-link">AIF360
documentation</a>.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> test(dataset, model, thresh_arr):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        <span class="co"># sklearn classifier</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        y_val_pred_prob <span class="op">=</span> model.predict_proba(dataset.features)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        pos_ind <span class="op">=</span> np.where(model.classes_ <span class="op">==</span> dataset.favorable_label)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">AttributeError</span> <span class="im">as</span> e:</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        <span class="bu">print</span>(e)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        <span class="co"># aif360 inprocessing algorithm</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>        y_val_pred_prob <span class="op">=</span> model.predict(dataset).scores</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        pos_ind <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        </span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    pos_ind <span class="op">=</span> np.where(model.classes_ <span class="op">==</span> dataset.favorable_label)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    metric_arrs <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    </span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    <span class="cf">for</span> thresh <span class="kw">in</span> thresh_arr:</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>        y_val_pred <span class="op">=</span> (y_val_pred_prob[:, pos_ind] <span class="op">&gt;</span> thresh).astype(np.float64)</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>        dataset_pred <span class="op">=</span> dataset.copy()</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>        dataset_pred.labels <span class="op">=</span> y_val_pred</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>        metric <span class="op">=</span> ClassificationMetric(</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>                dataset, dataset_pred,</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>                unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>                privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        <span class="co"># various metrics - can look up what they are on your own</span></span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        metric_arrs[<span class="st">'bal_acc'</span>].append((metric.true_positive_rate()</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>                                     <span class="op">+</span> metric.true_negative_rate()) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>        metric_arrs[<span class="st">'avg_odds_diff'</span>].append(metric.average_odds_difference())</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>        metric_arrs[<span class="st">'disp_imp'</span>].append(metric.disparate_impact())</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>        metric_arrs[<span class="st">'stat_par_diff'</span>].append(metric.statistical_parity_difference())</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>        metric_arrs[<span class="st">'eq_opp_diff'</span>].append(metric.equal_opportunity_difference())</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>        metric_arrs[<span class="st">'theil_ind'</span>].append(metric.theil_index())</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>    </span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>    <span class="cf">return</span> metric_arrs</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>thresh_arr <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.5</span>, <span class="dv">50</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>val_metrics <span class="op">=</span> test(dataset<span class="op">=</span>dataset_orig_panel19_val,</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>                   model<span class="op">=</span>lr_orig_panel19,</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>                   thresh_arr<span class="op">=</span>thresh_arr)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>lr_orig_best_ind <span class="op">=</span> np.argmax(val_metrics[<span class="st">'bal_acc'</span>])</span></code></pre>
</div>
<p>We will plot <code>val_metrics</code>. The x-axis will be the
threshold we use to output the label 1 (i.e., if the raw score is larger
than the threshold, we output 1).</p>
<p>The y-axis will show both balanced accuracy (in blue) and disparate
impact (in red).</p>
<p>Note that we plot 1 - Disparate Impact, so now a score of 0 indicates
no bias.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> plot(x, x_name, y_left, y_left_name, y_right, y_right_name):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    fig, ax1 <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    ax1.plot(x, y_left)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    ax1.set_xlabel(x_name, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    ax1.set_ylabel(y_left_name, color<span class="op">=</span><span class="st">'b'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    ax1.xaxis.set_tick_params(labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    ax1.yaxis.set_tick_params(labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    ax1.set_ylim(<span class="fl">0.5</span>, <span class="fl">0.8</span>)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    ax2 <span class="op">=</span> ax1.twinx()</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    ax2.plot(x, y_right, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    ax2.set_ylabel(y_right_name, color<span class="op">=</span><span class="st">'r'</span>, fontsize<span class="op">=</span><span class="dv">16</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'DI'</span> <span class="kw">in</span> y_right_name:</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        ax2.set_ylim(<span class="fl">0.</span>, <span class="fl">0.7</span>)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>        ax2.set_ylim(<span class="op">-</span><span class="fl">0.25</span>, <span class="fl">0.1</span>)</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    best_ind <span class="op">=</span> np.argmax(y_left)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    ax2.axvline(np.array(x)[best_ind], color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">':'</span>)</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    ax2.yaxis.set_tick_params(labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    ax2.grid(<span class="va">True</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>disp_imp <span class="op">=</span> np.array(val_metrics[<span class="st">'disp_imp'</span>])</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>disp_imp_err <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> disp_imp</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>plot(thresh_arr, <span class="st">'Classification Thresholds'</span>,</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>     val_metrics[<span class="st">'bal_acc'</span>], <span class="st">'Balanced Accuracy'</span>,</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>     disp_imp_err, <span class="st">'1 - DI'</span>)</span></code></pre>
</div>
<p>If you like, you can plot other metrics, e.g., average odds
difference.</p>
<p>In the next cell, we write a function to print out a variety of other
metrics. Since we look at 1 - disparate impact, <strong>all of these
metrics have a value of 0 if they are perfectly fair</strong>. Again,
you can learn more details about the various metrics in the <a href="https://aif360.readthedocs.io/en/stable/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric" class="external-link">AIF360
documentation</a>.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> describe_metrics(metrics, thresh_arr):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    best_ind <span class="op">=</span> np.argmax(metrics[<span class="st">'bal_acc'</span>])</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Threshold corresponding to Best balanced accuracy: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(thresh_arr[best_ind]))</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Best balanced accuracy: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(metrics[<span class="st">'bal_acc'</span>][best_ind]))</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    disp_imp_at_best_ind <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> metrics[<span class="st">'disp_imp'</span>][best_ind]</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Corresponding 1-DI value: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(disp_imp_at_best_ind))</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Corresponding average odds difference value: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(metrics[<span class="st">'avg_odds_diff'</span>][best_ind]))</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Corresponding statistical parity difference value: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(metrics[<span class="st">'stat_par_diff'</span>][best_ind]))</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Corresponding equal opportunity difference value: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(metrics[<span class="st">'eq_opp_diff'</span>][best_ind]))</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Corresponding Theil index value: </span><span class="sc">{:6.4f}</span><span class="st">"</span>.<span class="bu">format</span>(metrics[<span class="st">'theil_ind'</span>][best_ind]))</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>describe_metrics(val_metrics, thresh_arr)</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="test-the-model">Test the model<a class="anchor" aria-label="anchor" href="#test-the-model"></a></h3>
<p>Now that we have used the validation data to select the best
threshold, we will evaluate the test the model on the test data.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>lr_metrics <span class="op">=</span> test(dataset<span class="op">=</span>dataset_orig_panel19_test,</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>                       model<span class="op">=</span>lr_orig_panel19,</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>                       thresh_arr<span class="op">=</span>[thresh_arr[lr_orig_best_ind]])</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>describe_metrics(lr_metrics, [thresh_arr[lr_orig_best_ind]])</span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="mitigate-bias-with-in-processing">Mitigate bias with in-processing<a class="anchor" aria-label="anchor" href="#mitigate-bias-with-in-processing"></a>
<a class="anchor" aria-label="anchor" href="#mitigate-bias-with-in-processing"></a></h2>
<hr class="half-width"><p>We will use reweighting as an in-processing step to try to increase
fairness. AIF360 has a function that performs reweighting that we will
use. If you’re interested, you can look at details about how it works in
<a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html" class="external-link">the
documentation</a>.</p>
<p>If you look at the documentation, you will see that AIF360 classifies
reweighting as a preprocessing, not an in-processing intervention.
Technically, AIF360’s implementation modifies the dataset, not the
learning algorithm so it is pre-processing. But, it is functionally
equivalent to modifying the learning algorithm’s loss function, so we
follow the convention of the fair ML field and call it
in-processing.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># Reweighting is a AIF360 class to reweight the data </span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>RW <span class="op">=</span> Reweighing(unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>                privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>dataset_transf_panel19_train <span class="op">=</span> RW.fit_transform(dataset_orig_panel19_train)</span></code></pre>
</div>
<p>We’ll also define metrics for the reweighted data and print out the
disparate impact of the dataset.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>metric_transf_panel19_train <span class="op">=</span> BinaryLabelDatasetMetric(</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>        dataset_transf_panel19_train,</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>        unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>        privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>explainer_transf_panel19_train <span class="op">=</span> MetricTextExplainer(metric_transf_panel19_train)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="bu">print</span>(explainer_transf_panel19_train.disparate_impact())</span></code></pre>
</div>
<p>Then, we’ll train a model, validate it, and evaluate of the test
data.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># train</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>dataset <span class="op">=</span> dataset_transf_panel19_train</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>model <span class="op">=</span> make_pipeline(StandardScaler(),</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>                      LogisticRegression(solver<span class="op">=</span><span class="st">'liblinear'</span>, random_state<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>fit_params <span class="op">=</span> {<span class="st">'logisticregression__sample_weight'</span>: dataset.instance_weights}</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>lr_transf_panel19 <span class="op">=</span> model.fit(dataset.features, dataset.labels.ravel(), <span class="op">**</span>fit_params)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># validate</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>thresh_arr <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.5</span>, <span class="dv">50</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>val_metrics <span class="op">=</span> test(dataset<span class="op">=</span>dataset_orig_panel19_val,</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>                   model<span class="op">=</span>lr_transf_panel19,</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>                   thresh_arr<span class="op">=</span>thresh_arr)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>lr_transf_best_ind <span class="op">=</span> np.argmax(val_metrics[<span class="st">'bal_acc'</span>])</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="co"># plot validation results</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>disp_imp <span class="op">=</span> np.array(val_metrics[<span class="st">'disp_imp'</span>])</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>disp_imp_err <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> np.minimum(disp_imp, <span class="dv">1</span><span class="op">/</span>disp_imp)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>plot(thresh_arr, <span class="st">'Classification Thresholds'</span>,</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>     val_metrics[<span class="st">'bal_acc'</span>], <span class="st">'Balanced Accuracy'</span>,</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>     disp_imp_err, <span class="st">'1 - min(DI, 1/DI)'</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># describe validation results</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>describe_metrics(val_metrics, thresh_arr)</span></code></pre>
</div>
<div class="section level3">
<h3 id="test">Test<a class="anchor" aria-label="anchor" href="#test"></a></h3>
<p>lr_transf_metrics = test(dataset=dataset_orig_panel19_test,
model=lr_transf_panel19, thresh_arr=[thresh_arr[lr_transf_best_ind]])
describe_metrics(lr_transf_metrics, [thresh_arr[lr_transf_best_ind]]) We
see that the disparate impact score on the test data is better after
reweighting than it was originally.</p>
<p>How do the other fairness metrics compare? ## Mitigate bias with
preprocessing We will use a method, <a href="https://fairlearn.org/main/api_reference/generated/fairlearn.postprocessing.ThresholdOptimizer.html#fairlearn.postprocessing.ThresholdOptimizer" class="external-link">ThresholdOptimizer</a>,
that is implemented in the library <a href="https://fairlearn.org/" class="external-link">Fairlearn</a>. ThresholdOptimizer finds
custom thresholds for each demographic group so as to achieve parity in
the desired group fairness metric.</p>
<p>We will focus on demographic parity, but feel free to try other
metrics if you’re curious on how it does.</p>
<p>The first step is creating the ThresholdOptimizer object. We pass in
the demographic parity constraint, and indicate that we would like to
optimize the balanced accuracy score (other options include accuracy,
and true or false positive rate – see <a href="https://fairlearn.org/main/api_reference/generated/fairlearn.postprocessing.ThresholdOptimizer.html#fairlearn.postprocessing.ThresholdOptimizer" class="external-link">the
documentation</a> for more details).</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>to <span class="op">=</span> ThresholdOptimizer(estimator<span class="op">=</span>model, constraints<span class="op">=</span><span class="st">"demographic_parity"</span>, objective<span class="op">=</span><span class="st">"balanced_accuracy_score"</span>, prefit<span class="op">=</span><span class="va">True</span>)</span></code></pre>
</div>
<p>Next, we fit the ThresholdOptimizer object to the validation
data.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>to.fit(dataset_orig_panel19_val.features, dataset_orig_panel19_val.labels, </span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>       sensitive_features<span class="op">=</span>dataset_orig_panel19_val.protected_attributes[:,<span class="dv">0</span>])</span></code></pre>
</div>
<p>Then, we’ll create a helper function, <code>mini_test</code> to allow
us to call the <code>describe_metrics</code> function even though we are
no longer evaluating our method as a variety of thresholds.</p>
<p>After that, we call the ThresholdOptimizer’s predict function on the
validation and test data, and then compute metrics and print the
results.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="kw">def</span> mini_test(dataset, preds):</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>    metric_arrs <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>    dataset_pred <span class="op">=</span> dataset.copy()</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    dataset_pred.labels <span class="op">=</span> preds</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>    metric <span class="op">=</span> ClassificationMetric(</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>            dataset, dataset_pred,</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>            unprivileged_groups<span class="op">=</span>unprivileged_groups,</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>            privileged_groups<span class="op">=</span>privileged_groups)</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>    <span class="co"># various metrics - can look up what they are on your own</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>    metric_arrs[<span class="st">'bal_acc'</span>].append((metric.true_positive_rate()</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a>                                    <span class="op">+</span> metric.true_negative_rate()) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>    metric_arrs[<span class="st">'avg_odds_diff'</span>].append(metric.average_odds_difference())</span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a>    metric_arrs[<span class="st">'disp_imp'</span>].append(metric.disparate_impact())</span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>    metric_arrs[<span class="st">'stat_par_diff'</span>].append(metric.statistical_parity_difference())</span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a>    metric_arrs[<span class="st">'eq_opp_diff'</span>].append(metric.equal_opportunity_difference())</span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a>    metric_arrs[<span class="st">'theil_ind'</span>].append(metric.theil_index())</span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a>    </span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a>    <span class="cf">return</span> metric_arrs</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>to_val_preds <span class="op">=</span> to.predict(dataset_orig_panel19_val.features, sensitive_features<span class="op">=</span>dataset_orig_panel19_val.protected_attributes[:,<span class="dv">0</span>])</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>to_test_preds <span class="op">=</span> to.predict(dataset_orig_panel19_test.features, sensitive_features<span class="op">=</span>dataset_orig_panel19_test.protected_attributes[:,<span class="dv">0</span>])</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>to_val_metrics <span class="op">=</span> mini_test(dataset_orig_panel19_val, to_val_preds)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>to_test_metrics <span class="op">=</span> mini_test(dataset_orig_panel19_test, to_test_preds)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Remember, `Threshold corresponding to Best balanced accuracy` is just a placeholder here."</span>)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>describe_metrics(to_val_metrics, [<span class="dv">0</span>])</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Remember, `Threshold corresponding to Best balanced accuracy` is just a placeholder here."</span>)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>describe_metrics(to_test_metrics, [<span class="dv">0</span>])</span></code></pre>
</div>
<p>Scroll up and see how these results compare with the original
classifier and with the in-processing technique.</p>
<p>A major difference is that the accuracy is lower, now. In practice,
it might be better to use an algorithm that allows a custom tradeoff
between the accuracy sacrifice and increased levels of fairness.</p>
<p>We can also see what threshold is being used for each demographic
group by examining the
<code>interpolated_thresholder_.interpretation_dict</code> property of
the ThresholdOptimzer.</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>threshold_rules_by_group <span class="op">=</span> to.interpolated_thresholder_.interpolation_dict</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>threshold_rules_by_group</span></code></pre>
</div>
<p>Recall that a value of 1 in the Race column corresponds to White
people, while a value of 0 corresponds to non-White people.</p>
<p>Due to the inherent randomness of the ThresholdOptimizer, you might
get slightly different results than your neighbors. When we ran the
previous cell, the output was</p>
<p><code>{0.0: {'p0': 0.9287205987170348,   'operation0': [&gt;0.5],   'p1': 0.07127940128296517,   'operation1': [&gt;-inf]},  1.0: {'p0': 0.002549618320610717,   'operation0': [&gt;inf],   'p1': 0.9974503816793893,   'operation1': [&gt;0.5]}}</code></p>
<p>This tells us that for non-White individuals:</p>
<ul><li><p>If the score is above 0.5, predict 1.</p></li>
<li><p>Otherwise, predict 1 with probability 0.071</p></li>
</ul><p>And for White individuals:</p>
<ul><li>If the score is above 0.5, predict 1 with probability 0.997</li>
</ul><p><strong>Discussion question:</strong> what are the pros and cons of
improving the model fairness by introducing randomization?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>It’s important to consider many dimensions of model performance: a
single accuracy score is not sufficient.</li>
<li>There is no single definition of “fair machine learning”: different
notions of fairness are appropriate in different contexts.</li>
<li>Representational harms and stereotypes can be perpetuated by
generative AI.</li>
<li>The fairness of a model can be improved by using techniques like
data reweighting and model postprocessing.</li>
</ul></div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/2-model-fitting.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/4-explainability-vs-interpretability.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/2-model-fitting.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Scientific validity
        </a>
        <a class="chapter-link float-end" href="../instructor/4-explainability-vs-interpretability.html" rel="next">
          Next: Interpretablility...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/fair-explainable-ml/edit/main/episodes/3-model-eval-and-fairness.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/fair-explainable-ml/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/fair-explainable-ml/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/fair-explainable-ml/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:apmeyer4@wisc.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.5" class="external-link">sandpaper (0.16.5)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.6" class="external-link">pegboard (0.7.6)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.3" class="external-link">varnish (1.0.3)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/fair-explainable-ml/instructor/3-model-eval-and-fairness.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "fairness, explainability, fair machine learning, interpretable machine learning, xai, lesson, The Carpentries",
  "name": "Model evaluation and fairness",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/fair-explainable-ml/instructor/3-model-eval-and-fairness.html",
  "identifier": "https://carpentries-incubator.github.io/fair-explainable-ml/instructor/3-model-eval-and-fairness.html",
  "dateCreated": "2023-12-05",
  "dateModified": "2024-06-19",
  "datePublished": "2024-07-31"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

