<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Trustworthy AI: Explainability, Bias, Fairness, and Safety: Model evaluation and fairness</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/2-model-eval-and-fairness.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Trustworthy AI: Explainability, Bias, Fairness, and Safety
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Trustworthy AI: Explainability, Bias, Fairness, and Safety
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Trustworthy AI: Explainability, Bias, Fairness, and Safety
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 42%" class="percentage">
    42%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 42%" aria-valuenow="42" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/2-model-eval-and-fairness.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="0-introduction.html">1. Overview</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="1-preparing-to-train.html">2. Preparing to train a model</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        3. Model evaluation and fairness
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#accuracy-metrics">Accuracy metrics</a></li>
<li><a href="#model-evaluation-pitfalls">Model evaluation pitfalls</a></li>
<li><a href="#measuring-fairness">Measuring fairness</a></li>
<li><a href="#fairness-in-generative-ai">Fairness in generative AI</a></li>
<li><a href="#improving-fairness-of-models">Improving fairness of models</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="3-model-fairness-deep-dive.html">4. Model fairness: hands-on</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="4-explainability-vs-interpretability.html">5. Interpretablility versus explainability</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="5a-explainable-AI-method-overview.html">6. Explainability methods overview</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="5b-deep-dive-into-methods.html">7. Explainability methods: deep dive</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="5c-probes.html">8. Explainability methods: linear probe</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="5d-gradcam.html">9. Explainability methods: GradCAM</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="6-confidence-intervals.html">10. Estimating model uncertainty</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="7a-OOD-detection-overview.html">11. OOD detection: overview</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush13">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading13">
        <a href="7b-OOD-detection-softmax.html">12. OOD detection: softmax</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush14">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading14">
        <a href="7c-OOD-detection-energy.html">13. OOD detection: energy</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush15">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading15">
        <a href="7d-OOD-detection-distance-based.html">14. OOD detection: distance-based</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush16">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading16">
        <a href="7e-OOD-detection-algo-design.html">15. OOD detection: training-time regularization</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush17">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading17">
        <a href="8-releasing-a-model.html">16. Documenting and releasing a model</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="1-preparing-to-train.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="3-model-fairness-deep-dive.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="1-preparing-to-train.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Preparing to train a
        </a>
        <a class="chapter-link float-end" href="3-model-fairness-deep-dive.html" rel="next">
          Next: Model fairness:...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Model evaluation and fairness</h1>
        <p>Last updated on 2024-10-15 |

        <a href="https://github.com/carpentries-incubator/fair-explainable-ml/edit/main/episodes/2-model-eval-and-fairness.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What metrics do we use to evaluate models?</li>
<li>What are some common pitfalls in model evaluation?</li>
<li>How do we define fairness and bias in machine learning
outcomes?</li>
<li>What types of bias and unfairness can occur in generative AI?</li>
<li>What techniques exist to improve the fairness of ML models?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Reason about model performance through standard evaluation
metrics.</li>
<li>Recall how underfitting, overfitting, and data leakage impact model
performance.</li>
<li>Understand and distinguish between various notions of fairness in
machine learning.</li>
<li>Understand general approaches for improving the fairness of ML
models.</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="accuracy-metrics">Accuracy metrics<a class="anchor" aria-label="anchor" href="#accuracy-metrics"></a></h2>
<hr class="half-width"><p>Stakeholders often want to know the accuracy of a machine learning
model – what percent of predictions are correct? Accuracy can be
decomposed into further metrics: e.g., in a binary prediction setting,
recall (the fraction of positive samples that are classified correctly)
and precision (the fraction of samples classified as positive that
actually are positive) are commonly-used metrics.</p>
<p>Suppose we have a model that performs binary classification (+, -) on
a test dataset of 1000 samples (let <span class="math inline">\(n\)</span>=1000). A <em>confusion matrix</em>
defines how many predictions we make in each of four quadrants: true
positive with positive prediction (++), true positive with negative
prediction (+-), true negative with positive prediction (-+), and true
negative with negative prediction (–).</p>
<table class="table"><thead><tr class="header"><th></th>
<th>True +</th>
<th>True -</th>
</tr></thead><tbody><tr class="odd"><td>Predicted +</td>
<td>300</td>
<td>80</td>
</tr><tr class="even"><td>Predicted -</td>
<td>25</td>
<td>595</td>
</tr></tbody></table><p>So, for instance, 80 samples have a true class of + but get predicted
as members of -.</p>
<p>We can compute the following metrics:</p>
<ul><li>Accuracy: What fraction of predictions are correct?
<ul><li>(300 + 595) / 100 = 0.895</li>
<li>Accuracy is 89.5%</li>
</ul></li>
<li>Precision: What fraction of predicted positives are true positives?
<ul><li>300 / (300 + 80) = 0.789</li>
<li>Precision is 78.9%</li>
</ul></li>
<li>Recall: What fraction of true positives are classified as positive?
<ul><li>300 / (300 + 25) = 0.923</li>
<li>Recall is 92.3%</li>
</ul></li>
</ul><div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>We’ve discussed binary classification but for other types of tasks
there are different metrics. For example,</p>
<ul><li>Multi-class problems often use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.top_k_accuracy_score.html" class="external-link">Top-K
accuracy</a>, a metric of how often the true response appears in their
top-K guesses.</li>
<li>Regression tasks often use the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" class="external-link">Area
Under the ROC curve (AUC ROC)</a> as a measure of how well the
classifier performs at different thresholds.</li>
</ul></div>
</div>
</div>
<div id="what-accuracy-metric-to-use" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="what-accuracy-metric-to-use" class="callout-inner">
<h3 class="callout-title">What accuracy metric to use?</h3>
<div class="callout-content">
<p>Different accuracy metrics may be more relevant in different
situations. Discuss with a partner or small groups whether precision,
recall, or some combination of the two is most relevant in the following
prediction tasks:</p>
<ol style="list-style-type: decimal"><li>Deciding what patients are high risk for a disease and who should
get additional low-cost screening.</li>
<li>Deciding what patients are high risk for a disease and should start
taking medication to lower the disease risk. The medication is expensive
and can have unpleasant side effects.</li>
</ol></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li><p>It is best if all patients who need the screening get it, and
there is little downside for doing screenings unnecessarily because the
screening costs are low. Thus, a high recall score is optimal.</p></li>
<li><p>Given the costs and side effects of the medicine, we do not want
patients not at risk for the disease to take the medication. So, a high
precision score is ideal.</p></li>
</ol></div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="model-evaluation-pitfalls">Model evaluation pitfalls<a class="anchor" aria-label="anchor" href="#model-evaluation-pitfalls"></a></h2>
<hr class="half-width"><div class="section level3">
<h3 id="overfitting-and-underfitting">Overfitting and underfitting<a class="anchor" aria-label="anchor" href="#overfitting-and-underfitting"></a></h3>
<p><strong>Overfitting</strong> is characterized by worse performance on
the test set than on the train set and can be fixed by switching to a
simpler model architecture or by adding regularization.</p>
<p><strong>Underfitting</strong> is characterized by poor performance on
both the training and test datasets. It can be fixed by collecting more
training data, switching to a more complex model architecture, or
improving feature quality.</p>
<figure><img src="https://kharshit.github.io/img/overfitting.png" alt="graphs of overfitting and underfitting" class="figure mx-auto d-block"><div class="figcaption">Example of overfitting/underfitting</div>
</figure><p>If you need a refresher on how to detect overfitting and underfitting
in your models, <a href="https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5" class="external-link">this
article</a> is a good resource.</p>
</div>
<div class="section level3">
<h3 id="data-leakage">Data Leakage<a class="anchor" aria-label="anchor" href="#data-leakage"></a></h3>
<p>Data leakage occurs when the model has access to the test data during
training and results in overconfidence in the model’s performance.</p>
<p><a href="https://www.sciencedirect.com/science/article/pii/S2666389923001599" class="external-link">Recent
work</a> by Sayash Kapoor and Arvind Narayanan shows that data leakage
is incredibly widespread in papers that use ML across several scientific
fields. They define 8 common ways that data leakage occurs,
including:</p>
<ol style="list-style-type: decimal"><li>No test set: there is no hold-out test-set, rather, the model is
evaluated on a subset of the training data. This is the “obvious,”
canonical example of data leakage.</li>
<li>Preprocessing on whole dataset: when preprocessing occurs on the
train + test sets, rather than just the train set, the model learns
information about the test set that it should not have access to until
later. For instance, missing feature imputation based on the full
dataset will be different than missing feature imputation based only on
the values in the train dataset.</li>
<li>Illegitimate features: sometimes, there are features that are
proxies for the outcome variable. For instance, if the goal is to
predict whether a patient has hypertension, including whether they are
on a common hypertension medication is data leakage since future, new
patients would not already be on this medication.</li>
<li>Temporal leakage: if the model predicts a future outcome, the train
set should contain information from the future. For instance, if the
task is to predict whether a patient will develop a particular disease
within 1 year, the dataset should not contain data points for the same
patient from multiple years.</li>
</ol></div>
</section><section><h2 class="section-heading" id="measuring-fairness">Measuring fairness<a class="anchor" aria-label="anchor" href="#measuring-fairness"></a></h2>
<hr class="half-width"><p>What does it mean for a machine learning model to be fair or
unbiased? There is no single definition of fairness, and we can talk
about fairness at several levels (ranging from training data, to model
internals, to how a model is deployed in practice). Similarly, bias is
often used as a catch-all term for any behavior that we think is unfair.
Even though there is no tidy definition of unfairness or bias, we can
use aggregate model outputs to gain an overall understanding of how
models behave with respect to different demographic groups – an approach
called group fairness.</p>
<p>In general, if there are no differences between groups in the real
world (e.g., if we lived in a utopia with no racial or gender gaps),
achieving fairness is easy. But, in practice, in many social settings
where prediction tools are used, there are differences between groups,
e.g., due to historical and current discrimination.</p>
<p>For instance, in a loan prediction setting in the United States, the
average white applicant may be better positioned to repay a loan than
the average Black applicant due to differences in generational wealth,
education opportunities, and other factors stemming from anti-Black
racism. Suppose that a bank uses a machine learning model to decide who
gets a loan. Suppose that 50% of white applicants are granted a loan,
with a precision of 90% and a recall of 70% – in other words, 90% of
white people granted loans end up repaying them, and 70% of all people
who would have repaid the loan, if given the opportunity, get the loan.
Consider the following scenarios:</p>
<ul><li>(Demographic parity) We give loans to 50% of Black applicants in a
way that maximizes overall accuracy</li>
<li>(Equalized odds) We give loans to X% of Black applicants, where X is
chosen to maximize accuracy subject to keeping precision equal to
90%.</li>
<li>(Group level calibration) We give loans to X% of Black applicants,
where X is chosen to maximize accuracy while keeping recall equal to
70%.</li>
</ul><p>There are <em>many</em> notions of statistical group fairness, but
most boil down to one of the three above options: demographic parity,
equalized odds, and group-level calibration. All three are forms of
<em>distributional</em> (or <em>outcome</em>) fairness. Another
dimension, though, is <em>procedural</em> fairness: whether decisions
are made in a just way, regardless of final outcomes. Procedural
fairness contains many facets, but one way to operationalize it is to
consider individual fairness (also called counterfactual fairness),
which was suggested in 2012 by <a href="https://dl.acm.org/doi/abs/10.1145/2090236.2090255" class="external-link">Dwork et
al.</a> as a way to ensure that “similar individuals [are treated]
similarly”. For instance, if two individuals differ only on their race
or gender, they should receive the same outcome from an algorithm that
decides whether to approve a loan application.</p>
<p>In practice, it’s hard to use individual fairness because defining a
complete set of rules about when two individuals are sufficiently
“similar” is challenging.</p>
<div id="matching-fairness-terminology-with-definitions" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="matching-fairness-terminology-with-definitions" class="callout-inner">
<h3 class="callout-title">Matching fairness terminology with definitions</h3>
<div class="callout-content">
<p>Match the following types of formal fairness with their definitions.
(A) Individual fairness, (B) Equalized odds, (C) Demographic parity, and
(D) Group-level calibration</p>
<ol style="list-style-type: decimal"><li>The model is equally accurate across all demographic groups.</li>
<li>Different demographic groups have the same true positive rates and
false positive rates.</li>
<li>Similar people are treated similarly.</li>
<li>People from different demographic groups receive each outcome at the
same rate.</li>
</ol></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>A - 3, B - 2, C - 4, D - 1</p>
</div>
</div>
</div>
</div>
<p>But some types of unfairness cannot be directly measured by
group-level statistical data. In particular, generative AI opens up new
opportunities for bias and unfairness. Bias can occur through
representational harms (e.g., creating content that over-represents one
population subgroup at the expense of another), or through stereotypes
(e.g., creating content that reinforces real-world stereotypes about a
group of people). We’ll discuss some specific examples of bias in
generative models next.</p>
</section><section><h2 class="section-heading" id="fairness-in-generative-ai">Fairness in generative AI<a class="anchor" aria-label="anchor" href="#fairness-in-generative-ai"></a></h2>
<hr class="half-width"><p>Generative models learn from statistical patterns in real-world data.
These statistical patterns reflect instances of bias in real-world data
- what data is available on the internet, what stereotypes does it
reinforce, and what forms of representation are missing?</p>
<div class="section level3">
<h3 id="natural-language">Natural language<a class="anchor" aria-label="anchor" href="#natural-language"></a></h3>
<p>One set of social stereotypes that large AI models can learn is
gender based. For instance, certain occupations are associated with men,
and others with women. For instance, in the U.S., doctors are
historically and stereotypically usually men.</p>
<p>In 2016, Caliskan et al. <a href="https://www.fatml.org/schedule/2016/presentation/semantics-derived-automatically-language-corpora" class="external-link">showed
that machine translation systems exhibit gender bias</a>, for instance,
by reverting to stereotypical gendered pronouns in ambiguous
translations, like in Turkish – a language without gendered pronouns –
to English.</p>
<p>In response, Google <a href="https://blog.research.google/2018/12/providing-gender-specific-translations.html" class="external-link">tweaked
their translator algorithms</a> to identify and correct for gender
stereotypes in Turkish and several other widely-spoken languages. So
when we repeat a similar experiment today, we get the following
output:</p>
<figure><img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-turkish-nlp-stereotypes.png" alt='Screenshot of Google Translate output. The English sentence "The doctor is on her lunch break" is translated to Turkish, and then the Turkish output is translated back to English as either "The doctor is on his lunch break" or "The doctor is on his lunch break".' class="figure mx-auto d-block"><div class="figcaption">Turkish Google Translate example (screenshot
from 1/9/2024)</div>
</figure><p>But for other, less widely-spoken languages, the original problem
persists:</p>
<figure><img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-norwegian-nlp-stereotypes.png" alt='Screenshot of Google Translate output. The English sentence "The doctor is on her lunch break" is translated to Norwegian, and then the Norwegian output is translated back to English as "The doctor is on his lunch break".' class="figure mx-auto d-block"><div class="figcaption">Norwegian Google Translate example (screenshot
from 1/9/2024)</div>
</figure><p>We’re not trying to slander Google Translate here – the translation,
without additional context, is ambiguous. And even if they extended the
existing solution to Norwegian and other languages, the underlying
problem (stereotypes in the training data) still exists. And with
generative AI such as ChatGPT, the problem can be even more
pernicious.</p>
<div id="red-teaming-large-language-models" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="red-teaming-large-language-models" class="callout-inner">
<h3 class="callout-title">Red-teaming large language models</h3>
<div class="callout-content">
<p>In cybersecurity, “red-teaming” is when well-intentioned people think
like a hacker in order to make a system safer. In the context of Large
Language Models (LLMs), red-teaming is used to try to get LLMs to output
offensive, inaccurate, or unsafe content, with the goal of understanding
the limitations of the LLM.</p>
<p>Try out red-teaming with ChatGPT or another LLM. Specifically, can
you construct a prompt that causes the LLM to output stereotypes? Here
are some example prompts, but feel free to get creative!</p>
<ul><li><p>“Tell me a story about a doctor” (or other profession with
gender)</p></li>
<li><p>If you speak a language other than English, how does are
ambiguous gendered pronouns handled? For instance, try the prompt
“Translate ‘The doctor is here’ to Spanish”. Is a masculine or feminine
pronoun used for the doctor in Spanish?</p></li>
</ul><p>If you use LLMs in your research, consider whether any of these
issues are likely to be present for your use cases. If you do not use
LLMs in your research, consider how these biases can affect downstream
uses of the LLM’s output.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>Most publicly-available LLM providers set up guardrails to avoid
propagating biases present in their training data. For instance, as of
the time of this writing (January 2024), the first suggested prompt,
“Tell me a story about a doctor,” consistently creates a story about a
woman doctor. Similarly, substituting other professions that have strong
associations with men for “doctor” (e.g., “electrical engineer,”
“garbage collector,” and “US President”) yield stories with female or
gender-neutral names and pronouns.</p>
</div>
</div>
</div>
</div>
<div id="discussing-other-fairness-issues" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussing-other-fairness-issues" class="callout-inner">
<h3 class="callout-title">Discussing other fairness issues</h3>
<div class="callout-content">
<p>If you use LLMs in your research, consider whether any of these
issues are likely to be present for your use cases. Share your thoughts
in small groups with other workshop participants.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="image-generation">Image generation<a class="anchor" aria-label="anchor" href="#image-generation"></a></h3>
<p>The same problems that language modeling face also affect image
generation. Consider, for instance, Melon et al. <a href="https://arxiv.org/pdf/2003.03808.pdf" class="external-link">developed an algorithm
called Pulse</a> that can convert blurry images to higher resolution.
But, biases were quickly unearthed and <a href="https://twitter.com/Chicken3gg/status/1274314622447820801?s=20&amp;t=_oORPJBJRaBW_J0zresFJQ" class="external-link">shared
via social media</a>.</p>
<div id="discussion5" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Who is shown in this blurred picture? <img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-obama.png" alt="blurry image of Barack Obama" class="figure"></p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p>While the picture is of Barack Obama, the upsampled image shows a
white face. <img src="https://raw.githubusercontent.com/carpentries-incubator/fair-explainable-ml/main/images/e4-obama-upsampled.png" alt="Unblurred version of the pixelated picture of Obama. Instead of showing Obama, it shows a white man." class="figure"></p>
<p>You can <a href="https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb#scrollTo=fU0aGtD4Nl4W" class="external-link">try
the model here</a>.</p>
</div>
</div>
</div>
</div>
<p>Menon and colleagues subsequently updated their paper to discuss this
issue of bias. They assert that the problems inherent in the PULSE model
are largely a result of the <a href="https://arxiv.org/abs/1812.04948" class="external-link">underlying StyleGAN model</a>,
which they had used in their work.</p>
<blockquote>
<p>Overall, it seems that sampling from StyleGAN yields white faces much
more frequently than faces of people of color … This bias extends to any
downstream application of StyleGAN, including the implementation of
PULSE using StyleGAN.</p>
<p>…</p>
<p>Results indicate a racial bias among the generated pictures, with
close to three-fourths (72.6%) of the pictures representing White
people. Asian (13.8%) and Black (10.1%) are considerably less frequent,
while Indians represent only a minor fraction of the pictures
(3.4%).</p>
</blockquote>
<p>These remarks get at a central issue: biases in any building block of
a system (data, base models, etc.) get propagated forwards. In
generative AI, such as text-to-image systems, this can result in
representational harms, <a href="https://arxiv.org/pdf/2211.03759.pdf" class="external-link">as documented by Bianchi et
al.</a> Fixing these issues of bias is still an active area of research.
One important step is to be careful in data collection, and try to get a
balanced dataset that does not contain harmful stereotypes. But large
language models use massive training datasets, so it is not possible to
manually verify data quality. Instead, researchers use heuristic
approaches to improve data quality, and then rely on various techniques
to improve models’ fairness, which we discuss next.</p>
</div>
</section><section><h2 class="section-heading" id="improving-fairness-of-models">Improving fairness of models<a class="anchor" aria-label="anchor" href="#improving-fairness-of-models"></a></h2>
<hr class="half-width"><p>Model developers frequently try to improve the fairness of there
model by intervening at one of three stages: pre-processing,
in-processing, or post-processing. We’ll cover techniques within each of
these paradigms in turn.</p>
<p>We start, though, by discussing why removing the sensitive
attribute(s) is not sufficient. Consider the task of deciding which loan
applicants are funded. Suppose we are concerned with racial bias in the
model outputs. If we remove race from the set of attributes available to
the model, the model cannot make <em>overly</em> racist decisions.
However, it could instead make decisions based on zip code, which in the
US is a very good proxy for race.</p>
<p>Can we simply remove all proxy variables? We could likely remove zip
code, if we cannot identify a causal relationship between where someone
lives and whether they will be able to repay a loan. But what about an
attribute like educational achievement? Someone with a college degree
(compared with someone with, say, less than a high school degree) has
better employment opportunities and therefore might reasonably be
expected to be more likely to be able to repay a loan. However,
educational attainment is still a proxy for race in the United States
due to historical (and ongoing) discrimination.</p>
<p><strong>Pre-processing</strong> generally modifies the dataset used
for learning. Techniques in this category include:</p>
<ul><li><p>Oversampling/undersampling: instead of training a machine
learning model on all of the data, <em>undersample</em> the majority
class by removing some of the majority class samples from the dataset in
order to have a more balanced dataset. Alternatively,
<em>oversample</em> the minority class by duplicating samples belonging
to this group.</p></li>
<li><p>Data augmentation: the number of samples from minority groups may
be increased by generating synthetic data with a generative adversarial
network (GAN). We won’t cover this method in this workshop (using a GAN
can be more computationally expensive than other techniques). If you’re
interested, you can learn more about this method from the paper <a href="https://link.springer.com/chapter/10.1007/978-3-030-58542-6_23" class="external-link">Inclusive
GAN: Improving Data and Minority Coverage in Generative
Models</a>.</p></li>
<li><p>Changing feature representations: various techniques have been
proposed to increase fairness by removing unfairness from the data
directly. To do so, the data is converted into an alternate
representation so that differences between demographic groups are
minimized, yet enough information is maintained in order to be able to
learn a model that performs well. An advantage of this method is that it
is model-agnostic, however, a challenge is it reduces the
interpretability of interpretable models and makes post-hoc
explainability less meaningful for black-box models.</p></li>
</ul><div id="pros-and-cons-of-preprocessing-options" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="pros-and-cons-of-preprocessing-options" class="callout-inner">
<h3 class="callout-title">Pros and cons of preprocessing options</h3>
<div class="callout-content">
<p>Discuss what you think the pros and cons of the different
pre-processing options are. What techniques might work better in
different settings?</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5"> Show me the solution </h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" aria-labelledby="headingSolution5" data-bs-parent="#accordionSolution5">
<div class="accordion-body">
<p>A downside of oversampling is that it may violate statistical
assumptions about independence of samples. A downside of undersampling
is that the total amount of data is reduced, potentially resulting in
models that perform less well overall.</p>
<p>A downside of using GANs to generate additional data is that this
process may be expensive and require higher levels of ML expertise.</p>
<p>A challenge with all techniques is that if there is not sufficient
data from minority groups, it may be hard to achieve good performance on
the groups without simply collecting more or higher-quality data.</p>
</div>
</div>
</div>
</div>
<p><strong>In-processing</strong> modifies the learning algorithm. Some
specific in-processing techniques include:</p>
<ul><li><p>Reweighting samples: many machine learning models allow for
reweighting individual samples, i.e., indicating that misclassifying
certain, rarer, samples should be penalized more severely in the loss
function. In the code example, we show how to reweight samples using
AIF360’s <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html" class="external-link">Reweighting</a>
function.</p></li>
<li><p>Incorporating fairness into the loss function: reweighting
explicitly instructs the loss function to penalize the misclassification
of certain samples more harshly. However, another option is to add a
term to the loss function corresponding to the fairness metric of
interest.</p></li>
</ul><p><strong>Post-processing</strong> modifies an existing model to
increase its fairness. Techniques in this category often compute a
custom <em>threshold</em> for each demographic group in order to satisfy
a specific notion of group fairness. For instance, if a machine learning
model for a binary prediction task uses 0.5 as a cutoff (e.g., raw
scores less than 0.5 get a prediction of 0 and others get a prediction
of 1), fair post-processing techniques may select different thresholds,
e.g., 0.4 or 0.6 for different demographic groups.</p>
<p>In the next episode, we explore two different bias mitigations
strategies implemented in the <a href="https://aif360.readthedocs.io/en/stable/" class="external-link">AIF360 Fairness
Toolkit</a>. <!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 --></p>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="1-preparing-to-train.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="3-model-fairness-deep-dive.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="1-preparing-to-train.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Preparing to train a
        </a>
        <a class="chapter-link float-end" href="3-model-fairness-deep-dive.html" rel="next">
          Next: Model fairness:...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/fair-explainable-ml/edit/main/episodes/2-model-eval-and-fairness.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/fair-explainable-ml/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/fair-explainable-ml/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/fair-explainable-ml/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:apmeyer4@wisc.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.10" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/fair-explainable-ml/2-model-eval-and-fairness.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "fairness, explainability, fair machine learning, interpretable machine learning, xai, lesson, The Carpentries",
  "name": "Model evaluation and fairness",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/fair-explainable-ml/2-model-eval-and-fairness.html",
  "identifier": "https://carpentries-incubator.github.io/fair-explainable-ml/2-model-eval-and-fairness.html",
  "dateCreated": "2023-12-05",
  "dateModified": "2024-10-15",
  "datePublished": "2024-12-10"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

